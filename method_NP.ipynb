{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2dd97eb1-5c1b-49cf-ab08-4f19ff72a18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import os\n",
    "import json\n",
    "from tools.data_utils import *\n",
    "from tools.optimization import EarlyStoppingCallback, evaluate_model\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import time\n",
    "from tools.NPmodels import *\n",
    "from tools.NPtrain import process_batch, NeuralProcessTrainer, evaluate_np\n",
    "from tabulate import tabulate\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import time\n",
    "from torch.utils.data import Subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda27cca-8e72-445d-9f07-dc21d742e64c",
   "metadata": {},
   "source": [
    "Customized collate function for pin problems\n",
    "\n",
    "It outputs three items: images, pins (coordinates), and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9230efa-c653-44a5-a6f7-76d0ad129aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    images = [sample['image'] for sample in batch]\n",
    "    pins = [sample['pins'] for sample in batch]\n",
    "    outputs = [sample['outputs'] for sample in batch]\n",
    "\n",
    "    return {\n",
    "        'image': torch.stack(images, dim=0),\n",
    "        'pins': pins,\n",
    "        'outputs': outputs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd38ff63-7385-41ec-959a-180f0fdcfe01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prepare(config, seed):\n",
    "    dataset = config['dataset']\n",
    "    mesh = True if config['mode'] == \"mesh\" else False\n",
    "    feature_extracted = True if config['feature'] == \"DDPM\" else False\n",
    "    modality = config['modality']\n",
    "    batch_size = config['batch_size']\n",
    "    n_pins = config['n_pins']\n",
    "    d = config['d']\n",
    "    r = config['r']\n",
    "\n",
    "    if dataset == \"Synthetic\":\n",
    "        input_shape = 28\n",
    "        if feature_extracted:\n",
    "            input_channel = 74\n",
    "        else:\n",
    "            input_channel = 3\n",
    "    elif dataset == \"PinMNIST\":\n",
    "        input_shape = 28\n",
    "        if feature_extracted:\n",
    "            input_channel = 71\n",
    "        else:\n",
    "            input_channel = 1\n",
    "    elif dataset == \"Building\":\n",
    "        input_shape = 100\n",
    "        if feature_extracted:\n",
    "            input_channel = 3584\n",
    "        else:\n",
    "            if modality == \"PS-RGBNIR\":\n",
    "                input_channel = 4\n",
    "            elif modality == \"PS-RGB\":\n",
    "                input_channel = 3\n",
    "            elif modality == \"PS-RGBNIR-SAR\":\n",
    "                input_channel = 8\n",
    "    elif dataset == \"Cars\":\n",
    "        if feature_extracted:\n",
    "            #Â TO DO: Check how many features does the DDPM version has\n",
    "            print('DDPM is still not available for this dataset')\n",
    "        else:\n",
    "            input_channel = 3\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    if feature_extracted:\n",
    "        folder = f\"{dataset}_ddpm\"\n",
    "    else:\n",
    "        folder = f\"{dataset}\"\n",
    "\n",
    "    if dataset == \"PinMNIST\":\n",
    "        test_data_folder = f\"./data/{folder}/random_fixedTrue_{n_pins}pins_{28}by{28}pixels_{r}radius_4seed\"\n",
    "        if mesh:\n",
    "            data_folder = f\"./data/{folder}/mesh_{d}step_{28}by{28}pixels_{r}radius_4seed\"\n",
    "            config['n_pins'] = (28 // d + 1) ** 2\n",
    "        else: # Random pins \n",
    "            data_folder = test_data_folder\n",
    "\n",
    "    elif dataset == \"Synthetic\":\n",
    "        folder += \"/28by28pixels_1000images_123456seed\"\n",
    "        test_data_folder = f\"./data/{folder}/random_{n_pins}pins\"\n",
    "        if mesh:\n",
    "            data_folder = f\"./data/{folder}/mesh_{d}step_pins\"\n",
    "            config['n_pins'] = (28 // d + 1) ** 2\n",
    "        else:\n",
    "\n",
    "            data_folder = test_data_folder\n",
    "    elif dataset == \"Building\":\n",
    "        test_data_folder = f\"./data/{folder}/random_n_pins_{n_pins}\"\n",
    "        if mesh:\n",
    "            data_folder = f\"./data/{folder}/mesh_{d}_step\"\n",
    "            config['n_pins'] = (100 // d + 1) ** 2\n",
    "        else:\n",
    "            data_folder = f\"./data/{folder}/random_n_pins_{n_pins}\"\n",
    "            data_folder = test_data_folder\n",
    "            \n",
    "    elif dataset == \"Cars\":\n",
    "        r = 100\n",
    "        test_data_folder = f\"./data/{folder}/test/random_fixedTrue_{n_pins}pins_{800}by{800}pixels_{r}radius_{seed}seed\"\n",
    "        if mesh:\n",
    "            train_data_folder = f\"./data/{folder}/train/mesh_{d}step_{800}by{800}pixels_{r}radius_{seed}seed\"\n",
    "            val_data_folder = f\"./data/{folder}/val/mesh_{d}step_{800}by{800}pixels_{r}radius_{seed}seed\"\n",
    "            config['n_pins'] = (800 // d + 1) ** 2\n",
    "        else: # Random pins \n",
    "            data_folder = test_data_folder\n",
    "\n",
    "    if dataset == \"Building\":\n",
    "        transform = transforms.Compose([\n",
    "        ToTensor(),  # Convert to tensor (as you were doing)\n",
    "        Resize100(),  # Resize to 100x100\n",
    "    ])\n",
    "    elif dataset == \"Cars\":\n",
    "        transform = transforms.Compose([\n",
    "        ExtractImage(), # Get image from image and mask combination\n",
    "        ToTensor(),  # Convert to tensor (as you were doing)\n",
    "        Resize200(),  # Resize to 200x200\n",
    "    ])\n",
    "    else:\n",
    "        transform = transforms.Compose([\n",
    "        ToTensor(),  # Convert to tensor (as you were doing)\n",
    "        Resize()  # Resize to 100x100\n",
    "    ])        \n",
    "    # As DDPM does not work well with Rotterdam Building dataset, we have not explored this dataset with different modalities with DDPM\n",
    "    if dataset == \"Building\":\n",
    "        root_dir = f\"./data/Building/{modality}/\"\n",
    "        transformed_dataset = PinDataset(csv_file=f\"{data_folder}/pins.csv\",\n",
    "                                     root_dir=root_dir, modality=modality,\n",
    "                                     transform=transform)\n",
    "        test_dataset = PinDataset(csv_file=f\"{test_data_folder}/pins.csv\",\n",
    "                                     root_dir=root_dir, modality=modality,\n",
    "                                     transform=transform)\n",
    "    elif dataset == \"Cars\":\n",
    "        root_dir=f\"./data/{folder}/images/\"\n",
    "        train_dataset = PinDataset(csv_file=f\"{train_data_folder}/pins.csv\",\n",
    "                                     root_dir=root_dir, transform=transform)\n",
    "        val_dataset = PinDataset(csv_file=f\"{val_data_folder}/pins.csv\",\n",
    "                                     root_dir=root_dir, transform=transform)\n",
    "        eval_dataset = PinDataset(csv_file=f\"{test_data_folder}/pins.csv\",\n",
    "                                     root_dir=root_dir, transform=transform)\n",
    "    else:\n",
    "        root_dir=f\"./data/{folder}/images/\"\n",
    "        transformed_dataset = PinDataset(csv_file=f\"{data_folder}/pins.csv\",\n",
    "                                     root_dir=root_dir, transform=transform)\n",
    "        test_dataset = PinDataset(csv_file=f\"{test_data_folder}/pins.csv\",\n",
    "                                     root_dir=root_dir, transform=transform)\n",
    "\n",
    "    dataset_size = len(transformed_dataset)\n",
    "    train_size = int(0.7 * dataset_size)\n",
    "    val_size = int(0.10 * dataset_size)\n",
    "    test_size = dataset_size - train_size - val_size\n",
    "\n",
    "    if dataset != \"Cars\":\n",
    "        if os.path.exists(f\"./data/{dataset}/train_indices.npy\"):\n",
    "            train_indices = np.load(f'./data/{dataset}/train_indices.npy')\n",
    "            val_indices = np.load(f'./data/{dataset}/val_indices.npy')\n",
    "            test_indices = np.load(f'./data/{dataset}/test_indices.npy')\n",
    "            # Use the indices to create new datasets\n",
    "            train_dataset = Subset(transformed_dataset, train_indices)\n",
    "            val_dataset = Subset(transformed_dataset, val_indices)\n",
    "            # test_dataset = Subset(transformed_dataset, test_indices)\n",
    "             # Use the indices to create new test datasets\n",
    "            eval_dataset = Subset(test_dataset, test_indices)\n",
    "        else:\n",
    "            # Split the dataset into train, validation, and test sets\n",
    "            train_dataset, val_dataset, test_dataset = random_split(\n",
    "                transformed_dataset, [train_size, val_size, test_size]\n",
    "            )\n",
    "            np.save(f'./data/{dataset}/train_indices.npy', train_dataset.indices)\n",
    "            np.save(f'./data/{dataset}/val_indices.npy', val_dataset.indices)\n",
    "            np.save(f'./data/{dataset}/test_indices.npy', test_dataset.indices)\n",
    "            # Use the indices to create new test datasets\n",
    "            eval_dataset = Subset(test_dataset, test_dataset.indices)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn)\n",
    "    eval_loader = DataLoader(eval_dataset, batch_size=batch_size, shuffle=False, collate_fn=custom_collate_fn)\n",
    "    return input_channel, input_shape, train_loader, val_loader, eval_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b47cdbd-82f3-47a0-9e15-a8e0af5891cb",
   "metadata": {},
   "source": [
    "Experiment pipeline for NP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50ee69e7-b091-4ece-8b4e-1ded41e0062c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline_ci_np(train_loader, val_loader, \n",
    "                    test_loader, epochs, val_every_epoch, config, device, num_runs=3, print_freq=2):\n",
    "    test_losses = []\n",
    "    # partial_percent = config['partial_percent']\n",
    "    experiment_id = int(time.time())\n",
    "    best_val_loss_NP = float('inf')\n",
    "    # config['experiment_id'] = experiment_id\n",
    "    experiment_id = config['experiment_id']\n",
    "\n",
    "    r_dim = np_config.r_dim\n",
    "    h_dim = np_config.h_dim\n",
    "    z_dim = np_config.z_dim\n",
    "    lr = config['best_lr']\n",
    "\n",
    "    # Create storage directory and store the experiment configuration\n",
    "    if not os.path.exists(f'./history/neural_processes/{experiment_id}'):\n",
    "        os.makedirs(f'./history/neural_processes/{experiment_id}')\n",
    "    with open(f\"./history/neural_processes/{experiment_id}/config_np.json\", \"w\") as outfile: \n",
    "        json.dump(config, outfile)\n",
    "        \n",
    "    global_val_loss = float('inf')\n",
    "    \n",
    "    for run in range(num_runs):\n",
    "        count = 0\n",
    "        GP_test_losses = []\n",
    "        \n",
    "        early_stopping = EarlyStoppingCallback(patience=5, min_delta=0.001)\n",
    "        model = NeuralProcessImg(r_dim, z_dim, h_dim).to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        np_trainer = NeuralProcessTrainer(device, model, optimizer, early_stopping, experiment_id, print_freq=print_freq)          \n",
    "\n",
    "        np_trainer.train(train_loader, val_loader, epochs)\n",
    "        if np_trainer.best_val_loss <= global_val_loss:\n",
    "            global_val_loss = np_trainer.best_val_loss \n",
    "            torch.save(np_trainer.neural_process.state_dict(), f'./history/neural_processes/{experiment_id}' + f'/best_{args.dataset}_np.pt')\n",
    "        count += 1\n",
    "\n",
    "    return experiment_id\n",
    "\n",
    "\n",
    "# Function to run the pipeline and save data\n",
    "def run_and_save_pipeline_np(train_loader, val_loader, test_loader, epochs, val_every_epoch, config, num_runs, device):\n",
    "    test_partial_percents = [0.25, 0.5, 0.75, 1]\n",
    "    test_losses = []\n",
    "    r2_list = []\n",
    "    table = []\n",
    "    table.append(['Dataset', 'Mode', 'd', 'n_pins', 'LR', 'PLP', 'MSE error', 'R2'])\n",
    "    experiment_id = run_pipeline_ci_np(train_loader, val_loader, \n",
    "                    test_loader, epochs, val_every_epoch, config, device, num_runs)\n",
    "    # Run final testing\n",
    "    model = NeuralProcessImg(r_dim, z_dim, h_dim).to(device)\n",
    "    # MSE\n",
    "    model.load_state_dict(torch.load(f'./history/neural_processes/{experiment_id}/best_{args.dataset}_np.pt'))\n",
    "    for partial_percent in test_partial_percents:\n",
    "        test_loss, r2 = evaluate_np(model, test_loader, device, partial_percent=partial_percent)\n",
    "        print(f\"pp: {partial_percent} MSE loss: {test_loss} R2 score: {r2}\")\n",
    "        table.append([args.dataset, args.mode, args.d, args.n_pins, config['best_lr'], partial_percent, test_loss, r2])\n",
    "        test_losses.append(test_loss)\n",
    "        r2_list.append(r2)\n",
    "    table = tabulate(table, headers='firstrow', tablefmt='fancy_grid', showindex=True)\n",
    "    print(table)\n",
    "    with open('./history/neural_processes/table.txt', 'a') as f:\n",
    "        f.write('\\n')\n",
    "        f.write(table + '\\n')  # Add a newline character after writing the table\n",
    "        f.write('\\n')  # Add an additional newline character for separation\n",
    "    print(\"saved\")\n",
    "    return test_losses, r2_list, experiment_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6960252-51eb-4049-863d-1932996a16da",
   "metadata": {},
   "source": [
    "Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbc10563-9861-4f04-bfda-77ce67b76c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.dataset = \"Synthetic\"\n",
    "        self.n = 1000\n",
    "        self.mode = \"random\"\n",
    "        self.feature = \"AE\"\n",
    "        self.modality = \"PS-RGBNIR\"\n",
    "        self.d = 10\n",
    "        self.n_pins = 100\n",
    "        self.r = 3\n",
    "        self.partial_percent = 0.8\n",
    "        self.epochs = 1000\n",
    "        self.batch_size = 50\n",
    "        self.learning_rate = 1e-4\n",
    "        self.val_every_epoch = 10\n",
    "        self.num_runs = 1\n",
    "        self.seed = 4\n",
    "\n",
    "class NP_config:\n",
    "    def __init__(self):\n",
    "        self.r_dim = 512\n",
    "        self.h_dim = 512\n",
    "        self.z_dim = 512\n",
    "        self.lr = 4e-5\n",
    "        self.epochs = 100\n",
    "\n",
    "np_config = NP_config()\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e35ab4e-0d87-40fc-9591-8b1be2640378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a random seed for PyTorch\n",
    "seed = 4  # You can use any integer value as the seed\n",
    "torch.manual_seed(seed)\n",
    "# Set a random seed for NumPy (if you're using NumPy operations)\n",
    "np.random.seed(seed)\n",
    "\n",
    " # Choose datasets\n",
    "dataset = args.dataset \n",
    "n = args.n\n",
    "mode = args.mode\n",
    "d = args.d\n",
    "n_pins = args.n_pins\n",
    "r = args.r\n",
    "partial_percent = args.partial_percent\n",
    "\n",
    "# Set your hyperparameters\n",
    "epochs = args.epochs\n",
    "batch_size = args.batch_size\n",
    "learning_rate = args.learning_rate\n",
    "val_every_epoch = args.val_every_epoch\n",
    "num_runs = args.num_runs\n",
    "   \n",
    "# config = {}\n",
    "config = vars(args)\n",
    "config[\"experiment_id\"] = 0\n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fe02c1-8dfb-45ea-a4df-559a881a5cb3",
   "metadata": {},
   "source": [
    "Loading datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "247eb8c4-b9ee-4857-a528-2867abb87b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_channel, input_shape, train_loader, val_loader, eval_loader = data_prepare(config, seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bda165f-142a-47fb-9839-d3e90ad8c1e2",
   "metadata": {},
   "source": [
    "Train neural processes with custom learning rates and output the summary table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87002a7-af7a-4598-b786-d6796423bfea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Avg_loss: 1425.687\n",
      "Epoch: 0, Val_loss 556.013\n",
      "Epoch: 1, Avg_loss: 395.175\n",
      "Epoch: 2, Avg_loss: 364.217\n",
      "Epoch: 2, Val_loss 363.040\n",
      "Epoch: 3, Avg_loss: 361.702\n",
      "Epoch: 4, Avg_loss: 360.983\n",
      "Epoch: 4, Val_loss 360.914\n",
      "Epoch: 5, Avg_loss: 360.404\n",
      "Epoch: 6, Avg_loss: 360.191\n",
      "Epoch: 6, Val_loss 360.526\n",
      "Epoch: 7, Avg_loss: 359.719\n",
      "Epoch: 8, Avg_loss: 359.144\n",
      "Epoch: 8, Val_loss 357.113\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "# define the NP \n",
    "r_dim, z_dim, h_dim = np_config.r_dim, np_config.z_dim, np_config.h_dim\n",
    "neural_processes = NeuralProcessImg(r_dim, z_dim, h_dim).to(device)\n",
    "optimizer = torch.optim.Adam(neural_processes.parameters(), learning_rate)\n",
    "config['best_lr'] = 1e-3\n",
    "# Run and save the pipeline data\n",
    "test_losses, r2, experiment_id = run_and_save_pipeline_np(train_loader, val_loader, eval_loader,\\\n",
    "                                           epochs, val_every_epoch, config, num_runs, device)\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"Time elapsed:\", elapsed_time, \"seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "npp",
   "language": "python",
   "name": "npp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
