{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3836b3f8-f993-415f-b442-f772829e47e6",
   "metadata": {},
   "source": [
    "Neural Point Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca94db99-f1a0-448f-91d0-d6b98ff5bd87",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from skimage import io, transform\n",
    "from functools import lru_cache\n",
    "from tools.plot_utils import visualize_pins, plot_label_pin, plot_all, plot_and_save, plot_loss\n",
    "from tools.data_utils import *\n",
    "from tools.losses import NPPLoss\n",
    "from tools.models import Autoencoder\n",
    "from tools.optimization import EarlyStoppingCallback, train_model, evaluate_model\n",
    "import matplotlib.pyplot as plt\n",
    "# from torch_lr_finder import LRFinder\n",
    "import time\n",
    "from tools.models import *\n",
    "from torch.utils.data import Subset\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68301657-0fbc-46fc-8a55-2352a41197c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.1+cu121'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa53412-0c89-4617-9b7e-efffa9b63834",
   "metadata": {},
   "source": [
    "# Dataset and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "000f94a1-69f9-488f-840c-0a0079b1bb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a random seed for PyTorch\n",
    "seed = 4  # You can use any integer value as the seed\n",
    "torch.manual_seed(seed)\n",
    "# Set a random seed for NumPy (if you're using NumPy operations)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "770be29d-6a82-4394-96c9-8c37bae5e821",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"PinMNIST\"\n",
    "feature_extracted = False\n",
    "learn_kernel = False\n",
    "n = 10\n",
    "mesh = False\n",
    "d = 10\n",
    "n_pins = 100\n",
    "fixed_pins = True\n",
    "r = 3\n",
    "d1, d2 = 28, 28\n",
    "\n",
    "partial_label_GP = False\n",
    "partial_percent = 0.5\n",
    "\n",
    "if feature_extracted:\n",
    "    folder = f\"{dataset}_ddpm\"\n",
    "else:\n",
    "    folder = f\"{dataset}\"\n",
    "\n",
    "if dataset == \"PinMNIST\":\n",
    "    if mesh:\n",
    "        data_folder = f\"./data/{folder}/mesh_{d}step\"\n",
    "        config['n_pins'] = (28 // d + 1) ** 2\n",
    "    else:\n",
    "        data_folder = f\"./data/{folder}/random_{n_pins}pins\"\n",
    "elif dataset == \"Synthetic\":\n",
    "    folder += \"/28by28pixels_1000images_123456seed\"\n",
    "    if mesh:\n",
    "        data_folder = f\"./data/{folder}/mesh_{d}step_pins\"\n",
    "        config['n_pins'] = (28 // d + 1) ** 2\n",
    "    else:\n",
    "        data_folder = f\"./data/{folder}/random_{n_pins}pins\"\n",
    "elif dataset == \"Building\":\n",
    "    if mesh:\n",
    "        data_folder = f\"./data/{folder}/mesh_{d}_step\"\n",
    "    else:\n",
    "        data_folder = f\"./data/{folder}/random_n_pins_{n_pins}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ac50a0b-10c2-49d1-a2fc-ed7826a04c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset == \"Building\":\n",
    "    resize = Resize100\n",
    "else:\n",
    "    resize = Resize\n",
    "        \n",
    "\n",
    "# Define batch size\n",
    "batch_size = 16\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    images = [sample['image'] for sample in batch]\n",
    "    pins = [sample['pins'] for sample in batch]\n",
    "    outputs = [sample['outputs'] for sample in batch]\n",
    "\n",
    "    return {\n",
    "        'image': torch.stack(images, dim=0),\n",
    "        'pins': pins,\n",
    "        'outputs': outputs}\n",
    "\n",
    "transform = transforms.Compose([\n",
    "        ToTensor(),  # Convert to tensor (as you were doing)\n",
    "        resize()  # Resize to 100x100\n",
    "    ])\n",
    "\n",
    "\n",
    "modality = \"PS-RGBNIR-SAR\"\n",
    "if dataset == \"Building\":\n",
    "    root_dir = \"/work/USACE_KRI/Project_1/spacenet/aoi_11_rotterdam/train/train/AOI_11_Rotterdam/\"+modality+\"/\"\n",
    "else:\n",
    "    root_dir=f\"./data/{folder}/images/\"\n",
    "    \n",
    "transformed_dataset = PinDataset(csv_file=f\"{data_folder}/pins.csv\",\n",
    "                                 root_dir=root_dir, modality=modality,\n",
    "                                 transform=transform)\n",
    "\n",
    "\n",
    "dataset_size = len(transformed_dataset)\n",
    "train_size = int(0.7 * dataset_size)\n",
    "val_size = int(0.10 * dataset_size)\n",
    "test_size = dataset_size - train_size - val_size\n",
    "\n",
    "\n",
    "if os.path.exists(f\"./data/{dataset}/train_indices.npy\"):\n",
    "    train_indices = np.load(f'./data/{dataset}/train_indices.npy')\n",
    "    val_indices = np.load(f'./data/{dataset}/val_indices.npy')\n",
    "    test_indices = np.load(f'./data/{dataset}/test_indices.npy')\n",
    "    # Use the indices to create new datasets\n",
    "    train_dataset = Subset(transformed_dataset, train_indices)\n",
    "    val_dataset = Subset(transformed_dataset, val_indices)\n",
    "    test_dataset = Subset(transformed_dataset, test_indices)\n",
    "else:\n",
    "    # Split the dataset into train, validation, and test sets\n",
    "    train_dataset, val_dataset, test_dataset = random_split(\n",
    "        transformed_dataset, [train_size, val_size, test_size]\n",
    "    )\n",
    "    np.save(f'./data/{dataset}/train_indices.npy', train_dataset.indices)\n",
    "    np.save(f'./data/{dataset}/val_indices.npy', val_dataset.indices)\n",
    "    np.save(f'./data/{dataset}/test_indices.npy', test_dataset.indices)\n",
    "\n",
    "# Create your DataLoader with the custom_collate_fn\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=custom_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "371fb7d3-e158-435e-94fa-0118817950ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for batch in train_loader:\n",
    "    images = batch['image'].to(device) # get RGB instead of RGBA\n",
    "    pins = batch['pins']\n",
    "    outputs = batch['outputs']\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e347c68-51b9-4be3-b854-11db4b1bfb64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1, 28, 28])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ce377de-650d-4e4c-9520-437fc8752263",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 1-dimensional, but 2 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m j\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m\n\u001b[1;32m      2\u001b[0m sample_img \u001b[38;5;241m=\u001b[39m images[j]\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[0;32m----> 3\u001b[0m count_image \u001b[38;5;241m=\u001b[39m \u001b[43mplot_label_pin\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_img\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpins\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m count_all_image \u001b[38;5;241m=\u001b[39m plot_all(sample_img[\u001b[38;5;241m0\u001b[39m], r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m      5\u001b[0m fig, axes \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n",
      "File \u001b[0;32m/work/DNAL/shi.cheng/NPP/Satellite_Fusion/tools/plot_utils.py:58\u001b[0m, in \u001b[0;36mplot_label_pin\u001b[0;34m(image, pins, labels)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pin, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(pins, labels):\n\u001b[1;32m     57\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m pin\n\u001b[0;32m---> 58\u001b[0m     \u001b[43mlabeled_image\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m label\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Plot the resulting image\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# plt.imshow(labeled_image, cmap='gray')\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# plt.title(\"Image with Labels Assigned to Pins\")\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# plt.show()\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m labeled_image\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array: array is 1-dimensional, but 2 were indexed"
     ]
    }
   ],
   "source": [
    "j=6\n",
    "sample_img = images[j].squeeze().detach().cpu()\n",
    "count_image = plot_label_pin(sample_img[0], pins[j], outputs[j])\n",
    "count_all_image = plot_all(sample_img[0], r=3)\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "sample_img = sample_img[4:]\n",
    "img = np.transpose(sample_img, (1, 2, 0))\n",
    "print(sample_img.shape, img.shape)\n",
    "im0 = axes[0].imshow(img)\n",
    "# Plot the sample_img in the first subplot\n",
    "# im0 = axes[0].imshow(sample_img)\n",
    "axes[0].set_title('Sample Image')\n",
    "fig.colorbar(im0, ax=axes[0], fraction=0.046, pad=0.04)  # Add colorbar\n",
    "\n",
    "# Plot the count_image in the second subplot\n",
    "im1 = axes[1].imshow(count_image)\n",
    "axes[1].set_title('Count Image')\n",
    "fig.colorbar(im1, ax=axes[1], fraction=0.046, pad=0.04)  # Add colorbar\n",
    "\n",
    "# Plot the count_all_image in the third subplot\n",
    "im2 = axes[2].imshow(count_all_image)\n",
    "axes[2].set_title('Count All Image')\n",
    "fig.colorbar(im2, ax=axes[2], fraction=0.046, pad=0.04)  # Add colorbar\n",
    "\n",
    "# Add spacing between subplots\n",
    "plt.tight_layout()\n",
    "# Display the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c306e1-061a-47bc-8584-46ad8e08ee8b",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa39fe1f-e261-4bc3-b8b4-75fc9ca989a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLRFinder:\n",
    "    def __init__(self, model, criterion, optimizer, device='cuda'):\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "        self.history = {'lr': [], 'loss': []}\n",
    "\n",
    "    def find_lr(self, train_loader, start_lr=1e-4, end_lr=0.1, num_iter=20,smooth_f=0.05):\n",
    "        model = self.model.to(self.device)\n",
    "        criterion = self.criterion\n",
    "        optimizer = self.optimizer\n",
    "        device = self.device\n",
    "        model.train()\n",
    "\n",
    "        lr_step = (end_lr / start_lr) ** (1 / num_iter)\n",
    "        lr = start_lr\n",
    "\n",
    "        for iteration in range(num_iter):\n",
    "            optimizer.param_groups[0]['lr'] = lr\n",
    "\n",
    "            total_loss = 0.0\n",
    "            for batch in train_loader:\n",
    "                x_train = batch['image'][:, :input_channel, :, :].to(device)\n",
    "                p_train = [tensor.to(device) for tensor in batch['pins']]\n",
    "                y_train = [tensor.to(device) for tensor in batch['outputs']]\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(x_train.float())\n",
    "                loss = criterion(y_train, outputs, p_train)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            avg_loss = total_loss / len(train_loader)\n",
    "            self.history['lr'].append(lr)\n",
    "            self.history['loss'].append(avg_loss)\n",
    "\n",
    "            lr *= lr_step\n",
    "            \n",
    "    def plot_lr_finder(self):\n",
    "        plt.plot(self.history['lr'], self.history['loss'])\n",
    "        plt.xscale('log')  # Use a logarithmic scale for better visualization\n",
    "        plt.xlabel('Learning Rate')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Learning Rate Finder Curve')\n",
    "        plt.show()\n",
    "        \n",
    "    def find_best_lr(self, skip_start=3, skip_end=3):\n",
    "        # Find the index of the minimum loss in the specified range\n",
    "        min_loss_index = skip_start + np.argmin(self.history['loss'][skip_start:-skip_end])\n",
    "\n",
    "        # Output the learning rate corresponding to the minimum loss\n",
    "        best_lr = self.history['lr'][min_loss_index]\n",
    "        return best_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11e9111c-3a3f-498b-9574-56626971725a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline_ci(sigmas, num_kernels_encoder, num_kernels_decoder, train_loader, val_loader,\n",
    "                    test_loader, input_channel, epochs, val_every_epoch, config, device, num_runs=3, exp_name=\"\"):\n",
    "    GP_test_losses_npp_true = []\n",
    "    test_losses_npp_true = []\n",
    "    test_losses_npp_false = []\n",
    "    GP_r2_losses_npp_true = []\n",
    "    r2_losses_npp_true = []\n",
    "    r2_losses_npp_false = []\n",
    "    partial_percent = config['partial_percent']\n",
    "    experiment_id = int(time.time())\n",
    "    best_val_loss_MSE = float('inf')\n",
    "    best_val_loss_NPP = float('inf')\n",
    "    best_sigma_NPP = float('inf')\n",
    "    config['experiment_id'] = experiment_id\n",
    "    deeper = config['deeper']\n",
    "    manual_lr = config['manual_lr']\n",
    "    learning_rates = config['best_lrs']\n",
    "    losses = {}\n",
    "\n",
    "    # Create storage directory and store the experiment configuration\n",
    "    if not os.path.exists(f'./history/{exp_name}/{experiment_id}'):\n",
    "        os.makedirs(f'./history/{exp_name}/{experiment_id}')\n",
    "    with open(f\"./history/{exp_name}/{experiment_id}/config.json\", \"w\") as outfile:\n",
    "        json.dump(config, outfile)\n",
    "\n",
    "    for run in range(num_runs):\n",
    "        count = 0\n",
    "        test_losses_vs_sigma_npp_true = []\n",
    "        R2_losses_vs_sigma_npp_true = []\n",
    "        GP_test_losses_vs_sigma_npp_true = []\n",
    "        GP_R2_losses_vs_sigma_npp_true = []\n",
    "        for sigma in sigmas:\n",
    "            early_stopping = EarlyStoppingCallback(patience=15, min_delta=0.001)\n",
    "            autoencoder = Autoencoder(num_kernels_encoder, num_kernels_decoder, input_channel=input_channel, deeper=deeper).to(device)\n",
    "            lr = learning_rates[count][1]\n",
    "            optimizer = optim.Adam(autoencoder.parameters(), lr=lr)\n",
    "            print(f\"training start for sigma:{sigma}\")\n",
    "            if sigma == 0:\n",
    "                # run plain\n",
    "                \n",
    "                criterion = NPPLoss(identity=True).to(device)\n",
    "                model, train_losses, val_losses, best_val_loss = train_model(autoencoder, train_loader, val_loader,\n",
    "                                                                             input_channel, epochs, \\\n",
    "                                                                             val_every_epoch, lr,\n",
    "                                                                             criterion, optimizer, device, early_stopping,\n",
    "                                                                             experiment_id, exp_name, best_val_loss_MSE, manual_lr, sigma=0)\n",
    "                losses[f\"MSE_run{run}_train\"] = train_losses\n",
    "                losses[f\"MSE_run{run}_val\"] = val_losses\n",
    "                if best_val_loss < best_val_loss_MSE:\n",
    "                    best_val_loss_MSE = best_val_loss\n",
    "\n",
    "                test_loss_npp_false, r2_loss_npp_false = evaluate_model(autoencoder, test_loader, input_channel, device,\n",
    "                                                                        partial_label_GP=False, partial_percent=partial_percent)\n",
    "                print(f\"MSE Test loss:{test_loss_npp_false:.3f}\")\n",
    "                print(f\"R2 Test loss:{r2_loss_npp_false:.3f}\")\n",
    "                test_losses_npp_false.append(test_loss_npp_false)\n",
    "                r2_losses_npp_false.append(r2_loss_npp_false)        \n",
    "            else:\n",
    "                # run NPP\n",
    "                criterion = NPPLoss(identity=False, sigma=sigma).to(device)               \n",
    "                \n",
    "                model, train_losses, val_losses, best_val_loss = train_model(autoencoder, train_loader, val_loader,\n",
    "                                                                             input_channel, epochs, \\\n",
    "                                                                             val_every_epoch, lr,\n",
    "                                                                             criterion, optimizer, device, early_stopping,\n",
    "                                                                             experiment_id, exp_name, best_val_loss_NPP, manual_lr, sigma=sigma)\n",
    "                losses[f\"NPP_run{run}_sigma{sigma}_train\"] = train_losses\n",
    "                losses[f\"NPP_run{run}_sigma{sigma}_val\"] = val_losses\n",
    "                if best_val_loss < best_val_loss_NPP:\n",
    "                    best_val_loss_NPP = best_val_loss\n",
    "                    best_sigma_NPP = sigma\n",
    "\n",
    "                test_loss, r2_loss = evaluate_model(autoencoder, test_loader, input_channel, device,\n",
    "                                                    partial_label_GP=False, partial_percent=partial_percent)\n",
    "                GP_test_loss, GP_r2_loss = evaluate_model(autoencoder, test_loader, input_channel, device,\n",
    "                                                          partial_label_GP=True, partial_percent=partial_percent)\n",
    "                print(f\"NPP sigma={sigma} Test loss:{test_loss:.3f}, R2 loss:{r2_loss:.3f}, GP Test loss:{GP_test_loss:.3f}\"\n",
    "                      f\", GP R2 loss:{GP_r2_loss:.3f}\")\n",
    "                test_losses_vs_sigma_npp_true.append(test_loss)\n",
    "                R2_losses_vs_sigma_npp_true.append(r2_loss)\n",
    "                GP_test_losses_vs_sigma_npp_true.append(GP_test_loss)\n",
    "                GP_R2_losses_vs_sigma_npp_true.append(GP_r2_loss)\n",
    "            count += 1\n",
    "\n",
    "        test_losses_npp_true.append(test_losses_vs_sigma_npp_true)\n",
    "        GP_test_losses_npp_true.append(GP_test_losses_vs_sigma_npp_true)\n",
    "        r2_losses_npp_true.append(R2_losses_vs_sigma_npp_true)\n",
    "        GP_r2_losses_npp_true.append(GP_R2_losses_vs_sigma_npp_true)\n",
    "    with open(f\"./history/{exp_name}/{experiment_id}/losses.json\", \"w\") as outfile:\n",
    "        json.dump(losses, outfile)\n",
    "    return GP_test_losses_npp_true, test_losses_npp_true, test_losses_npp_false, r2_losses_npp_false, r2_losses_npp_true, GP_r2_losses_npp_true, best_sigma_NPP, experiment_id\n",
    "\n",
    "    \n",
    "# Function to run the pipeline and save data\n",
    "def run_and_save_pipeline(sigmas, num_kernels_encoder, num_kernels_decoder, train_loader, val_loader, test_loader,\n",
    "                          input_channel, epochs, val_every_epoch, config, num_runs, exp_name, device):\n",
    "    # Run the pipeline\n",
    "    GP_test_loss_npp_true, test_loss_npp_true, test_loss_npp_false, r2_loss_npp_false, r2_losses_npp_true, GP_r2_losses_npp_true, best_sigma_NPP, experiment_id = run_pipeline_ci(\n",
    "        sigmas, num_kernels_encoder, num_kernels_decoder, train_loader, val_loader, test_loader, input_channel, epochs,\n",
    "        val_every_epoch, config, device, num_runs, exp_name)\n",
    "    partial_percent = config['partial_percent']\n",
    "    # Run final testing\n",
    "    autoencoder = Autoencoder(num_kernels_encoder, num_kernels_decoder, input_channel=input_channel, deeper=config[\"deeper\"]).to(device)\n",
    "    if sigmas[0] == 0:\n",
    "        # MSE\n",
    "        autoencoder.load_state_dict(torch.load(f'./history/{exp_name}/{experiment_id}/best_model_MSE.pth'))\n",
    "        best_MSE_test_loss, best_R2_test_loss = evaluate_model(autoencoder, test_loader, input_channel, device,\n",
    "                                                               partial_label_GP=False,\n",
    "                                                               partial_percent=partial_percent)\n",
    "        f = open(f\"./history/{exp_name}/{experiment_id}/results.txt\", \"w\")\n",
    "        f.write(f\"Results {experiment_id}:\\n MSE: {best_MSE_test_loss}, R2: {best_R2_test_loss} \")\n",
    "        f.close()\n",
    "        print(\"metrics saved\")\n",
    "    else:\n",
    "        # NPP\n",
    "        autoencoder.load_state_dict(torch.load(f'./history/{exp_name}/{experiment_id}/best_model_NPP.pth'))\n",
    "        best_NPP_test_loss, best_NPP_R2_test_loss = evaluate_model(autoencoder, test_loader, input_channel, device,\n",
    "                                                                   partial_label_GP=False,\n",
    "                                                                   partial_percent=partial_percent)\n",
    "        GP_best_NPP_test_loss, GP_best_NPP_r2_test_loss = evaluate_model(autoencoder, test_loader, input_channel, device,\n",
    "                                                                         partial_label_GP=True,\n",
    "                                                                         partial_percent=partial_percent)\n",
    "        f = open(f\"./history/{exp_name}/{experiment_id}/results.txt\", \"w\")\n",
    "        f.write(f\"| NPP (sigma {best_sigma_NPP}): {best_NPP_test_loss}, R2: {best_NPP_R2_test_loss}; GP: {GP_best_NPP_test_loss}, R2: {GP_best_NPP_r2_test_loss}\")\n",
    "        f.close()\n",
    "        print(\"metrics saved\")\n",
    "        \n",
    "    print(\"start saving losses!\")\n",
    "    # Save losses\n",
    "    save_loss(test_loss_npp_true, f'./history/{exp_name}/{experiment_id}/test_loss_npp_true.npy')\n",
    "    save_loss(test_loss_npp_false, f'./history/{exp_name}/{experiment_id}/test_loss_npp_false.npy')\n",
    "    save_loss(GP_test_loss_npp_true, f'./history/{exp_name}/{experiment_id}/GP_test_loss_npp_true.npy')\n",
    "    # Save r2 scores\n",
    "    save_loss(r2_loss_npp_false, f'./history/{exp_name}/{experiment_id}/r2_loss_npp_false.npy')\n",
    "    save_loss(r2_losses_npp_true, f'./history/{exp_name}/{experiment_id}/r2_losses_npp_true.npy')\n",
    "    save_loss(GP_r2_losses_npp_true, f'./history/{exp_name}/{experiment_id}/GP_r2_losses_npp_true.npy')\n",
    "    \n",
    "    return (GP_test_loss_npp_true, test_loss_npp_true, test_loss_npp_false), (r2_loss_npp_false, r2_losses_npp_true, GP_r2_losses_npp_true), experiment_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c681ad7-b14c-4d02-ad7f-da35693469d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Run and save the pipeline data\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m loss_vs_sigma_data \u001b[38;5;241m=\u001b[39m \u001b[43mrun_and_save_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43msigmas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_kernels_encoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_kernels_decoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43minput_channel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_every_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_lrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearn_kernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_runs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Plot and save the plot using the saved data\u001b[39;00m\n\u001b[1;32m     25\u001b[0m plot_and_save(loss_vs_sigma_data, sigmas, dataset, learning_rate)\n",
      "Cell \u001b[0;32mIn[11], line 98\u001b[0m, in \u001b[0;36mrun_and_save_pipeline\u001b[0;34m(sigmas, num_kernels_encoder, num_kernels_decoder, train_loader, val_loader, test_loader, input_channel, epochs, val_every_epoch, config, num_runs, exp_name, device)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_and_save_pipeline\u001b[39m(sigmas, num_kernels_encoder, num_kernels_decoder, train_loader, val_loader, test_loader,\n\u001b[1;32m     96\u001b[0m                           input_channel, epochs, val_every_epoch, config, num_runs, exp_name, device):\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;66;03m# Run the pipeline\u001b[39;00m\n\u001b[0;32m---> 98\u001b[0m     GP_test_loss_npp_true, test_loss_npp_true, test_loss_npp_false, r2_loss_npp_false, r2_losses_npp_true, GP_r2_losses_npp_true, best_sigma_NPP, experiment_id \u001b[38;5;241m=\u001b[39m \u001b[43mrun_pipeline_ci\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m        \u001b[49m\u001b[43msigmas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_kernels_encoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_kernels_decoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_channel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m        \u001b[49m\u001b[43mval_every_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_runs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexp_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m     partial_percent \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpartial_percent\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;66;03m# Run final testing\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 9\u001b[0m, in \u001b[0;36mrun_pipeline_ci\u001b[0;34m(sigmas, num_kernels_encoder, num_kernels_decoder, train_loader, val_loader, test_loader, input_channel, epochs, val_every_epoch, config, device, num_runs, exp_name)\u001b[0m\n\u001b[1;32m      7\u001b[0m r2_losses_npp_true \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      8\u001b[0m r2_losses_npp_false \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 9\u001b[0m partial_percent \u001b[38;5;241m=\u001b[39m \u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpartial_percent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     10\u001b[0m experiment_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(time\u001b[38;5;241m.\u001b[39mtime())\n\u001b[1;32m     11\u001b[0m best_val_loss_MSE \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "# Generate a unique experiment_id using a timestamp\n",
    "  # Using timestamp as experiment_id\n",
    "\n",
    "# Set your hyperparameters\n",
    "# dataset = \"MNIST\"\n",
    "input_channel = 1 if dataset == \"PinMNIST\" else 3\n",
    "epochs = 200\n",
    "sigmas = [0.1, 0.2, 0.5, 1, 2]  # Set the sigma values you want to test\n",
    "# best_lrs = [0.05, 0.05, 0.05, 0.05, 0.05, 0.001] #MNIST\n",
    "best_lrs = [0.1, 0.001, 0.001, 0.001, 0.001, 0.001] #Synthetic\n",
    "num_kernels_encoder = [64, 32]\n",
    "num_kernels_decoder = [64]\n",
    "# learning_rate = 0.01\n",
    "val_every_epoch = 5\n",
    "num_runs = 1\n",
    "learn_kernel = False\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "config = {\n",
    "    \"dataset\": \"PinMNIST\",\n",
    "    \"modality\": \"PS-RGBNIR\",\n",
    "    \"feature\": \"AE\",\n",
    "    \"mode\": \"mesh\",\n",
    "    \"n\": 100,\n",
    "    \"d\": 10,\n",
    "    \"n_pins\": 500,\n",
    "    \"partial_percent\": 0.00,\n",
    "    \"r\": 3,\n",
    "    \"epochs\": 200,\n",
    "    \"batch_size\": 64,\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"val_every_epoch\": 5,\n",
    "    \"num_runs\": 3,\n",
    "    \"manual_lr\": False,\n",
    "    \"sigmas\": [0],\n",
    "    \"num_encoder\": [64, 32],\n",
    "    \"num_decoder\": [64],\n",
    "    \"deeper\": False,\n",
    "    \"experiment_name\": None\n",
    "}\n",
    "\n",
    "# Run and save the pipeline data\n",
    "loss_vs_sigma_data = run_and_save_pipeline(sigmas, num_kernels_encoder, num_kernels_decoder, train_loader, val_loader, test_loader,\n",
    "                          input_channel, epochs, val_every_epoch, config, num_runs, exp_name, device)\n",
    "\n",
    "# Plot and save the plot using the saved data\n",
    "plot_and_save(loss_vs_sigma_data, sigmas, dataset, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2eb83af-4204-4c06-a574-1ac9916a914e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53b7484-3290-43cb-a85e-39cca0f9e140",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_vs_sigma_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527b04d7-2308-48e9-9a75-d8071dd3d0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_loss_npp_false = [test_loss_npp_false for i in range((len(sigmas)))]\n",
    "# test_loss_npp_true.pop(1)\n",
    "test_loss_npp_true, test_loss_npp_false = load_data('./history/test_loss_npp_true.npy'), load_data('./history/test_loss_npp_false.npy')\n",
    "test_loss_npp_true, test_loss_npp_false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8bf364-d010-40de-ad6c-ce837b5dee2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_and_save(loss_vs_sigma_data, sigmas, dataset, learning_rate, dataset, model_name=\"Auto encoder\"):\n",
    "    # Unpack the data\n",
    "    test_loss_npp_true, test_loss_npp_false = loss_vs_sigma_data\n",
    "    test_loss_npp_false = [test_loss_npp_false for i in range(len(sigmas))]\n",
    "\n",
    "    # Calculate mean and confidence intervals for NPP=True runs\n",
    "    mean_test_loss_npp_true = np.mean(test_loss_npp_true, axis=0)\n",
    "    ci_test_loss_npp_true = 1.96 * np.std(test_loss_npp_true, axis=0) / np.sqrt(len(test_loss_npp_true))\n",
    "\n",
    "    # Duplicate NPP=False values for plotting\n",
    "    mean_test_loss_npp_false = np.mean(test_loss_npp_false, axis=1)\n",
    "    ci_test_loss_npp_false = 1.96 * np.std(test_loss_npp_false, axis=1) / np.sqrt(len(test_loss_npp_false))\n",
    "\n",
    "    # Plot mean and confidence intervals for NPP=True\n",
    "    plt.plot(sigmas, mean_test_loss_npp_true, marker='o', label='NPP=True', color='blue')\n",
    "\n",
    "    # Plot mean and confidence intervals for duplicated NPP=False\n",
    "    plt.plot(sigmas, mean_test_loss_npp_false, color='red', linestyle='--', label='NPP=False')\n",
    "\n",
    "    # Fill between for NPP=True with blue color\n",
    "    plt.fill_between(sigmas, mean_test_loss_npp_true - ci_test_loss_npp_true, mean_test_loss_npp_true + ci_test_loss_npp_true, color='blue', alpha=0.2)\n",
    "\n",
    "    # Fill between for NPP=False with red color\n",
    "    plt.fill_between(sigmas, mean_test_loss_npp_false - ci_test_loss_npp_false, mean_test_loss_npp_false + ci_test_loss_npp_false, color='red', alpha=0.2)\n",
    "\n",
    "    plt.xlabel('Sigma')\n",
    "    plt.ylabel('Test Loss')\n",
    "    plt.title(f'Test Loss vs. Sigma:{dataset} dataset with {model_name}')\n",
    "    plt.legend()\n",
    "\n",
    "    # Create a directory to save the results if it doesn't exist\n",
    "    results_dir = './results'\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "    # Generate a filename based on parameters in the title\n",
    "    filename = f\"test_loss_vs_sigma_{dataset}_{model_name}_lr_{learning_rate}.png\"\n",
    "    filepath = os.path.join(results_dir, filename)\n",
    "\n",
    "    # Save the plot\n",
    "    plt.savefig(filepath)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# plot_and_save(loss_vs_sigma_data, sigmas, dataset, learning_rate)\n",
    "\n",
    "# Plot and save the plot using the saved data\n",
    "plot_and_save(loss_vs_sigma_data, sigmas, dataset, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9d2e71-c6c2-4077-9b60-f4913207dbd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "satellite",
   "language": "python",
   "name": "satellite"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
