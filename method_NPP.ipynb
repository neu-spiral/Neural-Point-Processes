{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3836b3f8-f993-415f-b442-f772829e47e6",
   "metadata": {},
   "source": [
    "Neural Point Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca94db99-f1a0-448f-91d0-d6b98ff5bd87",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from skimage import io, transform\n",
    "from functools import lru_cache\n",
    "from tools.plot_utils import visualize_pins, plot_label_pin, plot_all, plot_and_save, plot_loss\n",
    "from tools.data_utils import *\n",
    "from tools.losses import NPPLoss\n",
    "from tools.models import Autoencoder\n",
    "from tools.optimization import EarlyStoppingCallback, train_model, evaluate_model\n",
    "import matplotlib.pyplot as plt\n",
    "# from torch_lr_finder import LRFinder\n",
    "import time\n",
    "from tools.models import *\n",
    "from torch.utils.data import Subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa53412-0c89-4617-9b7e-efffa9b63834",
   "metadata": {},
   "source": [
    "# Dataset and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "000f94a1-69f9-488f-840c-0a0079b1bb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a random seed for PyTorch\n",
    "seed = 4  # You can use any integer value as the seed\n",
    "torch.manual_seed(seed)\n",
    "# Set a random seed for NumPy (if you're using NumPy operations)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "770be29d-6a82-4394-96c9-8c37bae5e821",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"PinMNIST\"\n",
    "feature_extracted = False\n",
    "learn_kernel = False\n",
    "n = 10\n",
    "mesh = False\n",
    "d = 10\n",
    "n_pins = 100\n",
    "fixed_pins = True\n",
    "r = 3\n",
    "d1, d2 = 28, 28\n",
    "\n",
    "partial_label_GP = False\n",
    "partial_percent = 0.5\n",
    "\n",
    "if feature_extracted:\n",
    "    folder = f\"{dataset}_ddpm\"\n",
    "else:\n",
    "    folder = f\"{dataset}\"\n",
    "\n",
    "if dataset == \"PinMNIST\":\n",
    "    if mesh:\n",
    "        data_folder = f\"./data/{folder}/mesh_{d}step_{28}by{28}pixels_{r}radius_{seed}seed\"\n",
    "        config['n_pins'] = (28 // d + 1) ** 2\n",
    "    else:\n",
    "        data_folder = f\"./data/{folder}/random_fixedTrue_{n_pins}pins_{28}by{28}pixels_{r}radius_{seed}seed\"\n",
    "elif dataset == \"Synthetic\":\n",
    "    folder += \"/28by28pixels_1000images_123456seed\"\n",
    "    if mesh:\n",
    "        data_folder = f\"./data/{folder}/mesh_{d}step_pins\"\n",
    "        config['n_pins'] = (28 // d + 1) ** 2\n",
    "    else:\n",
    "        data_folder = f\"./data/{folder}/random_{n_pins}pins\"\n",
    "elif dataset == \"Building\":\n",
    "    if mesh:\n",
    "        data_folder = f\"./data/{folder}/mesh_{d}_step\"\n",
    "    else:\n",
    "        data_folder = f\"./data/{folder}/random_n_pins_{n_pins}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ac50a0b-10c2-49d1-a2fc-ed7826a04c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset == \"Building\":\n",
    "    resize = Resize100\n",
    "else:\n",
    "    resize = Resize\n",
    "        \n",
    "\n",
    "# Define batch size\n",
    "batch_size = 16\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    images = [sample['image'] for sample in batch]\n",
    "    pins = [sample['pins'] for sample in batch]\n",
    "    outputs = [sample['outputs'] for sample in batch]\n",
    "\n",
    "    return {\n",
    "        'image': torch.stack(images, dim=0),\n",
    "        'pins': pins,\n",
    "        'outputs': outputs}\n",
    "\n",
    "transform = transforms.Compose([\n",
    "        ToTensor(),  # Convert to tensor (as you were doing)\n",
    "        resize()  # Resize to 100x100\n",
    "    ])\n",
    "\n",
    "\n",
    "modality = \"PS-RGBNIR-SAR\"\n",
    "if dataset == \"Building\":\n",
    "    root_dir = \"/work/USACE_KRI/Project_1/spacenet/aoi_11_rotterdam/train/train/AOI_11_Rotterdam/\"+modality+\"/\"\n",
    "else:\n",
    "    root_dir=f\"./data/{folder}/images/\"\n",
    "    \n",
    "transformed_dataset = PinDataset(csv_file=f\"{data_folder}/pins.csv\",\n",
    "                                 root_dir=root_dir, modality=modality,\n",
    "                                 transform=transform)\n",
    "\n",
    "\n",
    "dataset_size = len(transformed_dataset)\n",
    "train_size = int(0.7 * dataset_size)\n",
    "val_size = int(0.10 * dataset_size)\n",
    "test_size = dataset_size - train_size - val_size\n",
    "\n",
    "\n",
    "if os.path.exists(f\"./data/{dataset}/train_indices.npy\"):\n",
    "    train_indices = np.load(f'./data/{dataset}/train_indices.npy')\n",
    "    val_indices = np.load(f'./data/{dataset}/val_indices.npy')\n",
    "    test_indices = np.load(f'./data/{dataset}/test_indices.npy')\n",
    "    # Use the indices to create new datasets\n",
    "    train_dataset = Subset(transformed_dataset, train_indices)\n",
    "    val_dataset = Subset(transformed_dataset, val_indices)\n",
    "    test_dataset = Subset(transformed_dataset, test_indices)\n",
    "else:\n",
    "    # Split the dataset into train, validation, and test sets\n",
    "    train_dataset, val_dataset, test_dataset = random_split(\n",
    "        transformed_dataset, [train_size, val_size, test_size]\n",
    "    )\n",
    "    np.save(f'./data/{dataset}/train_indices.npy', train_dataset.indices)\n",
    "    np.save(f'./data/{dataset}/val_indices.npy', val_dataset.indices)\n",
    "    np.save(f'./data/{dataset}/test_indices.npy', test_dataset.indices)\n",
    "\n",
    "# Create your DataLoader with the custom_collate_fn\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=custom_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "371fb7d3-e158-435e-94fa-0118817950ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shi.cheng/anaconda3/envs/satellite/lib/python3.8/site-packages/torch/cuda/__init__.py:146: UserWarning: \n",
      "NVIDIA A100-SXM4-80GB with CUDA capability sm_80 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_70.\n",
      "If you want to use the NVIDIA A100-SXM4-80GB GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(incompatible_device_warn.format(device_name, capability, \" \".join(arch_list), device_name))\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for batch in train_loader:\n",
    "    images = batch['image'].to(device) # get RGB instead of RGBA\n",
    "    pins = batch['pins']\n",
    "    outputs = batch['outputs']\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e347c68-51b9-4be3-b854-11db4b1bfb64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1, 28, 28])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ce377de-650d-4e4c-9520-437fc8752263",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 1-dimensional, but 2 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m j\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m\n\u001b[1;32m      2\u001b[0m sample_img \u001b[38;5;241m=\u001b[39m images[j]\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[0;32m----> 3\u001b[0m count_image \u001b[38;5;241m=\u001b[39m \u001b[43mplot_label_pin\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_img\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpins\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m count_all_image \u001b[38;5;241m=\u001b[39m plot_all(sample_img[\u001b[38;5;241m0\u001b[39m], r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m      5\u001b[0m fig, axes \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n",
      "File \u001b[0;32m/work/DNAL/shi.cheng/NPP/Satellite_Fusion/tools/plot_utils.py:58\u001b[0m, in \u001b[0;36mplot_label_pin\u001b[0;34m(image, pins, labels)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pin, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(pins, labels):\n\u001b[1;32m     57\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m pin\n\u001b[0;32m---> 58\u001b[0m     \u001b[43mlabeled_image\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m label\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Plot the resulting image\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# plt.imshow(labeled_image, cmap='gray')\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# plt.title(\"Image with Labels Assigned to Pins\")\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# plt.show()\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m labeled_image\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array: array is 1-dimensional, but 2 were indexed"
     ]
    }
   ],
   "source": [
    "j=6\n",
    "sample_img = images[j].squeeze().detach().cpu()\n",
    "count_image = plot_label_pin(sample_img[0], pins[j], outputs[j])\n",
    "count_all_image = plot_all(sample_img[0], r=3)\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "sample_img = sample_img[4:]\n",
    "img = np.transpose(sample_img, (1, 2, 0))\n",
    "print(sample_img.shape, img.shape)\n",
    "im0 = axes[0].imshow(img)\n",
    "# Plot the sample_img in the first subplot\n",
    "# im0 = axes[0].imshow(sample_img)\n",
    "axes[0].set_title('Sample Image')\n",
    "fig.colorbar(im0, ax=axes[0], fraction=0.046, pad=0.04)  # Add colorbar\n",
    "\n",
    "# Plot the count_image in the second subplot\n",
    "im1 = axes[1].imshow(count_image)\n",
    "axes[1].set_title('Count Image')\n",
    "fig.colorbar(im1, ax=axes[1], fraction=0.046, pad=0.04)  # Add colorbar\n",
    "\n",
    "# Plot the count_all_image in the third subplot\n",
    "im2 = axes[2].imshow(count_all_image)\n",
    "axes[2].set_title('Count All Image')\n",
    "fig.colorbar(im2, ax=axes[2], fraction=0.046, pad=0.04)  # Add colorbar\n",
    "\n",
    "# Add spacing between subplots\n",
    "plt.tight_layout()\n",
    "# Display the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c306e1-061a-47bc-8584-46ad8e08ee8b",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa39fe1f-e261-4bc3-b8b4-75fc9ca989a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLRFinder:\n",
    "    def __init__(self, model, criterion, optimizer, device='cuda'):\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "        self.history = {'lr': [], 'loss': []}\n",
    "\n",
    "    def find_lr(self, train_loader, start_lr=1e-4, end_lr=0.1, num_iter=20,smooth_f=0.05):\n",
    "        model = self.model.to(self.device)\n",
    "        criterion = self.criterion\n",
    "        optimizer = self.optimizer\n",
    "        device = self.device\n",
    "        model.train()\n",
    "\n",
    "        lr_step = (end_lr / start_lr) ** (1 / num_iter)\n",
    "        lr = start_lr\n",
    "\n",
    "        for iteration in range(num_iter):\n",
    "            optimizer.param_groups[0]['lr'] = lr\n",
    "\n",
    "            total_loss = 0.0\n",
    "            for batch in train_loader:\n",
    "                x_train = batch['image'][:, :input_channel, :, :].to(device)\n",
    "                p_train = [tensor.to(device) for tensor in batch['pins']]\n",
    "                y_train = [tensor.to(device) for tensor in batch['outputs']]\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(x_train.float())\n",
    "                loss = criterion(y_train, outputs, p_train)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            avg_loss = total_loss / len(train_loader)\n",
    "            self.history['lr'].append(lr)\n",
    "            self.history['loss'].append(avg_loss)\n",
    "\n",
    "            lr *= lr_step\n",
    "            \n",
    "    def plot_lr_finder(self):\n",
    "        plt.plot(self.history['lr'], self.history['loss'])\n",
    "        plt.xscale('log')  # Use a logarithmic scale for better visualization\n",
    "        plt.xlabel('Learning Rate')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Learning Rate Finder Curve')\n",
    "        plt.show()\n",
    "        \n",
    "    def find_best_lr(self, skip_start=3, skip_end=3):\n",
    "        # Find the index of the minimum loss in the specified range\n",
    "        min_loss_index = skip_start + np.argmin(self.history['loss'][skip_start:-skip_end])\n",
    "\n",
    "        # Output the learning rate corresponding to the minimum loss\n",
    "        best_lr = self.history['lr'][min_loss_index]\n",
    "        return best_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e04a8eba-47fc-4225-a7e9-7442ae7aca97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Case 1: identity=True\n",
    "# Set your hyperparameters\n",
    "# input_channel = 1 if dataset == \"PinMNIST\" else 3\n",
    "# epochs = 20\n",
    "# sigmas = [0.1, 0.2, 0.5, 1, 2, 5]  # Set the sigma values you want to test\n",
    "# num_kernels_encoder = [64, 32]\n",
    "# num_kernels_decoder = [64]\n",
    "# learning_rate = 0.01\n",
    "# val_every_epoch = 5\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = Autoencoder(num_kernels_encoder, num_kernels_decoder, input_channel=input_channel).to(device)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "# criterion_MSE = NPPLoss(identity=True).to(device)\n",
    "# lr_finder_MSE = CustomLRFinder(model, criterion_MSE, optimizer, device=device)\n",
    "# lr_finder_MSE.find_lr(train_loader, start_lr=1e-5, end_lr=1, num_iter=20)\n",
    "# best_lr_MSE = lr_finder_MSE.find_best_lr()\n",
    "# print(f\"Best Learning Rate for MSE: {best_lr_MSE}\")\n",
    "\n",
    "\n",
    "# # Cases 2-6: identity=False, varying sigmas\n",
    "# best_lrs = [(0,best_lr_MSE)]\n",
    "\n",
    "# sigmas = [0.1, 0.2, 0.5, 1, 2]\n",
    "\n",
    "# for sigma in sigmas:\n",
    "#     criterion_NPP = NPPLoss(identity=False, sigma=sigma).to(device)\n",
    "#     lr_finder_NPP = CustomLRFinder(model, criterion_NPP, optimizer, device=device)\n",
    "#     lr_finder_NPP.find_lr(train_loader, start_lr=1e-4, end_lr=1, num_iter=10)\n",
    "#     best_lr_NPP = lr_finder_NPP.find_best_lr()\n",
    "#     best_lrs.append((sigma, best_lr_NPP))\n",
    "#     print(f\"Best Learning Rate for NPP sigma={sigma}: {best_lr_NPP}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11e9111c-3a3f-498b-9574-56626971725a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline_ci(sigmas, num_kernels_encoder, num_kernels_decoder, train_loader, val_loader, \n",
    "                    test_loader, input_channel, epochs, val_every_epoch, learning_rates, device, learn_kernel, num_runs=3):\n",
    "    test_losses_npp_true = []\n",
    "    GP_test_losses_npp_true = []\n",
    "    test_losses_npp_false= []\n",
    "    experiment_id = int(time.time())\n",
    "    \n",
    "    for run in range(num_runs):\n",
    "        count = 0\n",
    "        test_losses_vs_sigma_npp_true = []\n",
    "        GP_test_losses_vs_sigma_npp_true = []\n",
    "        test_loss_npp_false = None\n",
    "\n",
    "        # Run NPP=False once and collect the test loss\n",
    "        early_stopping = EarlyStoppingCallback(patience=10, min_delta=0.001)\n",
    "        criterion = NPPLoss(identity=True).to(device)\n",
    "\n",
    "        autoencoder = Autoencoder(num_kernels_encoder, num_kernels_decoder, input_channel=input_channel).to(device)\n",
    "        optimizer = optim.Adam(autoencoder.parameters(), lr=learning_rates[count])\n",
    "        model, train_losses, val_losses = train_model(autoencoder, train_loader, val_loader, input_channel, epochs,\\\n",
    "                                                      val_every_epoch, learning_rates[count], criterion, optimizer, device, early_stopping, experiment_id)\n",
    "\n",
    "        test_loss_npp_false = evaluate_model(autoencoder, test_loader, input_channel, device, partial_label_GP=False, partial_percent=partial_percent)\n",
    "        print(f\"MSE Test loss:{test_loss_npp_false:.3f}\")\n",
    "        test_losses_npp_false.append(test_loss_npp_false)\n",
    "        \n",
    "        count += 1\n",
    "        # Run LR Finder for different sigma values\n",
    "        for sigma in sigmas:\n",
    "            early_stopping = EarlyStoppingCallback(patience=5, min_delta=0.001)\n",
    "            criterion = NPPLoss(identity=False, sigma=sigma).to(device)\n",
    "            autoencoder = Autoencoder(num_kernels_encoder, num_kernels_decoder, input_channel=input_channel).to(device)\n",
    "            if learn_kernel:\n",
    "                optimizer = optim.Adam(list(autoencoder.parameters())+[criterion.sigma], lr=learning_rates[count])\n",
    "            else:\n",
    "                optimizer = optim.Adam(autoencoder.parameters(), lr=learning_rates[count])\n",
    "            model, train_losses, val_losses = train_model(autoencoder, train_loader, val_loader, input_channel, epochs,\\\n",
    "                                                          val_every_epoch, learning_rates[count], criterion, optimizer, device, early_stopping, experiment_id)\n",
    "            test_loss = evaluate_model(autoencoder, test_loader, input_channel, device, partial_label_GP=False, partial_percent=partial_percent, sigma=sigma)\n",
    "            GP_test_loss = evaluate_model(autoencoder, test_loader, input_channel, device, partial_label_GP=True, partial_percent=partial_percent, sigma=sigma)\n",
    "            print(f\"NPP sigma={sigma} Test loss:{test_loss:.3f}, GP Test loss:{GP_test_loss:.3f}\")\n",
    "            test_losses_vs_sigma_npp_true.append(test_loss)\n",
    "            GP_test_losses_vs_sigma_npp_true.append(GP_test_loss)\n",
    "            count += 1\n",
    "\n",
    "        test_losses_npp_true.append(test_losses_vs_sigma_npp_true)\n",
    "        GP_test_losses_npp_true.append(GP_test_losses_vs_sigma_npp_true)\n",
    "    return GP_test_losses_npp_true, test_losses_npp_true, test_losses_npp_false\n",
    "\n",
    "    \n",
    "# Function to run the pipeline and save data\n",
    "def run_and_save_pipeline(sigmas, num_kernels_encoder, num_kernels_decoder, train_loader, val_loader, test_loader, input_channel, epochs, val_every_epoch, learning_rates, device, learn_kernel, num_runs):\n",
    "    # Run the pipeline\n",
    "    GP_test_loss_npp_true, test_loss_npp_true, test_loss_npp_false= run_pipeline_ci(sigmas, num_kernels_encoder, num_kernels_decoder, train_loader, val_loader, test_loader, input_channel, epochs, val_every_epoch, learning_rates, device, learn_kernel, num_runs)\n",
    "    print(\"start saving!\")\n",
    "    # Save the data\n",
    "    save_loss(test_loss_npp_true, './history/test_loss_npp_true.npy')\n",
    "    save_loss(GP_test_loss_npp_true, './history/GP_test_loss_npp_true.npy')\n",
    "    save_loss(test_loss_npp_false, './history/test_loss_npp_false.npy')\n",
    "    print(\"saved\")\n",
    "    return GP_test_loss_npp_true, test_loss_npp_true, test_loss_npp_false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c681ad7-b14c-4d02-ad7f-da35693469d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "train_model() missing 3 required positional arguments: 'exp_name', 'global_best_val_loss', and 'manual_lr'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Run and save the pipeline data\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m loss_vs_sigma_data \u001b[38;5;241m=\u001b[39m \u001b[43mrun_and_save_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43msigmas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_kernels_encoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_kernels_decoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43minput_channel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_every_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_lrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearn_kernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_runs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_runs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Plot and save the plot using the saved data\u001b[39;00m\n\u001b[1;32m     25\u001b[0m plot_and_save(loss_vs_sigma_data, sigmas, dataset, learning_rate)\n",
      "Cell \u001b[0;32mIn[11], line 54\u001b[0m, in \u001b[0;36mrun_and_save_pipeline\u001b[0;34m(sigmas, num_kernels_encoder, num_kernels_decoder, train_loader, val_loader, test_loader, input_channel, epochs, val_every_epoch, learning_rates, device, learn_kernel, num_runs)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_and_save_pipeline\u001b[39m(sigmas, num_kernels_encoder, num_kernels_decoder, train_loader, val_loader, test_loader, input_channel, epochs, val_every_epoch, learning_rates, device, learn_kernel, num_runs):\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;66;03m# Run the pipeline\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m     GP_test_loss_npp_true, test_loss_npp_true, test_loss_npp_false\u001b[38;5;241m=\u001b[39m \u001b[43mrun_pipeline_ci\u001b[49m\u001b[43m(\u001b[49m\u001b[43msigmas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_kernels_encoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_kernels_decoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_channel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_every_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearn_kernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_runs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart saving!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# Save the data\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 20\u001b[0m, in \u001b[0;36mrun_pipeline_ci\u001b[0;34m(sigmas, num_kernels_encoder, num_kernels_decoder, train_loader, val_loader, test_loader, input_channel, epochs, val_every_epoch, learning_rates, device, learn_kernel, num_runs)\u001b[0m\n\u001b[1;32m     18\u001b[0m autoencoder \u001b[38;5;241m=\u001b[39m Autoencoder(num_kernels_encoder, num_kernels_decoder, input_channel\u001b[38;5;241m=\u001b[39minput_channel)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     19\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(autoencoder\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rates[count])\n\u001b[0;32m---> 20\u001b[0m model, train_losses, val_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mautoencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_channel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43mval_every_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rates\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcount\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexperiment_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m test_loss_npp_false \u001b[38;5;241m=\u001b[39m evaluate_model(autoencoder, test_loader, input_channel, device, partial_label_GP\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, partial_percent\u001b[38;5;241m=\u001b[39mpartial_percent)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMSE Test loss:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_loss_npp_false\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: train_model() missing 3 required positional arguments: 'exp_name', 'global_best_val_loss', and 'manual_lr'"
     ]
    }
   ],
   "source": [
    "# Generate a unique experiment_id using a timestamp\n",
    "  # Using timestamp as experiment_id\n",
    "\n",
    "# Set your hyperparameters\n",
    "# dataset = \"MNIST\"\n",
    "input_channel = 1 if dataset == \"PinMNIST\" else 3\n",
    "epochs = 200\n",
    "sigmas = [0.1, 0.2, 0.5, 1, 2]  # Set the sigma values you want to test\n",
    "# best_lrs = [0.05, 0.05, 0.05, 0.05, 0.05, 0.001] #MNIST\n",
    "best_lrs = [0.1, 0.001, 0.001, 0.001, 0.001, 0.001] #Synthetic\n",
    "num_kernels_encoder = [32, 16]\n",
    "num_kernels_decoder = [32]\n",
    "# learning_rate = 0.01\n",
    "val_every_epoch = 5\n",
    "num_runs = 1\n",
    "learn_kernel = False\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Run and save the pipeline data\n",
    "loss_vs_sigma_data = run_and_save_pipeline(sigmas, num_kernels_encoder, num_kernels_decoder, train_loader, val_loader, test_loader,\\\n",
    "                                           input_channel, epochs, val_every_epoch, best_lrs, device, learn_kernel, num_runs=num_runs)\n",
    "\n",
    "\n",
    "# Plot and save the plot using the saved data\n",
    "plot_and_save(loss_vs_sigma_data, sigmas, dataset, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53b7484-3290-43cb-a85e-39cca0f9e140",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_vs_sigma_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527b04d7-2308-48e9-9a75-d8071dd3d0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_loss_npp_false = [test_loss_npp_false for i in range((len(sigmas)))]\n",
    "# test_loss_npp_true.pop(1)\n",
    "test_loss_npp_true, test_loss_npp_false = load_data('./history/test_loss_npp_true.npy'), load_data('./history/test_loss_npp_false.npy')\n",
    "test_loss_npp_true, test_loss_npp_false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8bf364-d010-40de-ad6c-ce837b5dee2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_and_save(loss_vs_sigma_data, sigmas, dataset, learning_rate, dataset, model_name=\"Auto encoder\"):\n",
    "    # Unpack the data\n",
    "    test_loss_npp_true, test_loss_npp_false = loss_vs_sigma_data\n",
    "    test_loss_npp_false = [test_loss_npp_false for i in range(len(sigmas))]\n",
    "\n",
    "    # Calculate mean and confidence intervals for NPP=True runs\n",
    "    mean_test_loss_npp_true = np.mean(test_loss_npp_true, axis=0)\n",
    "    ci_test_loss_npp_true = 1.96 * np.std(test_loss_npp_true, axis=0) / np.sqrt(len(test_loss_npp_true))\n",
    "\n",
    "    # Duplicate NPP=False values for plotting\n",
    "    mean_test_loss_npp_false = np.mean(test_loss_npp_false, axis=1)\n",
    "    ci_test_loss_npp_false = 1.96 * np.std(test_loss_npp_false, axis=1) / np.sqrt(len(test_loss_npp_false))\n",
    "\n",
    "    # Plot mean and confidence intervals for NPP=True\n",
    "    plt.plot(sigmas, mean_test_loss_npp_true, marker='o', label='NPP=True', color='blue')\n",
    "\n",
    "    # Plot mean and confidence intervals for duplicated NPP=False\n",
    "    plt.plot(sigmas, mean_test_loss_npp_false, color='red', linestyle='--', label='NPP=False')\n",
    "\n",
    "    # Fill between for NPP=True with blue color\n",
    "    plt.fill_between(sigmas, mean_test_loss_npp_true - ci_test_loss_npp_true, mean_test_loss_npp_true + ci_test_loss_npp_true, color='blue', alpha=0.2)\n",
    "\n",
    "    # Fill between for NPP=False with red color\n",
    "    plt.fill_between(sigmas, mean_test_loss_npp_false - ci_test_loss_npp_false, mean_test_loss_npp_false + ci_test_loss_npp_false, color='red', alpha=0.2)\n",
    "\n",
    "    plt.xlabel('Sigma')\n",
    "    plt.ylabel('Test Loss')\n",
    "    plt.title(f'Test Loss vs. Sigma:{dataset} dataset with {model_name}')\n",
    "    plt.legend()\n",
    "\n",
    "    # Create a directory to save the results if it doesn't exist\n",
    "    results_dir = './results'\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "    # Generate a filename based on parameters in the title\n",
    "    filename = f\"test_loss_vs_sigma_{dataset}_{model_name}_lr_{learning_rate}.png\"\n",
    "    filepath = os.path.join(results_dir, filename)\n",
    "\n",
    "    # Save the plot\n",
    "    plt.savefig(filepath)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# plot_and_save(loss_vs_sigma_data, sigmas, dataset, learning_rate)\n",
    "\n",
    "# Plot and save the plot using the saved data\n",
    "plot_and_save(loss_vs_sigma_data, sigmas, dataset, learning_rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "satellite",
   "language": "python",
   "name": "satellite"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
