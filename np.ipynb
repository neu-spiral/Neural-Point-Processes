{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2dd97eb1-5c1b-49cf-ab08-4f19ff72a18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import os\n",
    "import json\n",
    "from tools.data_utils import *\n",
    "from tools.optimization import EarlyStoppingCallback, evaluate_model\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import time\n",
    "from tools.NPmodels import *\n",
    "from tools.NPtrain import process_batch, NeuralProcessTrainer, evaluate_np\n",
    "from tabulate import tabulate\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9230efa-c653-44a5-a6f7-76d0ad129aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    images = [sample['image'] for sample in batch]\n",
    "    pins = [sample['pins'] for sample in batch]\n",
    "    outputs = [sample['outputs'] for sample in batch]\n",
    "\n",
    "    return {\n",
    "        'image': torch.stack(images, dim=0),\n",
    "        'pins': pins,\n",
    "        'outputs': outputs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50ee69e7-b091-4ece-8b4e-1ded41e0062c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline_ci_np(train_loader, val_loader, \n",
    "                    test_loader, input_channel, epochs, val_every_epoch, config, device, num_runs=3, print_freq=2):\n",
    "    test_losses = []\n",
    "    # partial_percent = config['partial_percent']\n",
    "    experiment_id = int(time.time())\n",
    "    best_val_loss_NP = float('inf')\n",
    "    # config['experiment_id'] = experiment_id\n",
    "    experiment_id = config['experiment_id']\n",
    "\n",
    "    r_dim = np_config.r_dim\n",
    "    h_dim = np_config.h_dim\n",
    "    z_dim = np_config.z_dim\n",
    "    lr = config['best_lr']\n",
    "\n",
    "    # Create storage directory and store the experiment configuration\n",
    "    if not os.path.exists(f'./results/neural_processes/{experiment_id}'):\n",
    "        os.makedirs(f'./results/neural_processes/{experiment_id}')\n",
    "    with open(f\"./results/neural_processes/{experiment_id}/config_np.json\", \"w\") as outfile: \n",
    "        json.dump(config, outfile)\n",
    "        \n",
    "    global_val_loss = float('inf')\n",
    "    \n",
    "    for run in range(num_runs):\n",
    "        count = 0\n",
    "        GP_test_losses = []\n",
    "        \n",
    "        early_stopping = EarlyStoppingCallback(patience=5, min_delta=0.001)\n",
    "        model = NeuralProcessImg(r_dim, z_dim, h_dim).to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        np_trainer = NeuralProcessTrainer(device, model, optimizer, early_stopping, experiment_id, print_freq=print_freq)          \n",
    "\n",
    "        np_trainer.train(train_loader, val_loader, epochs)\n",
    "        if np_trainer.best_val_loss <= global_val_loss:\n",
    "            global_val_loss = np_trainer.best_val_loss \n",
    "            torch.save(np_trainer.neural_process.state_dict(), f'./results/neural_processes/{experiment_id}' + f'/best_{args.dataset}_np.pt')\n",
    "        count += 1\n",
    "\n",
    "    return experiment_id\n",
    "\n",
    "\n",
    "# Function to run the pipeline and save data\n",
    "def run_and_save_pipeline_np(train_loader, val_loader, test_loader, input_channel, epochs, val_every_epoch, config, num_runs, device):\n",
    "    test_partial_percents = [0.25, 0.5, 0.75, 1]\n",
    "    test_losses = []\n",
    "    r2_list = []\n",
    "    table = []\n",
    "    table.append(['Dataset', 'Mode', 'd', 'n_pins', 'LR', 'PLP', 'MSE error', 'R2'])\n",
    "    experiment_id = run_pipeline_ci_np(train_loader, val_loader, \n",
    "                    test_loader, input_channel, epochs, val_every_epoch, config, device, num_runs)\n",
    "    # Run final testing\n",
    "    model = NeuralProcessImg(r_dim, z_dim, h_dim).to(device)\n",
    "    # MSE\n",
    "    model.load_state_dict(torch.load(f'./results/neural_processes/{experiment_id}/best_{args.dataset}_np.pt'))\n",
    "    for partial_percent in test_partial_percents:\n",
    "        test_loss, r2 = evaluate_np(model, test_loader, device, partial_percent=partial_percent)\n",
    "        print(f\"pp: {partial_percent} MSE loss: {test_loss} R2 score: {r2}\")\n",
    "        table.append([args.dataset, args.mode, args.d, args.n_pins, config['best_lr'], partial_percent, test_loss, r2])\n",
    "        test_losses.append(test_loss)\n",
    "        r2_list.append(r2)\n",
    "    table = tabulate(table, headers='firstrow', tablefmt='fancy_grid', showindex=True)\n",
    "    print(table)\n",
    "    with open('./results/neural_processes/table.txt', 'a') as f:\n",
    "        f.write('\\n')\n",
    "        f.write(table + '\\n')  # Add a newline character after writing the table\n",
    "        f.write('\\n')  # Add an additional newline character for separation\n",
    "    print(\"saved\")\n",
    "    return test_losses, r2_list, experiment_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbc10563-9861-4f04-bfda-77ce67b76c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args():\n",
    "    dataset = \"PinMNIST\"\n",
    "    n = 1000\n",
    "    mode = \"random\"\n",
    "    d = 3\n",
    "    n_pins = 10\n",
    "    r = 3\n",
    "    partial_percent = 0.8\n",
    "    # Set your hyperparameters\n",
    "    epochs = 1000\n",
    "    batch_size = 50\n",
    "    learning_rate = 1e-4\n",
    "    val_every_epoch = 10\n",
    "    num_runs = 1\n",
    "    seed = 4\n",
    "    \n",
    "class NP_config():\n",
    "    r_dim = 512\n",
    "    h_dim = 512\n",
    "    z_dim = 512\n",
    "    lr = 4e-5\n",
    "    epochs = 100\n",
    "\n",
    "np_config = NP_config()\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e35ab4e-0d87-40fc-9591-8b1be2640378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a random seed for PyTorch\n",
    "seed = 4  # You can use any integer value as the seed\n",
    "torch.manual_seed(seed)\n",
    "# Set a random seed for NumPy (if you're using NumPy operations)\n",
    "np.random.seed(seed)\n",
    "\n",
    " # Choose datasets\n",
    "dataset = args.dataset \n",
    "n = args.n\n",
    "mode = args.mode\n",
    "d = args.d\n",
    "n_pins = args.n_pins\n",
    "r = args.r\n",
    "partial_percent = args.partial_percent\n",
    "\n",
    "# Set your hyperparameters\n",
    "epochs = args.epochs\n",
    "batch_size = args.batch_size\n",
    "learning_rate = args.learning_rate\n",
    "val_every_epoch = args.val_every_epoch\n",
    "num_runs = args.num_runs\n",
    "   \n",
    "config = {}\n",
    "# np_config = {\"batch_size\": 32,\n",
    "#              \"r_dim\": 512,\n",
    "#              \"h_dim\": 512,\n",
    "#              \"z_dim\": 512,\n",
    "#              \"lr\": 4e-5,\n",
    "#              \"epochs\": 100\n",
    "#             }\n",
    "\n",
    "# config = vars(args)\n",
    "config = {\"experiment_id\":0}\n",
    "\n",
    "input_channel = 1 if dataset == \"PinMNIST\" else 3\n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "247eb8c4-b9ee-4857-a528-2867abb87b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if feature_extracted:\n",
    "#     folder = f\"{dataset}_ddpm\"\n",
    "# else:\n",
    "folder = f\"{dataset}\"\n",
    "\n",
    "if dataset == \"PinMNIST\":\n",
    "    if mode == \"mesh\":\n",
    "        data_folder = f\"./data/{folder}/mesh_{d}step_{28}by{28}pixels_{r}radius_{seed}seed\"\n",
    "    else:\n",
    "        data_folder = f\"./data/{folder}/random_fixedTrue_{n_pins}pins_{28}by{28}pixels_{r}radius_{seed}seed\"\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    ToTensor(),         # Convert to tensor (as you were doing)\n",
    "    Resize()  # Resize to 100x100\n",
    "])\n",
    "\n",
    "transformed_dataset = PinDataset(csv_file=f\"{data_folder}/pins.csv\",\n",
    "                                      root_dir=f\"./data/{dataset}/images/\",\n",
    "                                      transform=transform)\n",
    "\n",
    "dataset_size = len(transformed_dataset)\n",
    "train_size = int(0.7 * dataset_size)\n",
    "val_size = int(0.1 * dataset_size)\n",
    "test_size = dataset_size - train_size - val_size\n",
    "\n",
    "# Split the dataset into train, validation, and test sets\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    transformed_dataset, [train_size, val_size, test_size]\n",
    ")\n",
    "\n",
    "# Create your DataLoader with the custom_collate_fn\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c87002a7-af7a-4598-b786-d6796423bfea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Avg_loss: 430.210\n",
      "Epoch: 0, Val_loss 100.160\n",
      "Epoch: 1, Avg_loss: 75.492\n",
      "Epoch: 2, Avg_loss: 50.549\n",
      "Epoch: 2, Val_loss 46.425\n",
      "Epoch: 3, Avg_loss: 43.584\n",
      "Epoch: 4, Avg_loss: 41.153\n",
      "Epoch: 4, Val_loss 40.204\n",
      "Epoch: 5, Avg_loss: 39.531\n",
      "Epoch: 6, Avg_loss: 38.395\n",
      "Epoch: 6, Val_loss 38.309\n",
      "Epoch: 7, Avg_loss: 37.610\n",
      "Epoch: 8, Avg_loss: 37.043\n",
      "Epoch: 8, Val_loss 36.531\n",
      "Epoch: 9, Avg_loss: 36.694\n",
      "Epoch: 10, Avg_loss: 36.325\n",
      "Epoch: 10, Val_loss 35.660\n",
      "Epoch: 11, Avg_loss: 36.030\n",
      "Epoch: 12, Avg_loss: 35.808\n",
      "Epoch: 12, Val_loss 35.544\n",
      "Epoch: 13, Avg_loss: 35.582\n",
      "Epoch: 14, Avg_loss: 35.282\n",
      "Epoch: 14, Val_loss 35.784\n",
      "Epoch: 15, Avg_loss: 34.811\n",
      "Epoch: 16, Avg_loss: 34.347\n",
      "Epoch: 16, Val_loss 33.397\n",
      "Epoch: 17, Avg_loss: 33.915\n",
      "Epoch: 18, Avg_loss: 33.695\n",
      "Epoch: 18, Val_loss 34.117\n",
      "Epoch: 19, Avg_loss: 33.674\n",
      "Epoch: 20, Avg_loss: 33.484\n",
      "Epoch: 20, Val_loss 33.506\n",
      "Epoch: 21, Avg_loss: 33.415\n",
      "Epoch: 22, Avg_loss: 33.021\n",
      "Epoch: 22, Val_loss 33.224\n",
      "Epoch: 23, Avg_loss: 32.848\n",
      "Epoch: 24, Avg_loss: 32.788\n",
      "Epoch: 24, Val_loss 35.004\n",
      "Epoch: 25, Avg_loss: 32.730\n",
      "Epoch: 26, Avg_loss: 32.803\n",
      "Epoch: 26, Val_loss 33.356\n",
      "Epoch: 27, Avg_loss: 32.634\n",
      "Epoch: 28, Avg_loss: 32.583\n",
      "Epoch: 28, Val_loss 31.845\n",
      "Epoch: 29, Avg_loss: 32.394\n",
      "Epoch: 30, Avg_loss: 32.471\n",
      "Epoch: 30, Val_loss 32.116\n",
      "Epoch: 31, Avg_loss: 32.390\n",
      "Epoch: 32, Avg_loss: 32.230\n",
      "Epoch: 32, Val_loss 31.233\n",
      "Epoch: 33, Avg_loss: 31.836\n",
      "Epoch: 34, Avg_loss: 31.599\n",
      "Epoch: 34, Val_loss 31.041\n",
      "Epoch: 35, Avg_loss: 32.366\n",
      "Epoch: 36, Avg_loss: 32.574\n",
      "Epoch: 36, Val_loss 32.400\n",
      "Epoch: 37, Avg_loss: 32.085\n",
      "Epoch: 38, Avg_loss: 32.043\n",
      "Epoch: 38, Val_loss 32.354\n",
      "Epoch: 39, Avg_loss: 31.564\n",
      "Epoch: 40, Avg_loss: 31.074\n",
      "Epoch: 40, Val_loss 30.265\n",
      "Epoch: 41, Avg_loss: 31.122\n",
      "Epoch: 42, Avg_loss: 30.731\n",
      "Epoch: 42, Val_loss 30.811\n",
      "Epoch: 43, Avg_loss: 30.733\n",
      "Epoch: 44, Avg_loss: 30.449\n",
      "Epoch: 44, Val_loss 30.379\n",
      "Epoch: 45, Avg_loss: 30.291\n",
      "Epoch: 46, Avg_loss: 30.049\n",
      "Epoch: 46, Val_loss 29.399\n",
      "Epoch: 47, Avg_loss: 29.991\n",
      "Epoch: 48, Avg_loss: 30.009\n",
      "Epoch: 48, Val_loss 29.004\n",
      "Epoch: 49, Avg_loss: 30.351\n",
      "Epoch: 50, Avg_loss: 30.128\n",
      "Epoch: 50, Val_loss 30.651\n",
      "Epoch: 51, Avg_loss: 30.020\n",
      "Epoch: 52, Avg_loss: 29.347\n",
      "Epoch: 52, Val_loss 30.402\n",
      "Epoch: 53, Avg_loss: 29.172\n",
      "Epoch: 54, Avg_loss: 29.069\n",
      "Epoch: 54, Val_loss 27.458\n",
      "Epoch: 55, Avg_loss: 29.208\n",
      "Epoch: 56, Avg_loss: 28.992\n",
      "Epoch: 56, Val_loss 28.588\n",
      "Epoch: 57, Avg_loss: 28.748\n",
      "Epoch: 58, Avg_loss: 28.770\n",
      "Epoch: 58, Val_loss 27.112\n",
      "Epoch: 59, Avg_loss: 28.783\n",
      "Epoch: 60, Avg_loss: 28.458\n",
      "Epoch: 60, Val_loss 26.933\n",
      "Epoch: 61, Avg_loss: 28.593\n",
      "Epoch: 62, Avg_loss: 28.522\n",
      "Epoch: 62, Val_loss 28.068\n",
      "Epoch: 63, Avg_loss: 28.327\n",
      "Epoch: 64, Avg_loss: 28.226\n",
      "Epoch: 64, Val_loss 27.303\n",
      "Epoch: 65, Avg_loss: 28.297\n",
      "Epoch: 66, Avg_loss: 28.151\n",
      "Epoch: 66, Val_loss 29.088\n",
      "Epoch: 67, Avg_loss: 27.876\n",
      "Epoch: 68, Avg_loss: 27.780\n",
      "Epoch: 68, Val_loss 28.817\n",
      "Epoch: 69, Avg_loss: 27.539\n",
      "Epoch: 70, Avg_loss: 27.883\n",
      "Epoch: 70, Val_loss 26.182\n",
      "Epoch: 71, Avg_loss: 27.324\n",
      "Epoch: 72, Avg_loss: 27.555\n",
      "Epoch: 72, Val_loss 25.291\n",
      "Epoch: 73, Avg_loss: 27.346\n",
      "Epoch: 74, Avg_loss: 26.907\n",
      "Epoch: 74, Val_loss 25.404\n",
      "Epoch: 75, Avg_loss: 26.791\n",
      "Epoch: 76, Avg_loss: 27.290\n",
      "Epoch: 76, Val_loss 25.756\n",
      "Epoch: 77, Avg_loss: 27.448\n",
      "Epoch: 78, Avg_loss: 27.254\n",
      "Epoch: 78, Val_loss 28.447\n",
      "Epoch: 79, Avg_loss: 28.593\n",
      "Epoch: 80, Avg_loss: 31.608\n",
      "Epoch: 80, Val_loss 29.461\n",
      "Epoch: 81, Avg_loss: 29.072\n",
      "Epoch: 82, Avg_loss: 28.324\n",
      "Epoch: 82, Val_loss 27.058\n",
      "Early stopping after 82 epochs.\n",
      "tensor(-425.4838, device='cuda:0') tensor([[0.1071],\n",
      "        [4.9758],\n",
      "        [2.4982],\n",
      "        [9.0202],\n",
      "        [1.6388]], device='cuda:0') tensor([[ 0.0000],\n",
      "        [15.0118],\n",
      "        [ 0.0000],\n",
      "        [12.8235],\n",
      "        [15.2980]], device='cuda:0')\n",
      "tensor(-inf, device='cuda:0') tensor([[-0.0806],\n",
      "        [ 0.0300],\n",
      "        [ 5.7444],\n",
      "        [ 0.3537],\n",
      "        [-0.0352]], device='cuda:0') tensor([[ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [19.1608],\n",
      "        [ 2.7922],\n",
      "        [ 0.4980]], device='cuda:0')\n",
      "tensor(-inf, device='cuda:0') tensor([[4.4976],\n",
      "        [6.7113],\n",
      "        [7.2470],\n",
      "        [8.2753],\n",
      "        [4.2947]], device='cuda:0') tensor([[ 1.0471],\n",
      "        [16.4824],\n",
      "        [11.1255],\n",
      "        [19.5176],\n",
      "        [ 0.0000]], device='cuda:0')\n",
      "tensor(-inf, device='cuda:0') tensor([[7.9860],\n",
      "        [6.9989],\n",
      "        [0.9064],\n",
      "        [0.0725],\n",
      "        [0.5183]], device='cuda:0') tensor([[3.1137],\n",
      "        [7.9176],\n",
      "        [0.0000],\n",
      "        [0.0000],\n",
      "        [0.0863]], device='cuda:0')\n",
      "pp: 0.25 MSE loss: 56.02553176879883 R2 score: -inf\n",
      "tensor(-inf, device='cuda:0') tensor([[ 3.5300],\n",
      "        [-0.0304],\n",
      "        [ 4.7744],\n",
      "        [-0.0107],\n",
      "        [ 8.2470]], device='cuda:0') tensor([[0.0000],\n",
      "        [0.0000],\n",
      "        [0.0000],\n",
      "        [0.0000],\n",
      "        [4.2980]], device='cuda:0')\n",
      "tensor(-inf, device='cuda:0') tensor([[0.6932],\n",
      "        [8.4738],\n",
      "        [3.2046],\n",
      "        [0.8142],\n",
      "        [4.9700]], device='cuda:0') tensor([[ 0.0000],\n",
      "        [11.4039],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 9.4039]], device='cuda:0')\n",
      "tensor(-inf, device='cuda:0') tensor([[ 7.0749],\n",
      "        [ 4.0948],\n",
      "        [11.3425],\n",
      "        [12.2463],\n",
      "        [ 9.8746]], device='cuda:0') tensor([[12.5020],\n",
      "        [ 4.7490],\n",
      "        [15.5020],\n",
      "        [29.0039],\n",
      "        [ 3.2510]], device='cuda:0')\n",
      "tensor(-inf, device='cuda:0') tensor([[5.9073],\n",
      "        [5.1703],\n",
      "        [7.5376],\n",
      "        [9.3591],\n",
      "        [6.6375]], device='cuda:0') tensor([[0.7843],\n",
      "        [0.0000],\n",
      "        [8.6941],\n",
      "        [9.0353],\n",
      "        [0.0000]], device='cuda:0')\n",
      "pp: 0.5 MSE loss: 50.001773834228516 R2 score: -inf\n",
      "tensor(-inf, device='cuda:0') tensor([[15.0061],\n",
      "        [ 6.6410],\n",
      "        [12.6496],\n",
      "        [11.6269],\n",
      "        [ 4.3099]], device='cuda:0') tensor([[21.0549],\n",
      "        [ 0.0000],\n",
      "        [23.5020],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000]], device='cuda:0')\n",
      "tensor(-inf, device='cuda:0') tensor([[ 6.6670],\n",
      "        [ 8.1512],\n",
      "        [ 5.7720],\n",
      "        [10.3175],\n",
      "        [ 2.1868]], device='cuda:0') tensor([[ 0.0000],\n",
      "        [28.6431],\n",
      "        [ 0.3059],\n",
      "        [23.4941],\n",
      "        [ 0.0000]], device='cuda:0')\n",
      "tensor(-inf, device='cuda:0') tensor([[-0.0207],\n",
      "        [ 0.0240],\n",
      "        [ 3.0031],\n",
      "        [ 2.2129],\n",
      "        [ 9.6598]], device='cuda:0') tensor([[ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 2.7490],\n",
      "        [20.3647]], device='cuda:0')\n",
      "tensor(-inf, device='cuda:0') tensor([[-0.0248],\n",
      "        [ 6.1691],\n",
      "        [13.8084],\n",
      "        [12.4070],\n",
      "        [ 7.4067]], device='cuda:0') tensor([[ 0.0000],\n",
      "        [ 7.2667],\n",
      "        [24.0314],\n",
      "        [19.1137],\n",
      "        [ 9.2392]], device='cuda:0')\n",
      "pp: 0.75 MSE loss: 49.68486022949219 R2 score: -inf\n",
      "tensor(-inf, device='cuda:0') tensor([[5.6076],\n",
      "        [6.6644],\n",
      "        [2.4144],\n",
      "        [3.5880],\n",
      "        [7.6443]], device='cuda:0') tensor([[15.3216],\n",
      "        [19.0078],\n",
      "        [ 0.0000],\n",
      "        [17.3843],\n",
      "        [ 9.3882]], device='cuda:0')\n",
      "tensor(-inf, device='cuda:0') tensor([[3.9080],\n",
      "        [0.9011],\n",
      "        [7.8365],\n",
      "        [9.2884],\n",
      "        [6.6775]], device='cuda:0') tensor([[12.5020],\n",
      "        [ 4.7490],\n",
      "        [15.5020],\n",
      "        [29.0039],\n",
      "        [ 3.2510]], device='cuda:0')\n",
      "tensor(-inf, device='cuda:0') tensor([[ 2.3460e-01],\n",
      "        [ 1.4608e+01],\n",
      "        [-8.0169e-03],\n",
      "        [ 1.1029e+01],\n",
      "        [ 9.1351e+00]], device='cuda:0') tensor([[ 0.0000],\n",
      "        [17.6196],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 9.6157]], device='cuda:0')\n",
      "tensor(-inf, device='cuda:0') tensor([[ 9.0162],\n",
      "        [ 8.0490],\n",
      "        [11.4518],\n",
      "        [13.2644],\n",
      "        [10.6512]], device='cuda:0') tensor([[0.7843],\n",
      "        [0.0000],\n",
      "        [8.6941],\n",
      "        [9.0353],\n",
      "        [0.0000]], device='cuda:0')\n",
      "pp: 1 MSE loss: 50.196048736572266 R2 score: -inf\n",
      "╒════╤═══════════╤════════╤═════╤══════════╤════════╤═══════╤═════════════╤══════╕\n",
      "│    │ Dataset   │ Mode   │   d │   n_pins │     LR │   PLP │   MSE error │   R2 │\n",
      "╞════╪═══════════╪════════╪═════╪══════════╪════════╪═══════╪═════════════╪══════╡\n",
      "│  0 │ PinMNIST  │ random │   3 │       10 │ 0.0001 │  0.25 │     56.0255 │ -inf │\n",
      "├────┼───────────┼────────┼─────┼──────────┼────────┼───────┼─────────────┼──────┤\n",
      "│  1 │ PinMNIST  │ random │   3 │       10 │ 0.0001 │  0.5  │     50.0018 │ -inf │\n",
      "├────┼───────────┼────────┼─────┼──────────┼────────┼───────┼─────────────┼──────┤\n",
      "│  2 │ PinMNIST  │ random │   3 │       10 │ 0.0001 │  0.75 │     49.6849 │ -inf │\n",
      "├────┼───────────┼────────┼─────┼──────────┼────────┼───────┼─────────────┼──────┤\n",
      "│  3 │ PinMNIST  │ random │   3 │       10 │ 0.0001 │  1    │     50.196  │ -inf │\n",
      "╘════╧═══════════╧════════╧═════╧══════════╧════════╧═══════╧═════════════╧══════╛\n",
      "saved\n",
      "Time elapsed: 130.66030526161194 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "r_dim, z_dim, h_dim = np_config.r_dim, np_config.z_dim, np_config.h_dim\n",
    "if (config['experiment_id'] == 0): # Training\n",
    "\n",
    "    neural_processes = NeuralProcessImg(r_dim, z_dim, h_dim).to(device)\n",
    "    optimizer = torch.optim.Adam(neural_processes.parameters(), learning_rate)\n",
    "#     lr_finder_NP =NPLRFinder(neural_processes, optimizer, device=device)\n",
    "#     lr_finder_NP.find_lr(train_loader,input_channel=input_channel, start_lr=1e-5, end_lr=1, num_iter=20)\n",
    "#     best_lr_NP = lr_finder_NP.find_best_lr()\n",
    "#     print(f\"Best Learning Rate for NP: {best_lr_NP}\")\n",
    "\n",
    "#     config['best_lr'] = best_lr_NP\n",
    "    config['best_lr'] = 1e-4\n",
    "    # Run and save the pipeline data\n",
    "    test_losses, r2, experiment_id = run_and_save_pipeline_np(train_loader, val_loader, test_loader,\\\n",
    "                                               input_channel, epochs, val_every_epoch, config, num_runs, device)\n",
    "\n",
    "else: # Testing\n",
    "    experiment_id = config['experiment_id']\n",
    "    if not os.path.exists(f'./results/neural_processes/{experiment_id}'):\n",
    "        raise Exception(f\"Could not find experiment with id: {experiment_id}\")\n",
    "    else:\n",
    "        neural_processes = NeuralProcessImg(r_dim, z_dim, h_dim).to(device)\n",
    "        try:\n",
    "            neural_processes.load_state_dict(torch.load(f'./results/neural_processes/{experiment_id}/best_model_NP.pth'))\n",
    "        except:\n",
    "            raise Exception(\"The model you provided does not correspond with the selected architecture. Please revise and try again.\")\n",
    "        \n",
    "        \n",
    "        filename = f\"test_{folder}_{partial_percent}\"\n",
    "        with open(f\"./results/neural_processes/{experiment_id}/{filename}\", \"w\") as f:\n",
    "            f.write(f\"MSE {best_MSE_test_loss}; NPP {best_NPP_test_loss}, {GP_best_NPP_test_loss} (GP)\")\n",
    "            \n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"Time elapsed:\", elapsed_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229220c4-86c2-427c-bb91-57502b26cec9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pycox",
   "language": "python",
   "name": "pycox"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
