{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81f69dfd-eb80-4504-8950-8247e590a908",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-18T00:51:53.678041Z",
     "start_time": "2024-02-18T00:51:51.874885Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.optim import Adam\n",
    "from torchvision.transforms import transforms, Compose, ToTensor, Lambda\n",
    "from torchvision.datasets.mnist import MNIST, FashionMNIST\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from tools.models import *\n",
    "from tools.data_utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import imageio\n",
    "from argparse import ArgumentParser\n",
    "import einops\n",
    "from tools.plot_utils import show_images, show_forward, generate_new_images\n",
    "from tools.models import Autoencoder\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "# Setting reproducibility\n",
    "SEED = 0\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Definitions\n",
    "STORE_PATH_MNIST = f\"./history/ddpm_model_mnist.pt\"\n",
    "STORE_PATH_SYNTH = f\"./history/ddpm_model_synth.pt\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df3f446c-842c-402f-8b95-904157154838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNet(\n",
      "  (time_embed): Embedding(1000, 100)\n",
      "  (te1): Sequential(\n",
      "    (0): Linear(in_features=100, out_features=4, bias=True)\n",
      "    (1): SiLU()\n",
      "    (2): Linear(in_features=4, out_features=4, bias=True)\n",
      "  )\n",
      "  (b1): Sequential(\n",
      "    (0): MyBlock(\n",
      "      (ln): LayerNorm((4, 100, 100), eps=1e-05, elementwise_affine=True)\n",
      "      (conv1): Conv2d(4, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (activation): SiLU()\n",
      "    )\n",
      "    (1): MyBlock(\n",
      "      (ln): LayerNorm((10, 100, 100), eps=1e-05, elementwise_affine=True)\n",
      "      (conv1): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (activation): SiLU()\n",
      "    )\n",
      "    (2): MyBlock(\n",
      "      (ln): LayerNorm((10, 100, 100), eps=1e-05, elementwise_affine=True)\n",
      "      (conv1): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (activation): SiLU()\n",
      "    )\n",
      "  )\n",
      "  (down1): Conv2d(10, 10, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (te2): Sequential(\n",
      "    (0): Linear(in_features=100, out_features=10, bias=True)\n",
      "    (1): SiLU()\n",
      "    (2): Linear(in_features=10, out_features=10, bias=True)\n",
      "  )\n",
      "  (b2): Sequential(\n",
      "    (0): MyBlock(\n",
      "      (ln): LayerNorm((10, 50, 50), eps=1e-05, elementwise_affine=True)\n",
      "      (conv1): Conv2d(10, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (activation): SiLU()\n",
      "    )\n",
      "    (1): MyBlock(\n",
      "      (ln): LayerNorm((20, 50, 50), eps=1e-05, elementwise_affine=True)\n",
      "      (conv1): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (activation): SiLU()\n",
      "    )\n",
      "    (2): MyBlock(\n",
      "      (ln): LayerNorm((20, 50, 50), eps=1e-05, elementwise_affine=True)\n",
      "      (conv1): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (activation): SiLU()\n",
      "    )\n",
      "  )\n",
      "  (down2): Conv2d(20, 20, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (te3): Sequential(\n",
      "    (0): Linear(in_features=100, out_features=20, bias=True)\n",
      "    (1): SiLU()\n",
      "    (2): Linear(in_features=20, out_features=20, bias=True)\n",
      "  )\n",
      "  (b3): Sequential(\n",
      "    (0): MyBlock(\n",
      "      (ln): LayerNorm((20, 25, 25), eps=1e-05, elementwise_affine=True)\n",
      "      (conv1): Conv2d(20, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (activation): SiLU()\n",
      "    )\n",
      "    (1): MyBlock(\n",
      "      (ln): LayerNorm((40, 25, 25), eps=1e-05, elementwise_affine=True)\n",
      "      (conv1): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (activation): SiLU()\n",
      "    )\n",
      "    (2): MyBlock(\n",
      "      (ln): LayerNorm((40, 25, 25), eps=1e-05, elementwise_affine=True)\n",
      "      (conv1): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (activation): SiLU()\n",
      "    )\n",
      "  )\n",
      "  (down3): Sequential(\n",
      "    (0): Conv2d(40, 40, kernel_size=(2, 2), stride=(1, 1))\n",
      "    (1): SiLU()\n",
      "    (2): Conv2d(40, 40, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  )\n",
      "  (te_mid): Sequential(\n",
      "    (0): Linear(in_features=100, out_features=40, bias=True)\n",
      "    (1): SiLU()\n",
      "    (2): Linear(in_features=40, out_features=40, bias=True)\n",
      "  )\n",
      "  (b_mid): Sequential(\n",
      "    (0): MyBlock(\n",
      "      (ln): LayerNorm((40, 12, 12), eps=1e-05, elementwise_affine=True)\n",
      "      (conv1): Conv2d(40, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (activation): SiLU()\n",
      "    )\n",
      "    (1): MyBlock(\n",
      "      (ln): LayerNorm((20, 12, 12), eps=1e-05, elementwise_affine=True)\n",
      "      (conv1): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (activation): SiLU()\n",
      "    )\n",
      "    (2): MyBlock(\n",
      "      (ln): LayerNorm((20, 12, 12), eps=1e-05, elementwise_affine=True)\n",
      "      (conv1): Conv2d(20, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (activation): SiLU()\n",
      "    )\n",
      "  )\n",
      "  (up1): Sequential(\n",
      "    (0): ConvTranspose2d(40, 40, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): SiLU()\n",
      "    (2): ConvTranspose2d(40, 40, kernel_size=(2, 2), stride=(1, 1))\n",
      "  )\n",
      "  (te4): Sequential(\n",
      "    (0): Linear(in_features=100, out_features=80, bias=True)\n",
      "    (1): SiLU()\n",
      "    (2): Linear(in_features=80, out_features=80, bias=True)\n",
      "  )\n",
      "  (b4): Sequential(\n",
      "    (0): MyBlock(\n",
      "      (ln): LayerNorm((80, 25, 25), eps=1e-05, elementwise_affine=True)\n",
      "      (conv1): Conv2d(80, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (activation): SiLU()\n",
      "    )\n",
      "    (1): MyBlock(\n",
      "      (ln): LayerNorm((40, 25, 25), eps=1e-05, elementwise_affine=True)\n",
      "      (conv1): Conv2d(40, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (activation): SiLU()\n",
      "    )\n",
      "    (2): MyBlock(\n",
      "      (ln): LayerNorm((20, 25, 25), eps=1e-05, elementwise_affine=True)\n",
      "      (conv1): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (activation): SiLU()\n",
      "    )\n",
      "  )\n",
      "  (up2): ConvTranspose2d(20, 20, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (te5): Sequential(\n",
      "    (0): Linear(in_features=100, out_features=40, bias=True)\n",
      "    (1): SiLU()\n",
      "    (2): Linear(in_features=40, out_features=40, bias=True)\n",
      "  )\n",
      "  (b5): Sequential(\n",
      "    (0): MyBlock(\n",
      "      (ln): LayerNorm((40, 50, 50), eps=1e-05, elementwise_affine=True)\n",
      "      (conv1): Conv2d(40, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (activation): SiLU()\n",
      "    )\n",
      "    (1): MyBlock(\n",
      "      (ln): LayerNorm((20, 50, 50), eps=1e-05, elementwise_affine=True)\n",
      "      (conv1): Conv2d(20, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (activation): SiLU()\n",
      "    )\n",
      "    (2): MyBlock(\n",
      "      (ln): LayerNorm((10, 50, 50), eps=1e-05, elementwise_affine=True)\n",
      "      (conv1): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (activation): SiLU()\n",
      "    )\n",
      "  )\n",
      "  (up3): ConvTranspose2d(10, 10, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (te_out): Sequential(\n",
      "    (0): Linear(in_features=100, out_features=20, bias=True)\n",
      "    (1): SiLU()\n",
      "    (2): Linear(in_features=20, out_features=20, bias=True)\n",
      "  )\n",
      "  (b_out): Sequential(\n",
      "    (0): MyBlock(\n",
      "      (ln): LayerNorm((20, 100, 100), eps=1e-05, elementwise_affine=True)\n",
      "      (conv1): Conv2d(20, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (activation): SiLU()\n",
      "    )\n",
      "    (1): MyBlock(\n",
      "      (ln): LayerNorm((10, 100, 100), eps=1e-05, elementwise_affine=True)\n",
      "      (conv1): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (activation): SiLU()\n",
      "    )\n",
      "    (2): MyBlock(\n",
      "      (ln): LayerNorm((10, 100, 100), eps=1e-05, elementwise_affine=True)\n",
      "      (conv1): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (activation): SiLU()\n",
      "    )\n",
      "  )\n",
      "  (conv_out): Conv2d(10, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "unet = UNet(4, shape=100)\n",
    "print(unet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f4f6309-022d-4db3-9b22-0bc481d7b6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Define the input dimensions\n",
    "batch_size = 2\n",
    "channels = 4\n",
    "height = 100\n",
    "width = 100\n",
    "\n",
    "# Generate random inputs for testing\n",
    "input_data = torch.randn(batch_size, channels, height, width)\n",
    "n_steps = 1000\n",
    "time_steps = torch.randint(0, n_steps, (batch_size,))\n",
    "output = unet(input_data, time_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "614b7d10-f19b-42b5-a42f-2c59d1d31f04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.9269,  1.4873,  0.9007,  ...,  0.4880,  0.7846,  0.0286],\n",
       "         [ 0.6408,  0.5832,  1.0669,  ...,  0.3581,  0.4788,  1.3537],\n",
       "         [ 0.5261,  2.1120, -0.5208,  ...,  0.2539,  0.9364,  0.7122],\n",
       "         ...,\n",
       "         [-0.2736,  1.4906,  0.8558,  ...,  0.7944,  0.0610,  0.0932],\n",
       "         [-0.4450, -0.1099, -0.5998,  ..., -0.1095,  1.8557, -2.2152],\n",
       "         [ 0.9302,  1.6466,  1.3799,  ...,  0.8227, -0.8546,  0.7941]],\n",
       "\n",
       "        [[-1.1723,  0.4068, -0.6165,  ..., -0.0192,  0.0207, -0.2878],\n",
       "         [-0.5640, -0.1440, -0.5088,  ..., -1.0222,  0.5673, -1.1977],\n",
       "         [-0.0802,  0.5409, -0.4572,  ..., -0.0517, -0.4695, -0.3873],\n",
       "         ...,\n",
       "         [ 0.8734,  0.8646,  0.0357,  ..., -0.3075,  1.3650,  0.5020],\n",
       "         [-0.8316,  0.6496,  0.0167,  ...,  0.6172, -1.9147, -0.9924],\n",
       "         [ 1.4250, -0.6707,  0.3655,  ...,  0.6578, -0.6725, -1.2849]],\n",
       "\n",
       "        [[ 1.4459,  0.4772, -0.5228,  ...,  0.1008,  0.6087,  1.0949],\n",
       "         [-0.6739,  1.1621,  1.1202,  ..., -1.7153, -0.2934,  0.6504],\n",
       "         [-1.4390,  2.0600,  0.6876,  ...,  0.5865,  1.3265, -0.7136],\n",
       "         ...,\n",
       "         [ 1.6553, -2.1122, -1.3458,  ..., -0.4699, -0.8730,  0.0389],\n",
       "         [ 0.9340, -0.9786,  1.7162,  ...,  0.7260, -0.3035,  0.5816],\n",
       "         [ 1.3330, -2.1475,  1.4343,  ...,  0.0116,  1.3838,  1.8371]],\n",
       "\n",
       "        [[-2.2203,  1.0216,  0.3616,  ..., -0.3258,  1.8332,  0.3409],\n",
       "         [ 0.2532,  0.6610, -1.2548,  ...,  0.0140, -1.8898,  0.6219],\n",
       "         [ 1.5294,  1.3300, -0.5021,  ..., -0.5077,  1.1148, -0.2436],\n",
       "         ...,\n",
       "         [ 1.0626, -0.4799,  0.3451,  ..., -0.0253,  0.1694,  0.0814],\n",
       "         [-0.4084, -0.2451,  2.2626,  ...,  0.0137, -0.8066, -1.8169],\n",
       "         [-0.8408, -0.8605, -0.8146,  ..., -0.6758,  0.7128,  1.2584]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "468a1255-f8b1-40d6-af3b-bdc335f41fc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.1395e-02,  2.0801e-02,  1.8702e-02,  ...,  1.6494e-02,\n",
       "           1.6122e-02,  2.2055e-02],\n",
       "         [ 2.8393e-02,  3.2001e-02,  3.0398e-02,  ...,  2.7426e-02,\n",
       "           2.7457e-02,  2.9665e-02],\n",
       "         [ 2.6516e-02,  3.2388e-02,  2.8632e-02,  ...,  2.6351e-02,\n",
       "           2.7034e-02,  2.7365e-02],\n",
       "         ...,\n",
       "         [ 2.5700e-02,  3.0168e-02,  2.6019e-02,  ...,  2.5457e-02,\n",
       "           2.4742e-02,  2.7002e-02],\n",
       "         [ 2.7601e-02,  3.3288e-02,  2.9509e-02,  ...,  2.8398e-02,\n",
       "           2.7227e-02,  2.7824e-02],\n",
       "         [ 3.2938e-02,  3.1317e-02,  2.9038e-02,  ...,  2.9103e-02,\n",
       "           2.8610e-02,  2.9180e-02]],\n",
       "\n",
       "        [[ 1.0974e-01,  1.0383e-01,  1.0372e-01,  ...,  1.0300e-01,\n",
       "           1.0125e-01,  1.0323e-01],\n",
       "         [ 1.0474e-01,  9.7586e-02,  9.7670e-02,  ...,  9.8008e-02,\n",
       "           9.3648e-02,  9.7954e-02],\n",
       "         [ 1.0564e-01,  9.8228e-02,  9.8884e-02,  ...,  9.5183e-02,\n",
       "           9.3588e-02,  9.7373e-02],\n",
       "         ...,\n",
       "         [ 1.0610e-01,  9.9402e-02,  9.8501e-02,  ...,  9.6128e-02,\n",
       "           9.2134e-02,  9.7779e-02],\n",
       "         [ 1.0453e-01,  9.7683e-02,  9.6283e-02,  ...,  9.4588e-02,\n",
       "           9.0797e-02,  9.7391e-02],\n",
       "         [ 1.0034e-01,  9.7097e-02,  9.6565e-02,  ...,  9.5651e-02,\n",
       "           9.1290e-02,  9.9113e-02]],\n",
       "\n",
       "        [[-2.0855e-02, -2.2321e-02, -2.0656e-02,  ..., -2.1315e-02,\n",
       "          -2.1281e-02, -1.5115e-02],\n",
       "         [-2.9925e-02, -3.9008e-02, -3.7992e-02,  ..., -3.7973e-02,\n",
       "          -3.4781e-02, -2.5364e-02],\n",
       "         [-2.9241e-02, -3.7214e-02, -3.7113e-02,  ..., -3.6432e-02,\n",
       "          -3.5908e-02, -2.4937e-02],\n",
       "         ...,\n",
       "         [-3.0926e-02, -4.0208e-02, -4.0117e-02,  ..., -4.0510e-02,\n",
       "          -3.7423e-02, -2.5658e-02],\n",
       "         [-3.0466e-02, -3.8438e-02, -3.9767e-02,  ..., -3.8434e-02,\n",
       "          -3.6736e-02, -2.4397e-02],\n",
       "         [-1.1205e-02, -1.7867e-02, -1.8448e-02,  ..., -1.8774e-02,\n",
       "          -1.6428e-02, -1.4122e-02]],\n",
       "\n",
       "        [[-5.0904e-03, -3.3816e-03, -4.3882e-03,  ..., -3.4453e-03,\n",
       "          -1.4178e-03, -1.4403e-03],\n",
       "         [-1.5096e-03,  1.8318e-03, -3.0121e-04,  ...,  3.5743e-03,\n",
       "           8.1096e-03,  8.3388e-03],\n",
       "         [-4.2659e-03,  1.0512e-05, -1.6144e-03,  ...,  3.8100e-03,\n",
       "           7.5922e-03,  9.1245e-03],\n",
       "         ...,\n",
       "         [-3.8161e-03, -3.0835e-05, -2.2240e-03,  ...,  1.0690e-03,\n",
       "           5.4423e-03,  6.5601e-03],\n",
       "         [-2.9020e-03,  2.6552e-03,  1.3579e-03,  ...,  3.6958e-03,\n",
       "           6.0960e-03,  8.5478e-03],\n",
       "         [-2.0629e-03,  2.9799e-03,  2.6357e-03,  ...,  3.1226e-03,\n",
       "           6.1785e-03,  6.3088e-03]]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d956c195-0b6c-4fb0-ac09-522c9000278e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-18T00:51:57.332522Z",
     "start_time": "2024-02-18T00:51:57.326965Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "\n",
    "def training_loop(dataset, ddpm, loader, n_epochs, optim, device, display=False, store_path=\"ddpm_model.pt\"):\n",
    "    mse = nn.MSELoss()\n",
    "    best_loss = float(\"inf\")\n",
    "    n_steps = ddpm.n_steps\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0\n",
    "        for data in loader:\n",
    "            # Loading data\n",
    "            if dataset ==\"MNIST\":\n",
    "                x0 = data[0].to(device)\n",
    "            else:\n",
    "                x0 = data[\"image\"].float().to(device)\n",
    "            n = len(x0)\n",
    "\n",
    "            # Picking some noise for each of the images in the batch, a timestep and the respective alpha_bars\n",
    "            eta = torch.randn_like(x0).to(device)\n",
    "            t = torch.randint(0, n_steps, (n,)).to(device)\n",
    "\n",
    "            # Computing the noisy image based on x0 and the time-step (forward process)\n",
    "            noisy_imgs = ddpm(x0, t, eta)\n",
    "\n",
    "            # Getting model estimation of noise based on the images and the time-step\n",
    "            eta_theta, _ = ddpm.backward(noisy_imgs, t.reshape(n, -1))\n",
    "\n",
    "            # Optimizing the MSE between the noise plugged and the predicted noise\n",
    "            loss = mse(eta_theta, eta)\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            train_loss += loss.item() * len(x0) / len(loader.dataset)\n",
    "\n",
    "        # Display images generated at this epoch\n",
    "        if display:\n",
    "            show_images(generate_new_images(ddpm, device=device), f\"Images generated at epoch {epoch + 1}\")\n",
    "\n",
    "        log_string = f\"Loss at epoch {epoch + 1}: {train_loss:.3f}\"\n",
    "    \n",
    "        # Storing the model\n",
    "        if best_loss > train_loss:\n",
    "            best_loss = train_loss\n",
    "            torch.save(ddpm.state_dict(), store_path)\n",
    "            log_string += \" --> Best model ever (stored)\"\n",
    "\n",
    "        print(log_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e31a885-d47a-461e-b1df-511530aa3272",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "dataset = \"Building\"\n",
    "store_path = f\"./history/ddpm_model_{dataset}.pt\"\n",
    "batch_size = 128\n",
    "learning_rate = 0.001\n",
    "num_epochs = 20\n",
    "input_channel = 1 if dataset == \"PinMNIST\" else 4\n",
    "\n",
    "# Loading the data (converting each image into a tensor and normalizing between [-1, 1])\n",
    "if dataset == \"Building\":\n",
    "    resize = Resize100\n",
    "else:\n",
    "    resize = Resize\n",
    "    \n",
    "if dataset == \"MNIST\":\n",
    "    ds_fn = MNIST\n",
    "    dataset_fn = ds_fn(\"./datasets\", download=True, train=True, transform=transform)\n",
    "    train_dataloader = DataLoader(dataset_fn, batch_size, shuffle=True)\n",
    "\n",
    "    if dataset == \"Building\":\n",
    "        resize = Resize100\n",
    "    else:\n",
    "        resize = Resize\n",
    "elif dataset == \"Synthetic\":\n",
    "    # Use enough images to get great generation\n",
    "    data_folder = \"./data/Synthetic/10images_28by28pixels_4_distanced_grid_pins_4seed\"\n",
    "    dataset_fn = PinDataset(csv_file=f\"{data_folder}/pins.csv\",\n",
    "                                  root_dir=f\"{data_folder}/images/\",\n",
    "                                  transform=transforms.Compose([ToTensor(), resize()]))\n",
    "    train_dataloader = DataLoader(dataset_fn, batch_size, shuffle=True)\n",
    "    \n",
    "elif dataset == \"Building\":\n",
    "    transformed_dataset = PinDataset(csv_file=f\"./data/{dataset}/pins_full.csv\",\n",
    "                             root_dir=f\"./data/{dataset}/PS-RGBNIR/\",\n",
    "                             transform=Compose([ToTensor(), resize(), Lambda()]))\n",
    "            \n",
    "if os.path.exists(f\"./data/{dataset}/train_indices.npy\"):\n",
    "    train_indices = np.load(f'./data/{dataset}/train_indices.npy')\n",
    "    val_indices = np.load(f'./data/{dataset}/val_indices.npy')\n",
    "    test_indices = np.load(f'./data/{dataset}/test_indices.npy')\n",
    "    train_indices = np.concatenate((train_indices, np.arange(1000, 1697))) # add all un-used images for DDPM training\n",
    "    # Use the indices to create new datasets\n",
    "    train_dataset = Subset(transformed_dataset, train_indices)\n",
    "    val_dataset = Subset(transformed_dataset, val_indices)\n",
    "    test_dataset = Subset(transformed_dataset, test_indices)\n",
    "else:\n",
    "    # Split the dataset into train, validation, and test sets\n",
    "    train_dataset, val_dataset, test_dataset = random_split(\n",
    "        transformed_dataset, [train_size, val_size, test_size]\n",
    "    )\n",
    "    np.save(f'./data/{dataset}/train_indices.npy', train_dataset.indices)\n",
    "    np.save(f'./data/{dataset}/val_indices.npy', val_dataset.indices)\n",
    "    np.save(f'./data/{dataset}/test_indices.npy', test_dataset.indices)\n",
    "\n",
    "# Create your DataLoader with the custom_collate_fn\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=custom_collate_fn)\n",
    "\n",
    "# Initialize the autoencoder\n",
    "n_steps, min_beta, max_beta = 1000, 10 ** -4, 0.02  # Originally used by the authors\n",
    "if dataset == \"Building\":\n",
    "    model = DDPM(UNet(input_channel, shape=100), n_steps=n_steps, min_beta=min_beta, max_beta=max_beta, device=device)\n",
    "else:\n",
    "    model = DDPM(UNet(input_channel), n_steps=n_steps, min_beta=min_beta, max_beta=max_beta, device=device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "training_loop(dataset, model, train_loader, num_epochs, optimizer, device, store_path=store_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf741ccb-dba1-47e5-8332-7a8ace92ddd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "count = 0\n",
    "for batch in train_loader:\n",
    "    images = batch['image'].to(device) # get RGB instead of RGBA\n",
    "    pins = batch['pins']\n",
    "    outputs = batch['outputs']\n",
    "    print(count, len(images))\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435ce792-3ad7-415c-a9c2-f30cdcea198a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the trained model\n",
    "store_path = f\"./history/ddpm_model_{dataset}.pt\"\n",
    "best_model = DDPM(UNet(), n_steps=n_steps, device=device)\n",
    "best_model.load_state_dict(torch.load(store_path, map_location=device))\n",
    "best_model.eval()\n",
    "print(\"Model loaded\")\n",
    "\n",
    "print(\"Generating new images\")\n",
    "generated = generate_new_images(\n",
    "        best_model,\n",
    "        n_samples=100,\n",
    "        device=device,\n",
    "        gif_name=\"mnist.gif\"\n",
    "    )\n",
    "show_images(generated, \"Final result\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab02adbd-4de1-4b59-8e03-42d94793b676",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "show_forward(model, train_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1327841-1e33-49dd-9d44-00715ebdedae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visual evaluation on the test set\n",
    "\n",
    "# change for dataset\n",
    "num_kernels_encoder = [16, 8]\n",
    "num_kernels_decoder = [16]\n",
    "input_channel = 1\n",
    "\n",
    "test_dataloader = DataLoader(dataset, batch_size, shuffle=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_dataloader:\n",
    "        inputs, _ = data\n",
    "        inputs = inputs.view(inputs.size(0), -1)\n",
    "        \n",
    "        # Forward pass through the autoencoder\n",
    "        autoencoder = Autoencoder(num_kernels_encoder, num_kernels_decoder, input_channel=input_channel).to(device)\n",
    "        outputs = autoencoder(inputs)\n",
    "        \n",
    "        # Reshape the outputs to the original image shape (28x28)\n",
    "        outputs = outputs.view(outputs.size(0), 1, 28, 28)\n",
    "        \n",
    "        # Convert Tensors to NumPy arrays for visualization\n",
    "        inputs = inputs.view(inputs.size(0), 1, 28, 28).numpy()\n",
    "        outputs = outputs.numpy()\n",
    "        \n",
    "        # Plot the input and output images\n",
    "        for i in range(inputs.shape[0]):\n",
    "            plt.figure(figsize=(8, 4))\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.title('Input Image')\n",
    "            plt.imshow(np.squeeze(inputs[i]), cmap='gray')\n",
    "            plt.axis('off')\n",
    "            \n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.title('Output Image')\n",
    "            plt.imshow(np.squeeze(outputs[i]), cmap='gray')\n",
    "            plt.axis('off')\n",
    "            plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pycox",
   "language": "python",
   "name": "pycox"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
