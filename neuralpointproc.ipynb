{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3836b3f8-f993-415f-b442-f772829e47e6",
   "metadata": {},
   "source": [
    "Neural Point Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca94db99-f1a0-448f-91d0-d6b98ff5bd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d8cfc5-3e51-49d0-b091-52a4fc07b672",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SynthHeatmapDataset(Dataset):\n",
    "    \"\"\"Synthetic Heatmaps dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.pins_frame = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pins_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = os.path.join(self.root_dir,\n",
    "                                self.pins_frame.iloc[idx, 0])\n",
    "        image = io.imread(img_name)\n",
    "        pins = np.asarray(eval(self.pins_frame.iloc[idx, 1]))\n",
    "        outputs = np.asarray(eval(self.pins_frame.iloc[idx, 2]))\n",
    "\n",
    "        # sample = {'image': image, 'pins': pins, 'outputs': outputs}\n",
    "\n",
    "        if self.transform:\n",
    "            image, outputs, pins = self.transform(image, outputs, pins)\n",
    "\n",
    "        return image, outputs, pins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f583c85b-b4d0-4043-80d8-2267fa476037",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, image, outputs, pins):\n",
    "        # image, pins, outputs = sample['image'], sample['pins'], sample['outputs']\n",
    "\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C x H x W\n",
    "        image = image.transpose((2, 0, 1))\n",
    "        image = torch.from_numpy(image)\n",
    "        outputs = torch.from_numpy(outputs)\n",
    "        pins = torch.from_numpy(pins)\n",
    "        return image, outputs, pins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248b01de-976f-4b35-be40-a3faf4ef9672",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_dataset = SynthHeatmapDataset(csv_file=f\"{data_folder}/pins.csv\",\n",
    "                                          root_dir=f\"{data_folder}/images/\",\n",
    "                                          transform=transforms.Compose([ToTensor()])\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0924c3f-3735-4c1f-b940-f902f4ccd25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "train_data = \n",
    "test_data = \n",
    "val_data = \n",
    "\n",
    "x_train, y_train, p_train = DataLoader(transformed_dataset, batch_size=4, shuffle=True, num_workers=0)\n",
    "x_val, y_val, p_val = \n",
    "x_test, y_test, p_test = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d358a39b-4e43-40fe-8a6d-f3b3c4894579",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_kernel_matrix(X, Y, sigma):\n",
    "    \"\"\"\n",
    "    Calculate the Gaussian kernel matrix between two sets of PyTorch tensors X and Y.\n",
    "\n",
    "    Parameters:\n",
    "    X (torch.Tensor): First set of tensors with shape (n, d), where n is the number of vectors and d is the dimensionality.\n",
    "    Y (torch.Tensor): Second set of tensors with shape (m, d), where m is the number of vectors and d is the dimensionality.\n",
    "    sigma (float): The kernel bandwidth parameter.\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: The Gaussian kernel matrix of shape (n, m).\n",
    "    \"\"\"\n",
    "    if X.size(1) != Y.size(1):\n",
    "        raise ValueError(\"Input tensors must have the same dimension\")\n",
    "\n",
    "    n, m = X.size(0), Y.size(0)\n",
    "    X = X.unsqueeze(1)  # Shape (n, 1, d)\n",
    "    Y = Y.unsqueeze(0)  # Shape (1, m, d)\n",
    "\n",
    "    diff = torch.norm(X - Y, dim=2)  # Pairwise Euclidean distances between vectors\n",
    "    return torch.exp(- (diff ** 2) / (2 * sigma ** 2))\n",
    "\n",
    "\n",
    "def pseudo_inverse(kernel_matrix, epsilon=1e-5):\n",
    "    \"\"\"\n",
    "    Calculate the pseudo-inverse of a matrix using Singular Value Decomposition (SVD).\n",
    "\n",
    "    Parameters:\n",
    "    kernel_matrix (torch.Tensor): The matrix for which to compute the pseudo-inverse.\n",
    "    epsilon (float): A small value to avoid division by zero.\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: The pseudo-inverse of the input matrix.\n",
    "    \"\"\"\n",
    "    U, S, V = torch.svd(kernel_matrix)\n",
    "    S_inv = 1.0 / (S + epsilon)\n",
    "    pseudo_inv = torch.mm(V, torch.mm(torch.diag(S_inv), U.t()))\n",
    "    return pseudo_inv\n",
    "\n",
    "\n",
    "class NPPLoss(nn.Module):\n",
    "    def __init__(self, matrix):\n",
    "        super(NPPLoss, self).__init__()\n",
    "        self.matrix_list = None  # Initialize with None or an empty list\n",
    "    \n",
    "    def compute_kernel(self, pins, sigma=1.0):\n",
    "        matrix_list = []\n",
    "        for i in range(len(pins)):\n",
    "            X = Y = pins[i]\n",
    "            kernel_matrix = gaussian_kernel_matrix(X, Y, sigma)\n",
    "            pseudo_inv_matrix = pseudo_inverse(kernel_matrix)\n",
    "            matrix_list.append(pseudo_inv_matrix)\n",
    "        self.matrix_list = matrix_list  # Store the computed matrix list in self\n",
    "    \n",
    "    def forward(self, y_true, y_pred, pins):\n",
    "        loss = 0\n",
    "        \n",
    "        self.compute_kernel(pins)  # Use self.compute_kernel to compute matrix_list\n",
    "        for i in range(len(y_true)):  # Fix the loop\n",
    "            loss += torch.matmul((y_true[i] - y_pred[i][pins]).t(), torch.matmul(self.matrix_list[i], y_true[i] - y_pred[i][pins]))\n",
    "\n",
    "        return loss\n",
    "    \n",
    "class NPPLoss(nn.Module):\n",
    "    def __init__(self, pin=None, memorize=False):\n",
    "        super(NPPLoss, self).__init__()\n",
    "        self.memorize = memorize\n",
    "        if self.memorize:\n",
    "            self.matrix_list = compute_kernel(pins)\n",
    "\n",
    "    def compute_kernel(self, pins, sigma=1.0):\n",
    "        matrix_list = []\n",
    "        for i in range(len(pins)):\n",
    "            X = Y = pins[i]\n",
    "            kernel_matrix = gaussian_kernel_matrix(X, Y, sigma)\n",
    "            pseudo_inv_matrix = pseudo_inverse(kernel_matrix)\n",
    "            matrix_list.append(pseudo_inv_matrix)\n",
    "        return matrix_list\n",
    "        \n",
    "    def forward(self, y_true, y_pred, pins):\n",
    "        loss = 0\n",
    "        if self.memorize:  # Check the memorize flag\n",
    "            for i in range(len(y_true)):\n",
    "                loss += torch.matmul((y_true[i] - y_pred[i][pins]).t(), torch.matmul(self.matrix_list[i], y_true[i] - y_pred[i][pins]))\n",
    "        else:\n",
    "            matrix_list = self.compute_kernel(pins)  # Compute matrix_list for each forward pass\n",
    "            for i in range(len(y_true)):\n",
    "                loss += torch.matmul((y_true[i] - y_pred[i][pins]).t(), torch.matmul(matrix_list[i], y_true[i] - y_pred[i][pins]))\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc7f19e-62c4-407d-a1d0-385f207d3533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the encoder architecture\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),  # Input: 1 channel (grayscale), Output: 32 channels\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),  # Input: 32 channels, Output: 64 channels\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return x\n",
    "\n",
    "# Define the decoder architecture\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2),  # Input: 64 channels, Output: 32 channels\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 1, kernel_size=2, stride=2),  # Input: 32 channels, Output: 1 channel (grayscale)\n",
    "            nn.Sigmoid(),  # Apply Sigmoid activation to constrain output in [0, 1]\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# Define the Autoencoder as a combination of Encoder and Decoder\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be64cbc1-70ba-4ecf-aa2b-5cf6771bff77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_size = 28 * 28  # MNIST image size (28x28 pixels)\n",
    "hidden_size = 64\n",
    "batch_size = 128\n",
    "learning_rate = 0.001\n",
    "num_epochs = 20\n",
    "\n",
    "# Load ??? dataset -- train data_loader\n",
    "data_loader = torch.utils.data.DataLoader(dataset=???, batch_size=batch_size, shuffle=True)\n",
    "# and also the test/val data_loader\n",
    "\n",
    "# Create an instance of the Autoencoder\n",
    "autoencoder = Autoencoder()\n",
    "\n",
    "#calculate the pseudo_inv once before the training.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = NPPLoss(matrix_list)\n",
    "baseline_criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(autoencoder.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for data in data_loader:\n",
    "        x_train, y_train, p_train = data\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = autoencoder(x_train)\n",
    "        loss = criterion(y_train, outputs, p_train)\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Print loss\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Visualize original and reconstructed images\n",
    "with torch.no_grad():\n",
    "    data_iter = iter(data_loader)\n",
    "    images, _ = next(data_iter)\n",
    "    images = images.view(images.size(0), -1)\n",
    "\n",
    "    reconstructed_images = autoencoder(images)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    for i in range(5):\n",
    "        # Original Images\n",
    "        plt.subplot(2, 5, i + 1)\n",
    "        plt.imshow(images[i].view(28, 28).numpy(), cmap='gray')\n",
    "        plt.title('Original')\n",
    "\n",
    "        # Reconstructed Images\n",
    "        plt.subplot(2, 5, i + 6)\n",
    "        plt.imshow(reconstructed_images[i].view(28, 28).numpy(), cmap='gray')\n",
    "        plt.title('Reconstructed')\n",
    "\n",
    "    plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
