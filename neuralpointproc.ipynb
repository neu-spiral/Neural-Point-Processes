{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3836b3f8-f993-415f-b442-f772829e47e6",
   "metadata": {},
   "source": [
    "Neural Point Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca94db99-f1a0-448f-91d0-d6b98ff5bd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from skimage import io, transform\n",
    "from functools import lru_cache\n",
    "from tools.plot_utils import visualize_pins, plot_label_pin, plot_all\n",
    "from tools.data_utils import count_all, count_pins\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_lr_finder import LRFinder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa53412-0c89-4617-9b7e-efffa9b63834",
   "metadata": {},
   "source": [
    "# Dataset and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "000f94a1-69f9-488f-840c-0a0079b1bb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a random seed for PyTorch\n",
    "seed = 4  # You can use any integer value as the seed\n",
    "torch.manual_seed(seed)\n",
    "# Set a random seed for NumPy (if you're using NumPy operations)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "770be29d-6a82-4394-96c9-8c37bae5e821",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"PinMNIST\" \n",
    "n = 500\n",
    "mesh = False\n",
    "d = 4\n",
    "n_pins = 20\n",
    "# fixed_pins = True\n",
    "r = 3\n",
    "d1,d2 = 28,28\n",
    "\n",
    "if dataset == \"PinMNIST\":\n",
    "    if mesh:\n",
    "        data_folder = f\"./data/MNIST_{n}images_mesh_{d}step_{28}by{28}pixels_{r}radius_{seed}seed\"\n",
    "    else:\n",
    "        data_folder = f\"./data/MNIST_{n}images_random_fixed{fixed_pins}_{n_pins}pins_{28}by{28}pixels_{r}radius_{seed}seed\"\n",
    "\n",
    "if dataset == \"Synthetic\":\n",
    "    if mesh:\n",
    "        data_folder = f\"./data/Synthetic_{n}images_{d1}by{d2}pixels_{d}_distanced_grid_pins_{seed}seed/\"\n",
    "    else:\n",
    "        data_folder = f\"./data/Synthetic_{n}images_{d1}by{d2}pixels_upto{n_pins}pins_{seed}seed/\"\n",
    "        \n",
    "    \n",
    "    \n",
    "# data_folder = f\"./data/MNIST_{n}images_mesh_{d}step_{28}by{28}pixels_{r}radius_{seed}seed\"\n",
    "# data_folder = \"./data/Synthetic_100images_32by32pixels_upto500pins_4seed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79d8cfc5-3e51-49d0-b091-52a4fc07b672",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PinDataset(Dataset):\n",
    "    \"\"\"Synthetic Heatmaps dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.pins_frame = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pins_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = os.path.join(self.root_dir,\n",
    "                                self.pins_frame.iloc[idx, 0])\n",
    "        image = io.imread(img_name)\n",
    "        pins = np.asarray(eval(self.pins_frame.iloc[idx, 1]))\n",
    "        outputs = np.asarray(eval(self.pins_frame.iloc[idx, 2]))\n",
    "\n",
    "        sample = {'image': image, 'pins': pins, 'outputs': outputs}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "    \n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, pins, outputs = sample['image'], sample['pins'], sample['outputs']\n",
    "\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C x H x W\n",
    "        if len(image.shape) == 2:\n",
    "            image = image.reshape(image.shape[0], image.shape[1], 1)\n",
    "        image = image.transpose((2, 0, 1))\n",
    "        image = image/image.max()\n",
    "        return {'image': torch.from_numpy(image),\n",
    "                'pins': torch.from_numpy(pins),\n",
    "                'outputs': torch.from_numpy(outputs).to(torch.float32)}\n",
    "    \n",
    "# Define a custom transform to resize the image\n",
    "class Resize(object):\n",
    "    def __call__(self, sample, size=(28,28)):\n",
    "        image, pins, outputs = sample['image'], sample['pins'], sample['outputs']\n",
    "        \n",
    "        # Resize the image to desired sized pixels\n",
    "        image = transforms.functional.resize(image, size)\n",
    "        \n",
    "        return {'image': image, 'pins': pins, 'outputs': outputs}\n",
    "\n",
    "# Create a transform pipeline that includes the resizing step\n",
    "if dataset == \"PinMNIST\":\n",
    "    transform = transforms.Compose([\n",
    "        ToTensor(),         # Convert to tensor (as you were doing)\n",
    "        Resize()  # Resize to 100x100\n",
    "    ])\n",
    "\n",
    "    transformed_dataset = PinDataset(csv_file=f\"{data_folder}/pins.csv\",\n",
    "                                          root_dir=f\"{data_folder}/images/\",\n",
    "                                          transform=transform)\n",
    "elif dataset == \"Synthetic\":\n",
    "    transform = transforms.Compose([\n",
    "        ToTensor(),         # Convert to tensor (as you were doing)\n",
    "        Resize(size=(128,128))  # Resize to 100x100\n",
    "    ])\n",
    "\n",
    "    transformed_dataset = SynthHeatmapDataset(csv_file=f\"{data_folder}/pins.csv\",\n",
    "                                          root_dir=f\"{data_folder}/images/\",\n",
    "                                          transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7eb005b6-e4cf-46fd-a9e4-b341249662e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = len(transformed_dataset)\n",
    "train_size = int(0.7 * dataset_size)\n",
    "val_size = int(0.1 * dataset_size)\n",
    "test_size = dataset_size - train_size - val_size\n",
    "\n",
    "# Split the dataset into train, validation, and test sets\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    transformed_dataset, [train_size, val_size, test_size]\n",
    ")\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 64\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    images = [sample['image'] for sample in batch]\n",
    "    pins = [sample['pins'] for sample in batch]\n",
    "    outputs = [sample['outputs'] for sample in batch]\n",
    "\n",
    "\n",
    "    return {\n",
    "        'image': torch.stack(images, dim=0),\n",
    "        'pins': pins,\n",
    "        'outputs': outputs}\n",
    "\n",
    "# Create your DataLoader with the custom_collate_fn\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=custom_collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=custom_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0924c3f-3735-4c1f-b940-f902f4ccd25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_dataloader:\n",
    "    images = batch['image']  # get RGB instead of RGBA\n",
    "    pins = batch['pins']     \n",
    "    outputs = batch['outputs']  \n",
    "    # print(images.shape, len(pins), len(outputs))\n",
    "    # Your training or evaluation code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ce377de-650d-4e4c-9520-437fc8752263",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1500x500 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_img = images[0].squeeze().detach().cpu()/255\n",
    "count_image = plot_label_pin(sample_img, pins[0], outputs[0])\n",
    "# label = count_pins(sample_img, pins[0], r)\n",
    "# count_all_image = plot_label_pin(sample_img,pins[0], label)\n",
    "count_all_image = plot_all(sample_img, r=3)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))  # You can adjust the figsize as needed\n",
    "\n",
    "# Plot the sample_img in the first subplot\n",
    "im0 = axes[0].imshow(sample_img)\n",
    "axes[0].set_title('Sample Image')\n",
    "fig.colorbar(im0, ax=axes[0], fraction=0.046, pad=0.04)  # Add colorbar\n",
    "\n",
    "# Plot the count_image in the second subplot\n",
    "im1 = axes[1].imshow(count_image)\n",
    "axes[1].set_title('Count Image')\n",
    "fig.colorbar(im1, ax=axes[1], fraction=0.046, pad=0.04)  # Add colorbar\n",
    "\n",
    "# Plot the count_all_image in the third subplot\n",
    "im2 = axes[2].imshow(count_all_image)\n",
    "axes[2].set_title('Count All Image')\n",
    "fig.colorbar(im2, ax=axes[2], fraction=0.046, pad=0.04)  # Add colorbar\n",
    "\n",
    "# Add spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c32d3c9-4be0-4212-a456-9b4d674dde30",
   "metadata": {},
   "source": [
    "# Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d358a39b-4e43-40fe-8a6d-f3b3c4894579",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_kernel_matrix(X, Y, sigma):\n",
    "    \"\"\"\n",
    "    Calculate the Gaussian kernel matrix between two sets of PyTorch tensors X and Y.\n",
    "\n",
    "    Parameters:\n",
    "    X (torch.Tensor): First set of tensors with shape (n, d), where n is the number of vectors and d is the dimensionality.\n",
    "    Y (torch.Tensor): Second set of tensors with shape (m, d), where m is the number of vectors and d is the dimensionality.\n",
    "    sigma (float): The kernel bandwidth parameter.\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: The Gaussian kernel matrix of shape (n, m).\n",
    "    \"\"\"\n",
    "    if X.size(1) != Y.size(1):\n",
    "        raise ValueError(\"Input tensors must have the same dimension\")\n",
    "\n",
    "    n, m = X.size(0), Y.size(0)\n",
    "    X = X.unsqueeze(1)  # Shape (n, 1, d)\n",
    "    Y = Y.unsqueeze(0)  # Shape (1, m, d)\n",
    "\n",
    "    diff = torch.norm(X - Y, dim=2)  # Pairwise Euclidean distances between vectors\n",
    "    return torch.exp(- (diff ** 2) / (2 * sigma ** 2))\n",
    "\n",
    "\n",
    "def pseudo_inverse(kernel_matrix, epsilon=1e-5):\n",
    "    \"\"\"\n",
    "    Calculate the pseudo-inverse of a matrix using Singular Value Decomposition (SVD).\n",
    "\n",
    "    Parameters:\n",
    "    kernel_matrix (torch.Tensor): The matrix for which to compute the pseudo-inverse.\n",
    "    epsilon (float): A small value to avoid division by zero.\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: The pseudo-inverse of the input matrix.\n",
    "    \"\"\"\n",
    "    U, S, V = torch.svd(kernel_matrix)\n",
    "    S_inv = 1.0 / (S + epsilon)\n",
    "    pseudo_inv = torch.mm(V, torch.mm(torch.diag(S_inv), U.t()))\n",
    "    return pseudo_inv\n",
    "\n",
    "\n",
    "class NPPLoss(nn.Module):\n",
    "    def __init__(self, identity, sigma=1.0):\n",
    "        super(NPPLoss, self).__init__()\n",
    "        self.identity = identity\n",
    "        self.sigma = sigma  # Add sigma as an instance variable\n",
    "    \n",
    "    @lru_cache(maxsize=None)\n",
    "    def compute_kernel(self, pins):\n",
    "        matrix_list = []\n",
    "        for i in range(len(pins)):\n",
    "            X = Y = pins[i].float()\n",
    "            kernel_matrix = gaussian_kernel_matrix(X, Y, self.sigma)  # Use self.sigma\n",
    "            pseudo_inv_matrix = pseudo_inverse(kernel_matrix)\n",
    "            matrix_list.append(pseudo_inv_matrix)\n",
    "        return matrix_list\n",
    "    \n",
    "    def forward(self, y_true, y_pred, pins):\n",
    "        loss = 0\n",
    "        if self.identity:\n",
    "            for i in range(len(y_true)):\n",
    "                loss += 1/len(y_true[i]) * torch.matmul(\n",
    "                    (y_true[i] - y_pred[i].squeeze()[pins[i][:,0], pins[i][:,1]]).t(),\n",
    "                    (y_true[i] - y_pred[i].squeeze()[pins[i][:,0], pins[i][:,1]])\n",
    "                )\n",
    "        else:\n",
    "            matrix_list = self.compute_kernel(tuple(pins))\n",
    "            for i in range(len(y_true)):\n",
    "                loss += 1/len(y_true[i]) * torch.matmul(\n",
    "                    (y_true[i] - y_pred[i].squeeze()[pins[i][:,0], pins[i][:,1]]).t(),\n",
    "                    torch.matmul(matrix_list[i], y_true[i] - y_pred[i].squeeze()[pins[i][:,0], pins[i][:,1]])\n",
    "                )\n",
    "        loss /= len(y_true)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72e8ca4-3587-4153-8e2b-af426245aac5",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8dc7f19e-62c4-407d-a1d0-385f207d3533",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, num_kernels_encoder, num_kernels_decoder, input_channel=3):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.input_channel = input_channel\n",
    "        self.encoder = self._build_encoder(num_kernels_encoder)\n",
    "        self.decoder = self._build_decoder(num_kernels_decoder, num_kernels_encoder[-1])\n",
    "        \n",
    "    def _build_encoder(self, num_kernels):\n",
    "        layers = []\n",
    "        \n",
    "        for out_channels in num_kernels:\n",
    "            layers.append(nn.Conv2d(self.input_channel, out_channels, kernel_size=3, padding=1))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "            self.input_channel = out_channels  # Update input_channel for the next layer\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def _build_decoder(self, num_kernels, input_channel):\n",
    "        layers = []\n",
    "        \n",
    "        for out_channels in num_kernels:\n",
    "            layers.append(nn.ConvTranspose2d(input_channel, out_channels, kernel_size=2, stride=2))\n",
    "            layers.append(nn.ReLU())\n",
    "            input_channel = out_channels\n",
    "        \n",
    "        layers.append(nn.ConvTranspose2d(input_channel, 1, kernel_size=2, stride=2))\n",
    "        layers.append(nn.ReLU())\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da5344f8-0726-4edb-b353-899393e22362",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStoppingCallback:\n",
    "    def __init__(self, patience=10, min_delta=0.001):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.wait = 0\n",
    "        self.best_loss = float('inf')\n",
    "\n",
    "    def __call__(self, epoch, val_loss):\n",
    "        if val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.wait = 0\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            if self.wait >= self.patience:\n",
    "                print(f\"Early stopping after {epoch} epochs.\")\n",
    "                return True  # Stop training\n",
    "        return False  # Continue training\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c306e1-061a-47bc-8584-46ad8e08ee8b",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa39fe1f-e261-4bc3-b8b4-75fc9ca989a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLRFinder:\n",
    "    def __init__(self, model, criterion, optimizer, device='cuda'):\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "        self.history = {'lr': [], 'loss': []}\n",
    "\n",
    "    def find_lr(self, train_loader, start_lr=1e-7, end_lr=10, num_iter=100, smooth_f=0.05):\n",
    "        model = self.model.to(self.device)\n",
    "        criterion = self.criterion\n",
    "        optimizer = self.optimizer\n",
    "        device = self.device\n",
    "        model.train()\n",
    "\n",
    "        lr_step = (end_lr / start_lr) ** (1 / num_iter)\n",
    "        lr = start_lr\n",
    "\n",
    "        for iteration in range(num_iter):\n",
    "            optimizer.param_groups[0]['lr'] = lr\n",
    "\n",
    "            total_loss = 0.0\n",
    "            for batch in train_loader:\n",
    "                x_train = batch['image'][:, :input_channel, :, :].to(device)\n",
    "                p_train = [tensor.to(device) for tensor in batch['pins']]\n",
    "                y_train = [tensor.to(device) for tensor in batch['outputs']]\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(x_train.float())\n",
    "                loss = criterion(y_train, outputs, p_train)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            avg_loss = total_loss / len(train_loader)\n",
    "            self.history['lr'].append(lr)\n",
    "            self.history['loss'].append(avg_loss)\n",
    "\n",
    "            lr *= lr_step\n",
    "\n",
    "    def find_best_lr(self, skip_start=10, skip_end=5):\n",
    "        # Find the index of the minimum loss in the specified range\n",
    "        min_loss_index = skip_start + np.argmin(self.history['loss'][skip_start:-skip_end])\n",
    "\n",
    "        # Output the learning rate corresponding to the minimum loss\n",
    "        best_lr = self.history['lr'][min_loss_index]\n",
    "        return best_lr\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce40744e-f190-427b-a0c9-19a163a1a7c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_dataloader, val_dataloader, num_epochs, val_every_epoch, learning_rate, criterion, optimizer, device, early_stopping):\n",
    "    train_losses = []  # To track train loss for plotting\n",
    "    val_losses = []    # To track validation loss for plotting\n",
    "    best_val_loss = float('inf') \n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for batch in train_dataloader:\n",
    "            x_train = batch['image'][:, :input_channel, :, :].to(device)\n",
    "            p_train = [tensor.to(device) for tensor in batch['pins']]\n",
    "            y_train = [tensor.to(device) for tensor in batch['outputs']] \n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(x_train.float())\n",
    "            loss = criterion(y_train, outputs, p_train)\n",
    "\n",
    "            # Backpropagation and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        total_loss /= len(train_dataloader.dataset)\n",
    "        # Print train loss\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Train Loss: {total_loss:.4f}')\n",
    "        train_losses.append(total_loss)\n",
    "\n",
    "        # Check validation loss every val_every_epoch epochs\n",
    "        if (epoch) % val_every_epoch == 0:\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for val_batch in val_dataloader:\n",
    "                    x_val = val_batch['image'][:, :input_channel, :, :].to(device)\n",
    "                    p_val = [tensor.to(device) for tensor in val_batch['pins']]\n",
    "                    y_val = [tensor.to(device) for tensor in val_batch['outputs']]\n",
    "\n",
    "                    val_outputs = model(x_val.float())\n",
    "                    val_loss += criterion(y_val, val_outputs, p_val).item()\n",
    "\n",
    "            val_loss /= len(val_dataloader.dataset)  # Average validation loss\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                # Save the model\n",
    "                torch.save(model.state_dict(), 'best_model.pth')\n",
    "\n",
    "            if early_stopping(epoch, val_loss):\n",
    "                break  # Stop training early\n",
    "            print(f'Validation Loss: {val_loss:.4f}')\n",
    "            val_losses.append(val_loss)\n",
    "\n",
    "    # Reload the best model after training\n",
    "    model.load_state_dict(torch.load('best_model.pth'))\n",
    "\n",
    "    return model, train_losses, val_losses\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ed8108-2439-4133-aade-e939f78df961",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8552a478-5727-4a45-9b9b-f925506011f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    metric = NPPLoss(identity=True).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            x_test = batch['image'][:, :input_channel, :, :].to(device)\n",
    "            p_test = [tensor.to(device) for tensor in batch['pins']]\n",
    "            y_test = [tensor.to(device) for tensor in batch['outputs']]\n",
    "\n",
    "            test_outputs = model(x_test.float())\n",
    "            loss = metric(y_test, test_outputs, p_test)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    total_loss /= len(dataloader.dataset)\n",
    "    return total_loss\n",
    "\n",
    "def plot_loss(train_losses, val_losses, val_every_epoch, NPP, sigma, dataset, learning_rate, num_kernels_encoder, num_kernels_decoder, save_dir=\"./results/plots\"):\n",
    "    # Create a directory for saving plots if it doesn't exist\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    # Construct the filename based on parameters\n",
    "    filename = f\"loss_plot_NPP_{NPP}_sigma_{sigma}_dataset_{dataset}_lr_{learning_rate}_encoder_{num_kernels_encoder}_decoder_{num_kernels_decoder}.png\"\n",
    "    save_path = os.path.join(save_dir, filename)\n",
    "\n",
    "    # Plot the train and validation loss\n",
    "    plt.plot(range(len(train_losses)), train_losses, label='Train Loss')\n",
    "    plt.plot(range(val_every_epoch, len(train_losses), val_every_epoch), val_losses, '--', label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('MSE Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Save the plot\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11e9111c-3a3f-498b-9574-56626971725a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_and_save(loss_vs_sigma_data, sigmas, dataset, learning_rate, model_name=\"Auto encoder\"):\n",
    "    # Unpack the data\n",
    "    test_loss_npp_true, test_loss_npp_false= loss_vs_sigma_data\n",
    "\n",
    "    # Calculate mean and confidence intervals for NPP=True runs\n",
    "    mean_test_loss_npp_true = np.mean(test_loss_npp_true, axis=0)\n",
    "    ci_test_loss_npp_true = 1.96 * np.std(test_loss_npp_true, axis=0) / np.sqrt(len(test_loss_npp_true))\n",
    "\n",
    "    # Plot mean and confidence intervals for NPP=True\n",
    "    plt.errorbar(sigmas, mean_test_loss_npp_true, yerr=ci_test_loss_npp_true, marker='o', label='NPP=True')\n",
    "\n",
    "    # Plot NPP=False\n",
    "    plt.axhline(y=np.mean(test_loss_npp_false_runs), color='r', linestyle='--', label='NPP=False')\n",
    "\n",
    "    plt.xscale('log')\n",
    "    plt.xlabel('Sigma (log scale)')\n",
    "    plt.yscale('log') \n",
    "    plt.ylabel('Test Loss (log scale)')\n",
    "    plt.title(f'Test Loss vs. Sigma: {dataset} dataset with {model_name}')\n",
    "    plt.legend()\n",
    "\n",
    "    # Create a directory to save the results if it doesn't exist\n",
    "    results_dir = './results'\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "    # Generate a filename based on parameters in the title\n",
    "    filename = f\"test_loss_vs_sigma_{dataset}_{model_name}_lr_{learning_rate}.png\"\n",
    "    filepath = os.path.join(results_dir, filename)\n",
    "\n",
    "    # Save the plot\n",
    "    plt.savefig(filepath)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def run_pipeline_ci(input_channel, train_dataloader, val_dataloader, num_epochs, val_every_epoch, learning_rate, device, num_runs=5):\n",
    "    test_losses_npp_true = []\n",
    "    test_losses_npp_false= []\n",
    "\n",
    "    for run in range(num_runs):\n",
    "        test_losses_vs_sigma_npp_true = []\n",
    "        test_loss_npp_false = None\n",
    "\n",
    "        # Run NPP=False once and collect the test loss\n",
    "        early_stopping = EarlyStoppingCallback(patience=5, min_delta=0.001)\n",
    "        criterion = NPPLoss(identity=True).to(device)\n",
    "\n",
    "        autoencoder = Autoencoder(num_kernels_encoder, num_kernels_decoder, input_channel=input_channel).to(device)\n",
    "        \n",
    "        optimizer = optim.Adam(autoencoder.parameters(), lr=learning_rate)\n",
    "        model, train_losses, val_losses = train_model(autoencoder, train_dataloader, val_dataloader, num_epochs, val_every_epoch, learning_rate, criterion, optimizer, device, early_stopping)\n",
    "\n",
    "        test_loss_npp_false = evaluate_model(autoencoder, test_dataloader, device)\n",
    "        test_losses_npp_false.append(test_loss_npp_false)\n",
    "\n",
    "        # Run LR Finder for different sigma values\n",
    "        for sigma in sigmas:\n",
    "            early_stopping = EarlyStoppingCallback(patience=5, min_delta=0.001)\n",
    "            criterion = NPPLoss(identity=False, sigma=sigma).to(device)\n",
    "            autoencoder = Autoencoder(num_kernels_encoder, num_kernels_decoder, input_channel=input_channel).to(device)\n",
    "            optimizer = optim.Adam(autoencoder.parameters(), lr=learning_rate)\n",
    "            model, train_losses, val_losses = train_model(autoencoder, train_dataloader, val_dataloader, num_epochs, val_every_epoch, learning_rate, criterion, optimizer, device, early_stopping)\n",
    "            test_loss = evaluate_model(autoencoder, test_dataloader, device)\n",
    "            test_losses_vs_sigma_npp_true.append(test_loss)\n",
    "\n",
    "        test_losses_npp_true.append(test_losses_npp_true)\n",
    "    return test_loss_npp_true, test_loss_npp_false_runs\n",
    "\n",
    "def save_data(data, filename):\n",
    "    np.save(filename, data)\n",
    "\n",
    "# Function to load data\n",
    "def load_data(filename):\n",
    "    return np.load(filename)\n",
    "\n",
    "# Function to run the pipeline and save data\n",
    "def run_and_save_pipeline(input_channel, train_dataloader, val_dataloader, num_epochs, val_every_epoch, learning_rate, device):\n",
    "    # Run the pipeline\n",
    "    test_loss_npp_true, test_loss_npp_false= run_pipeline_ci(input_channel, train_dataloader, val_dataloader, num_epochs, val_every_epoch, learning_rate, device)\n",
    "\n",
    "    # Save the data\n",
    "    save_data(test_loss_npp_true, 'test_loss_npp_true.npy')\n",
    "    save_data(test_loss_npp_false_runs, 'test_loss_npp_false_runs.npy')\n",
    "\n",
    "    return test_loss_npp_true, test_loss_npp_false_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c681ad7-b14c-4d02-ad7f-da35693469d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Train Loss: 1.2927\n",
      "Validation Loss: 0.7211\n",
      "Epoch [2/200], Train Loss: 0.5416\n",
      "Epoch [3/200], Train Loss: 0.3619\n",
      "Epoch [4/200], Train Loss: 0.2739\n",
      "Epoch [5/200], Train Loss: 0.2346\n",
      "Epoch [6/200], Train Loss: 0.2149\n",
      "Validation Loss: 0.2573\n",
      "Epoch [7/200], Train Loss: 0.1938\n",
      "Epoch [8/200], Train Loss: 0.1712\n",
      "Epoch [9/200], Train Loss: 0.1512\n",
      "Epoch [10/200], Train Loss: 0.1300\n",
      "Epoch [11/200], Train Loss: 0.1139\n",
      "Validation Loss: 0.1415\n",
      "Epoch [12/200], Train Loss: 0.1070\n",
      "Epoch [13/200], Train Loss: 0.0958\n",
      "Epoch [14/200], Train Loss: 0.0839\n",
      "Epoch [15/200], Train Loss: 0.0760\n",
      "Epoch [16/200], Train Loss: 0.0706\n",
      "Validation Loss: 0.0893\n",
      "Epoch [17/200], Train Loss: 0.0687\n",
      "Epoch [18/200], Train Loss: 0.0652\n",
      "Epoch [19/200], Train Loss: 0.0625\n",
      "Epoch [20/200], Train Loss: 0.0621\n",
      "Epoch [21/200], Train Loss: 0.0618\n",
      "Validation Loss: 0.0831\n",
      "Epoch [22/200], Train Loss: 0.0605\n",
      "Epoch [23/200], Train Loss: 0.0578\n",
      "Epoch [24/200], Train Loss: 0.0554\n",
      "Epoch [25/200], Train Loss: 0.0528\n",
      "Epoch [26/200], Train Loss: 0.0514\n",
      "Validation Loss: 0.0738\n",
      "Epoch [27/200], Train Loss: 0.0508\n",
      "Epoch [28/200], Train Loss: 0.0477\n",
      "Epoch [29/200], Train Loss: 0.0463\n",
      "Epoch [30/200], Train Loss: 0.0445\n",
      "Epoch [31/200], Train Loss: 0.0432\n",
      "Validation Loss: 0.0606\n",
      "Epoch [32/200], Train Loss: 0.0422\n",
      "Epoch [33/200], Train Loss: 0.0414\n",
      "Epoch [34/200], Train Loss: 0.0398\n",
      "Epoch [35/200], Train Loss: 0.0395\n",
      "Epoch [36/200], Train Loss: 0.0382\n",
      "Validation Loss: 0.0532\n",
      "Epoch [37/200], Train Loss: 0.0374\n",
      "Epoch [38/200], Train Loss: 0.0374\n",
      "Epoch [39/200], Train Loss: 0.0377\n",
      "Epoch [40/200], Train Loss: 0.0387\n",
      "Epoch [41/200], Train Loss: 0.0387\n",
      "Validation Loss: 0.0508\n",
      "Epoch [42/200], Train Loss: 0.0374\n",
      "Epoch [43/200], Train Loss: 0.0365\n",
      "Epoch [44/200], Train Loss: 0.0348\n",
      "Epoch [45/200], Train Loss: 0.0357\n",
      "Epoch [46/200], Train Loss: 0.0352\n",
      "Validation Loss: 0.0437\n",
      "Epoch [47/200], Train Loss: 0.0340\n",
      "Epoch [48/200], Train Loss: 0.0346\n",
      "Epoch [49/200], Train Loss: 0.0349\n",
      "Epoch [50/200], Train Loss: 0.0311\n",
      "Epoch [51/200], Train Loss: 0.0307\n",
      "Validation Loss: 0.0441\n",
      "Epoch [52/200], Train Loss: 0.0307\n",
      "Epoch [53/200], Train Loss: 0.0299\n",
      "Epoch [54/200], Train Loss: 0.0298\n",
      "Epoch [55/200], Train Loss: 0.0296\n",
      "Epoch [56/200], Train Loss: 0.0297\n",
      "Validation Loss: 0.0412\n",
      "Epoch [57/200], Train Loss: 0.0293\n",
      "Epoch [58/200], Train Loss: 0.0292\n",
      "Epoch [59/200], Train Loss: 0.0285\n",
      "Epoch [60/200], Train Loss: 0.0277\n",
      "Epoch [61/200], Train Loss: 0.0271\n",
      "Validation Loss: 0.0379\n",
      "Epoch [62/200], Train Loss: 0.0275\n",
      "Epoch [63/200], Train Loss: 0.0265\n",
      "Epoch [64/200], Train Loss: 0.0271\n",
      "Epoch [65/200], Train Loss: 0.0278\n",
      "Epoch [66/200], Train Loss: 0.0270\n",
      "Validation Loss: 0.0376\n",
      "Epoch [67/200], Train Loss: 0.0259\n",
      "Epoch [68/200], Train Loss: 0.0278\n",
      "Epoch [69/200], Train Loss: 0.0277\n",
      "Epoch [70/200], Train Loss: 0.0264\n",
      "Epoch [71/200], Train Loss: 0.0260\n",
      "Validation Loss: 0.0401\n",
      "Epoch [72/200], Train Loss: 0.0259\n",
      "Epoch [73/200], Train Loss: 0.0248\n",
      "Epoch [74/200], Train Loss: 0.0254\n",
      "Epoch [75/200], Train Loss: 0.0278\n",
      "Epoch [76/200], Train Loss: 0.0270\n",
      "Validation Loss: 0.0356\n",
      "Epoch [77/200], Train Loss: 0.0242\n",
      "Epoch [78/200], Train Loss: 0.0252\n",
      "Epoch [79/200], Train Loss: 0.0263\n",
      "Epoch [80/200], Train Loss: 0.0295\n",
      "Epoch [81/200], Train Loss: 0.0249\n",
      "Validation Loss: 0.0374\n",
      "Epoch [82/200], Train Loss: 0.0240\n",
      "Epoch [83/200], Train Loss: 0.0238\n",
      "Epoch [84/200], Train Loss: 0.0257\n",
      "Epoch [85/200], Train Loss: 0.0246\n",
      "Epoch [86/200], Train Loss: 0.0242\n",
      "Validation Loss: 0.0350\n",
      "Epoch [87/200], Train Loss: 0.0226\n",
      "Epoch [88/200], Train Loss: 0.0231\n",
      "Epoch [89/200], Train Loss: 0.0238\n",
      "Epoch [90/200], Train Loss: 0.0257\n",
      "Epoch [91/200], Train Loss: 0.0247\n",
      "Validation Loss: 0.0364\n",
      "Epoch [92/200], Train Loss: 0.0229\n",
      "Epoch [93/200], Train Loss: 0.0226\n",
      "Epoch [94/200], Train Loss: 0.0235\n",
      "Epoch [95/200], Train Loss: 0.0263\n",
      "Epoch [96/200], Train Loss: 0.0262\n",
      "Validation Loss: 0.0369\n",
      "Epoch [97/200], Train Loss: 0.0265\n",
      "Epoch [98/200], Train Loss: 0.0243\n",
      "Epoch [99/200], Train Loss: 0.0219\n",
      "Epoch [100/200], Train Loss: 0.0237\n",
      "Epoch [101/200], Train Loss: 0.0241\n",
      "Early stopping after 100 epochs.\n",
      "Epoch [1/200], Train Loss: 1.5310\n",
      "Validation Loss: 0.8930\n",
      "Epoch [2/200], Train Loss: 0.5952\n",
      "Epoch [3/200], Train Loss: 0.4021\n",
      "Epoch [4/200], Train Loss: 0.2934\n",
      "Epoch [5/200], Train Loss: 0.2489\n",
      "Epoch [6/200], Train Loss: 0.2230\n",
      "Validation Loss: 0.2917\n",
      "Epoch [7/200], Train Loss: 0.2072\n",
      "Epoch [8/200], Train Loss: 0.1984\n",
      "Epoch [9/200], Train Loss: 0.1793\n",
      "Epoch [10/200], Train Loss: 0.1553\n",
      "Epoch [11/200], Train Loss: 0.1415\n",
      "Validation Loss: 0.2055\n",
      "Epoch [12/200], Train Loss: 0.1393\n",
      "Epoch [13/200], Train Loss: 0.1317\n",
      "Epoch [14/200], Train Loss: 0.1265\n",
      "Epoch [15/200], Train Loss: 0.1192\n",
      "Epoch [16/200], Train Loss: 0.1166\n",
      "Validation Loss: 0.1633\n",
      "Epoch [17/200], Train Loss: 0.1124\n",
      "Epoch [18/200], Train Loss: 0.1152\n",
      "Epoch [19/200], Train Loss: 0.1135\n",
      "Epoch [20/200], Train Loss: 0.1088\n",
      "Epoch [21/200], Train Loss: 0.1094\n",
      "Validation Loss: 0.1623\n",
      "Epoch [22/200], Train Loss: 0.1075\n",
      "Epoch [23/200], Train Loss: 0.1070\n",
      "Epoch [24/200], Train Loss: 0.1033\n",
      "Epoch [25/200], Train Loss: 0.1005\n",
      "Epoch [26/200], Train Loss: 0.1019\n",
      "Validation Loss: 0.1458\n",
      "Epoch [27/200], Train Loss: 0.1021\n",
      "Epoch [28/200], Train Loss: 0.1005\n",
      "Epoch [29/200], Train Loss: 0.1108\n",
      "Epoch [30/200], Train Loss: 0.1078\n",
      "Epoch [31/200], Train Loss: 0.1058\n",
      "Validation Loss: 0.1375\n",
      "Epoch [32/200], Train Loss: 0.1014\n",
      "Epoch [33/200], Train Loss: 0.1010\n",
      "Epoch [34/200], Train Loss: 0.1016\n",
      "Epoch [35/200], Train Loss: 0.0986\n",
      "Epoch [36/200], Train Loss: 0.1062\n",
      "Validation Loss: 0.1602\n",
      "Epoch [37/200], Train Loss: 0.1006\n",
      "Epoch [38/200], Train Loss: 0.0965\n",
      "Epoch [39/200], Train Loss: 0.0976\n",
      "Epoch [40/200], Train Loss: 0.0970\n",
      "Epoch [41/200], Train Loss: 0.0963\n",
      "Validation Loss: 0.1329\n",
      "Epoch [42/200], Train Loss: 0.0954\n",
      "Epoch [43/200], Train Loss: 0.0961\n",
      "Epoch [44/200], Train Loss: 0.0974\n",
      "Epoch [45/200], Train Loss: 0.0962\n",
      "Epoch [46/200], Train Loss: 0.0980\n",
      "Validation Loss: 0.1352\n",
      "Epoch [47/200], Train Loss: 0.0983\n",
      "Epoch [48/200], Train Loss: 0.0975\n",
      "Epoch [49/200], Train Loss: 0.0999\n",
      "Epoch [50/200], Train Loss: 0.0990\n",
      "Epoch [51/200], Train Loss: 0.0988\n",
      "Validation Loss: 0.1375\n",
      "Epoch [52/200], Train Loss: 0.0923\n",
      "Epoch [53/200], Train Loss: 0.0922\n",
      "Epoch [54/200], Train Loss: 0.0936\n",
      "Epoch [55/200], Train Loss: 0.0956\n",
      "Epoch [56/200], Train Loss: 0.0936\n",
      "Validation Loss: 0.1284\n",
      "Epoch [57/200], Train Loss: 0.0937\n",
      "Epoch [58/200], Train Loss: 0.0911\n",
      "Epoch [59/200], Train Loss: 0.1024\n",
      "Epoch [60/200], Train Loss: 0.0928\n",
      "Epoch [61/200], Train Loss: 0.0913\n",
      "Validation Loss: 0.1300\n",
      "Epoch [62/200], Train Loss: 0.0924\n",
      "Epoch [63/200], Train Loss: 0.0904\n",
      "Epoch [64/200], Train Loss: 0.0912\n",
      "Epoch [65/200], Train Loss: 0.0915\n",
      "Epoch [66/200], Train Loss: 0.0910\n",
      "Validation Loss: 0.1246\n",
      "Epoch [67/200], Train Loss: 0.0906\n",
      "Epoch [68/200], Train Loss: 0.0903\n",
      "Epoch [69/200], Train Loss: 0.0931\n",
      "Epoch [70/200], Train Loss: 0.0920\n",
      "Epoch [71/200], Train Loss: 0.0915\n",
      "Validation Loss: 0.1234\n",
      "Epoch [72/200], Train Loss: 0.0868\n",
      "Epoch [73/200], Train Loss: 0.0869\n",
      "Epoch [74/200], Train Loss: 0.0860\n",
      "Epoch [75/200], Train Loss: 0.0882\n",
      "Epoch [76/200], Train Loss: 0.0865\n",
      "Validation Loss: 0.1219\n",
      "Epoch [77/200], Train Loss: 0.0865\n",
      "Epoch [78/200], Train Loss: 0.0861\n",
      "Epoch [79/200], Train Loss: 0.0866\n",
      "Epoch [80/200], Train Loss: 0.0861\n",
      "Epoch [81/200], Train Loss: 0.0859\n",
      "Validation Loss: 0.1220\n",
      "Epoch [82/200], Train Loss: 0.0884\n",
      "Epoch [83/200], Train Loss: 0.0892\n",
      "Epoch [84/200], Train Loss: 0.0884\n",
      "Epoch [85/200], Train Loss: 0.0851\n",
      "Epoch [86/200], Train Loss: 0.0887\n",
      "Validation Loss: 0.1238\n",
      "Epoch [87/200], Train Loss: 0.0848\n",
      "Epoch [88/200], Train Loss: 0.0836\n",
      "Epoch [89/200], Train Loss: 0.0836\n",
      "Epoch [90/200], Train Loss: 0.0875\n",
      "Epoch [91/200], Train Loss: 0.0841\n",
      "Validation Loss: 0.1233\n",
      "Epoch [92/200], Train Loss: 0.0862\n",
      "Epoch [93/200], Train Loss: 0.0847\n",
      "Epoch [94/200], Train Loss: 0.0834\n",
      "Epoch [95/200], Train Loss: 0.0830\n",
      "Epoch [96/200], Train Loss: 0.0838\n",
      "Validation Loss: 0.1183\n",
      "Epoch [97/200], Train Loss: 0.0813\n",
      "Epoch [98/200], Train Loss: 0.0832\n",
      "Epoch [99/200], Train Loss: 0.0807\n",
      "Epoch [100/200], Train Loss: 0.0817\n",
      "Epoch [101/200], Train Loss: 0.0821\n",
      "Validation Loss: 0.1193\n",
      "Epoch [102/200], Train Loss: 0.0847\n",
      "Epoch [103/200], Train Loss: 0.0836\n",
      "Epoch [104/200], Train Loss: 0.0864\n",
      "Epoch [105/200], Train Loss: 0.0821\n",
      "Epoch [106/200], Train Loss: 0.0818\n",
      "Validation Loss: 0.1239\n",
      "Epoch [107/200], Train Loss: 0.0823\n",
      "Epoch [108/200], Train Loss: 0.0865\n",
      "Epoch [109/200], Train Loss: 0.0836\n",
      "Epoch [110/200], Train Loss: 0.0802\n",
      "Epoch [111/200], Train Loss: 0.0814\n",
      "Validation Loss: 0.1168\n",
      "Epoch [112/200], Train Loss: 0.0807\n",
      "Epoch [113/200], Train Loss: 0.0810\n",
      "Epoch [114/200], Train Loss: 0.0837\n",
      "Epoch [115/200], Train Loss: 0.0814\n",
      "Epoch [116/200], Train Loss: 0.0806\n",
      "Validation Loss: 0.1165\n",
      "Epoch [117/200], Train Loss: 0.0813\n",
      "Epoch [118/200], Train Loss: 0.0803\n",
      "Epoch [119/200], Train Loss: 0.0789\n",
      "Epoch [120/200], Train Loss: 0.0826\n",
      "Epoch [121/200], Train Loss: 0.0795\n",
      "Validation Loss: 0.1171\n",
      "Epoch [122/200], Train Loss: 0.0812\n",
      "Epoch [123/200], Train Loss: 0.0796\n",
      "Epoch [124/200], Train Loss: 0.0825\n",
      "Epoch [125/200], Train Loss: 0.0791\n",
      "Epoch [126/200], Train Loss: 0.0772\n",
      "Validation Loss: 0.1187\n",
      "Epoch [127/200], Train Loss: 0.0792\n",
      "Epoch [128/200], Train Loss: 0.0833\n",
      "Epoch [129/200], Train Loss: 0.0838\n",
      "Epoch [130/200], Train Loss: 0.0800\n",
      "Epoch [131/200], Train Loss: 0.0829\n",
      "Validation Loss: 0.1173\n",
      "Epoch [132/200], Train Loss: 0.0820\n",
      "Epoch [133/200], Train Loss: 0.0799\n",
      "Epoch [134/200], Train Loss: 0.0776\n",
      "Epoch [135/200], Train Loss: 0.0771\n",
      "Epoch [136/200], Train Loss: 0.0786\n",
      "Validation Loss: 0.1134\n",
      "Epoch [137/200], Train Loss: 0.0778\n",
      "Epoch [138/200], Train Loss: 0.0785\n",
      "Epoch [139/200], Train Loss: 0.0844\n",
      "Epoch [140/200], Train Loss: 0.0813\n",
      "Epoch [141/200], Train Loss: 0.0779\n",
      "Validation Loss: 0.1132\n",
      "Epoch [142/200], Train Loss: 0.0772\n",
      "Epoch [143/200], Train Loss: 0.0775\n",
      "Epoch [144/200], Train Loss: 0.0790\n",
      "Epoch [145/200], Train Loss: 0.0758\n",
      "Epoch [146/200], Train Loss: 0.0787\n",
      "Validation Loss: 0.1127\n",
      "Epoch [147/200], Train Loss: 0.0788\n",
      "Epoch [148/200], Train Loss: 0.0783\n",
      "Epoch [149/200], Train Loss: 0.0787\n",
      "Epoch [150/200], Train Loss: 0.0769\n",
      "Epoch [151/200], Train Loss: 0.0760\n",
      "Validation Loss: 0.1133\n",
      "Epoch [152/200], Train Loss: 0.0759\n",
      "Epoch [153/200], Train Loss: 0.0822\n",
      "Epoch [154/200], Train Loss: 0.0796\n",
      "Epoch [155/200], Train Loss: 0.0775\n",
      "Epoch [156/200], Train Loss: 0.0777\n",
      "Validation Loss: 0.1121\n",
      "Epoch [157/200], Train Loss: 0.0789\n",
      "Epoch [158/200], Train Loss: 0.0786\n",
      "Epoch [159/200], Train Loss: 0.0809\n",
      "Epoch [160/200], Train Loss: 0.0800\n",
      "Epoch [161/200], Train Loss: 0.0785\n",
      "Validation Loss: 0.1372\n",
      "Epoch [162/200], Train Loss: 0.0873\n",
      "Epoch [163/200], Train Loss: 0.0908\n",
      "Epoch [164/200], Train Loss: 0.0809\n",
      "Epoch [165/200], Train Loss: 0.0798\n",
      "Epoch [166/200], Train Loss: 0.0835\n",
      "Validation Loss: 0.1133\n",
      "Epoch [167/200], Train Loss: 0.0806\n",
      "Epoch [168/200], Train Loss: 0.0817\n",
      "Epoch [169/200], Train Loss: 0.0770\n",
      "Epoch [170/200], Train Loss: 0.0768\n",
      "Epoch [171/200], Train Loss: 0.0779\n",
      "Validation Loss: 0.1142\n",
      "Epoch [172/200], Train Loss: 0.0745\n",
      "Epoch [173/200], Train Loss: 0.0766\n",
      "Epoch [174/200], Train Loss: 0.0754\n",
      "Epoch [175/200], Train Loss: 0.0738\n",
      "Epoch [176/200], Train Loss: 0.0760\n",
      "Validation Loss: 0.1130\n",
      "Epoch [177/200], Train Loss: 0.0752\n",
      "Epoch [178/200], Train Loss: 0.0750\n",
      "Epoch [179/200], Train Loss: 0.0774\n",
      "Epoch [180/200], Train Loss: 0.0776\n",
      "Epoch [181/200], Train Loss: 0.0801\n",
      "Early stopping after 180 epochs.\n",
      "Epoch [1/200], Train Loss: 1.5114\n",
      "Validation Loss: 1.1430\n",
      "Epoch [2/200], Train Loss: 0.6146\n",
      "Epoch [3/200], Train Loss: 0.3837\n",
      "Epoch [4/200], Train Loss: 0.2616\n",
      "Epoch [5/200], Train Loss: 0.2203\n",
      "Epoch [6/200], Train Loss: 0.1830\n",
      "Validation Loss: 0.2346\n",
      "Epoch [7/200], Train Loss: 0.1527\n",
      "Epoch [8/200], Train Loss: 0.1249\n",
      "Epoch [9/200], Train Loss: 0.1108\n",
      "Epoch [10/200], Train Loss: 0.0899\n",
      "Epoch [11/200], Train Loss: 0.0781\n",
      "Validation Loss: 0.1047\n",
      "Epoch [12/200], Train Loss: 0.0738\n",
      "Epoch [13/200], Train Loss: 0.0701\n",
      "Epoch [14/200], Train Loss: 0.0703\n",
      "Epoch [15/200], Train Loss: 0.0656\n",
      "Epoch [16/200], Train Loss: 0.0624\n",
      "Validation Loss: 0.0853\n",
      "Epoch [17/200], Train Loss: 0.0596\n",
      "Epoch [18/200], Train Loss: 0.0616\n",
      "Epoch [19/200], Train Loss: 0.0575\n",
      "Epoch [20/200], Train Loss: 0.0570\n",
      "Epoch [21/200], Train Loss: 0.0545\n",
      "Validation Loss: 0.0707\n",
      "Epoch [22/200], Train Loss: 0.0513\n",
      "Epoch [23/200], Train Loss: 0.0494\n",
      "Epoch [24/200], Train Loss: 0.0505\n",
      "Epoch [25/200], Train Loss: 0.0509\n",
      "Epoch [26/200], Train Loss: 0.0451\n",
      "Validation Loss: 0.0661\n",
      "Epoch [27/200], Train Loss: 0.0444\n",
      "Epoch [28/200], Train Loss: 0.0448\n",
      "Epoch [29/200], Train Loss: 0.0402\n",
      "Epoch [30/200], Train Loss: 0.0401\n",
      "Epoch [31/200], Train Loss: 0.0381\n",
      "Validation Loss: 0.0564\n",
      "Epoch [32/200], Train Loss: 0.0382\n",
      "Epoch [33/200], Train Loss: 0.0387\n",
      "Epoch [34/200], Train Loss: 0.0381\n",
      "Epoch [35/200], Train Loss: 0.0372\n",
      "Epoch [36/200], Train Loss: 0.0392\n",
      "Validation Loss: 0.0495\n",
      "Epoch [37/200], Train Loss: 0.0358\n",
      "Epoch [38/200], Train Loss: 0.0346\n",
      "Epoch [39/200], Train Loss: 0.0342\n",
      "Epoch [40/200], Train Loss: 0.0319\n",
      "Epoch [41/200], Train Loss: 0.0322\n",
      "Validation Loss: 0.0521\n",
      "Epoch [42/200], Train Loss: 0.0320\n",
      "Epoch [43/200], Train Loss: 0.0313\n",
      "Epoch [44/200], Train Loss: 0.0321\n",
      "Epoch [45/200], Train Loss: 0.0340\n",
      "Epoch [46/200], Train Loss: 0.0310\n",
      "Validation Loss: 0.0463\n",
      "Epoch [47/200], Train Loss: 0.0319\n",
      "Epoch [48/200], Train Loss: 0.0301\n",
      "Epoch [49/200], Train Loss: 0.0306\n",
      "Epoch [50/200], Train Loss: 0.0305\n",
      "Epoch [51/200], Train Loss: 0.0313\n",
      "Validation Loss: 0.0447\n",
      "Epoch [52/200], Train Loss: 0.0318\n",
      "Epoch [53/200], Train Loss: 0.0300\n",
      "Epoch [54/200], Train Loss: 0.0280\n",
      "Epoch [55/200], Train Loss: 0.0289\n",
      "Epoch [56/200], Train Loss: 0.0305\n",
      "Validation Loss: 0.0432\n",
      "Epoch [57/200], Train Loss: 0.0300\n",
      "Epoch [58/200], Train Loss: 0.0293\n",
      "Epoch [59/200], Train Loss: 0.0309\n",
      "Epoch [60/200], Train Loss: 0.0297\n",
      "Epoch [61/200], Train Loss: 0.0319\n",
      "Validation Loss: 0.0553\n",
      "Epoch [62/200], Train Loss: 0.0296\n",
      "Epoch [63/200], Train Loss: 0.0292\n",
      "Epoch [64/200], Train Loss: 0.0275\n",
      "Epoch [65/200], Train Loss: 0.0291\n",
      "Epoch [66/200], Train Loss: 0.0286\n",
      "Validation Loss: 0.0476\n",
      "Epoch [67/200], Train Loss: 0.0281\n",
      "Epoch [68/200], Train Loss: 0.0260\n",
      "Epoch [69/200], Train Loss: 0.0259\n",
      "Epoch [70/200], Train Loss: 0.0261\n",
      "Epoch [71/200], Train Loss: 0.0270\n",
      "Validation Loss: 0.0500\n",
      "Epoch [72/200], Train Loss: 0.0288\n",
      "Epoch [73/200], Train Loss: 0.0269\n",
      "Epoch [74/200], Train Loss: 0.0259\n",
      "Epoch [75/200], Train Loss: 0.0250\n",
      "Epoch [76/200], Train Loss: 0.0247\n",
      "Validation Loss: 0.0431\n",
      "Epoch [77/200], Train Loss: 0.0269\n",
      "Epoch [78/200], Train Loss: 0.0296\n",
      "Epoch [79/200], Train Loss: 0.0346\n",
      "Epoch [80/200], Train Loss: 0.0302\n",
      "Epoch [81/200], Train Loss: 0.0262\n",
      "Early stopping after 80 epochs.\n",
      "Epoch [1/200], Train Loss: 1.8421\n",
      "Validation Loss: 2.5494\n",
      "Epoch [2/200], Train Loss: 1.8987\n",
      "Epoch [3/200], Train Loss: 1.8873\n",
      "Epoch [4/200], Train Loss: 1.8417\n",
      "Epoch [5/200], Train Loss: 1.8596\n",
      "Epoch [6/200], Train Loss: 1.9136\n",
      "Validation Loss: 2.5494\n",
      "Epoch [7/200], Train Loss: 1.8553\n",
      "Epoch [8/200], Train Loss: 1.8798\n",
      "Epoch [9/200], Train Loss: 1.8897\n",
      "Epoch [10/200], Train Loss: 1.8764\n",
      "Epoch [11/200], Train Loss: 1.8826\n",
      "Validation Loss: 2.5494\n",
      "Epoch [12/200], Train Loss: 1.9070\n",
      "Epoch [13/200], Train Loss: 1.8915\n",
      "Epoch [14/200], Train Loss: 1.8652\n",
      "Epoch [15/200], Train Loss: 1.8692\n",
      "Epoch [16/200], Train Loss: 1.9109\n",
      "Validation Loss: 2.5494\n",
      "Epoch [17/200], Train Loss: 1.8584\n",
      "Epoch [18/200], Train Loss: 1.8411\n",
      "Epoch [19/200], Train Loss: 1.9022\n",
      "Epoch [20/200], Train Loss: 1.8612\n",
      "Epoch [21/200], Train Loss: 1.8561\n",
      "Validation Loss: 2.5494\n",
      "Epoch [22/200], Train Loss: 1.9087\n",
      "Epoch [23/200], Train Loss: 1.8857\n",
      "Epoch [24/200], Train Loss: 1.8874\n",
      "Epoch [25/200], Train Loss: 1.8772\n",
      "Epoch [26/200], Train Loss: 1.8637\n",
      "Early stopping after 25 epochs.\n",
      "Epoch [1/200], Train Loss: 1.7683\n",
      "Validation Loss: 2.3603\n",
      "Epoch [2/200], Train Loss: 1.7506\n",
      "Epoch [3/200], Train Loss: 1.7720\n",
      "Epoch [4/200], Train Loss: 1.7627\n",
      "Epoch [5/200], Train Loss: 1.7423\n",
      "Epoch [6/200], Train Loss: 1.7275\n",
      "Validation Loss: 2.3603\n",
      "Epoch [7/200], Train Loss: 1.7824\n",
      "Epoch [8/200], Train Loss: 1.7581\n",
      "Epoch [9/200], Train Loss: 1.7148\n",
      "Epoch [10/200], Train Loss: 1.7721\n",
      "Epoch [11/200], Train Loss: 1.7703\n",
      "Validation Loss: 2.3603\n",
      "Epoch [12/200], Train Loss: 1.7314\n",
      "Epoch [13/200], Train Loss: 1.7107\n",
      "Epoch [14/200], Train Loss: 1.7504\n",
      "Epoch [15/200], Train Loss: 1.7489\n",
      "Epoch [16/200], Train Loss: 1.7413\n",
      "Validation Loss: 2.3603\n",
      "Epoch [17/200], Train Loss: 1.7528\n",
      "Epoch [18/200], Train Loss: 1.7761\n",
      "Epoch [19/200], Train Loss: 1.7768\n",
      "Epoch [20/200], Train Loss: 1.7487\n",
      "Epoch [21/200], Train Loss: 1.7665\n",
      "Validation Loss: 2.3603\n",
      "Epoch [22/200], Train Loss: 1.7595\n",
      "Epoch [23/200], Train Loss: 1.7208\n",
      "Epoch [24/200], Train Loss: 1.7524\n",
      "Epoch [25/200], Train Loss: 1.7304\n",
      "Epoch [26/200], Train Loss: 1.7367\n",
      "Early stopping after 25 epochs.\n",
      "Epoch [1/200], Train Loss: 1.4385\n",
      "Validation Loss: 1.7370\n",
      "Epoch [2/200], Train Loss: 1.2734\n",
      "Epoch [3/200], Train Loss: 1.1540\n",
      "Epoch [4/200], Train Loss: 1.1096\n",
      "Epoch [5/200], Train Loss: 1.1051\n",
      "Epoch [6/200], Train Loss: 1.1098\n",
      "Validation Loss: 1.5522\n",
      "Epoch [7/200], Train Loss: 1.0779\n",
      "Epoch [8/200], Train Loss: 1.0640\n",
      "Epoch [9/200], Train Loss: 1.0852\n",
      "Epoch [10/200], Train Loss: 1.0396\n",
      "Epoch [11/200], Train Loss: 1.0515\n",
      "Validation Loss: 1.4992\n",
      "Epoch [12/200], Train Loss: 1.0511\n",
      "Epoch [13/200], Train Loss: 1.0477\n",
      "Epoch [14/200], Train Loss: 1.0175\n",
      "Epoch [15/200], Train Loss: 1.0290\n",
      "Epoch [16/200], Train Loss: 1.0266\n",
      "Validation Loss: 1.6666\n",
      "Epoch [17/200], Train Loss: 1.0361\n",
      "Epoch [18/200], Train Loss: 1.0254\n",
      "Epoch [19/200], Train Loss: 0.9889\n",
      "Epoch [20/200], Train Loss: 1.0263\n",
      "Epoch [21/200], Train Loss: 1.0377\n",
      "Validation Loss: 1.5512\n",
      "Epoch [22/200], Train Loss: 1.0136\n",
      "Epoch [23/200], Train Loss: 1.0010\n",
      "Epoch [24/200], Train Loss: 1.0151\n",
      "Epoch [25/200], Train Loss: 0.9748\n",
      "Epoch [26/200], Train Loss: 0.9813\n",
      "Validation Loss: 1.6148\n",
      "Epoch [27/200], Train Loss: 0.9819\n",
      "Epoch [28/200], Train Loss: 0.9878\n",
      "Epoch [29/200], Train Loss: 0.9683\n",
      "Epoch [30/200], Train Loss: 0.9701\n",
      "Epoch [31/200], Train Loss: 0.9804\n",
      "Validation Loss: 1.6901\n",
      "Epoch [32/200], Train Loss: 0.9827\n",
      "Epoch [33/200], Train Loss: 0.9773\n",
      "Epoch [34/200], Train Loss: 0.9894\n",
      "Epoch [35/200], Train Loss: 0.9797\n",
      "Epoch [36/200], Train Loss: 0.9525\n",
      "Early stopping after 35 epochs.\n",
      "Epoch [1/200], Train Loss: 1.0856\n",
      "Validation Loss: 1.0887\n",
      "Epoch [2/200], Train Loss: 0.6812\n",
      "Epoch [3/200], Train Loss: 0.5019\n",
      "Epoch [4/200], Train Loss: 0.3741\n",
      "Epoch [5/200], Train Loss: 0.3009\n",
      "Epoch [6/200], Train Loss: 0.2505\n",
      "Validation Loss: 0.3222\n",
      "Epoch [7/200], Train Loss: 0.2193\n",
      "Epoch [8/200], Train Loss: 0.2194\n",
      "Epoch [9/200], Train Loss: 0.2042\n",
      "Epoch [10/200], Train Loss: 0.1716\n",
      "Epoch [11/200], Train Loss: 0.1469\n",
      "Validation Loss: 0.2450\n",
      "Epoch [12/200], Train Loss: 0.1267\n",
      "Epoch [13/200], Train Loss: 0.1200\n",
      "Epoch [14/200], Train Loss: 0.1154\n",
      "Epoch [15/200], Train Loss: 0.1137\n",
      "Epoch [16/200], Train Loss: 0.1022\n",
      "Validation Loss: 0.2259\n",
      "Epoch [17/200], Train Loss: 0.0963\n",
      "Epoch [18/200], Train Loss: 0.0919\n",
      "Epoch [19/200], Train Loss: 0.0854\n",
      "Epoch [20/200], Train Loss: 0.0839\n",
      "Epoch [21/200], Train Loss: 0.0911\n",
      "Validation Loss: 0.2316\n",
      "Epoch [22/200], Train Loss: 0.0817\n",
      "Epoch [23/200], Train Loss: 0.0922\n",
      "Epoch [24/200], Train Loss: 0.0910\n",
      "Epoch [25/200], Train Loss: 0.0763\n",
      "Epoch [26/200], Train Loss: 0.0853\n",
      "Validation Loss: 0.1311\n",
      "Epoch [27/200], Train Loss: 0.0747\n",
      "Epoch [28/200], Train Loss: 0.0713\n",
      "Epoch [29/200], Train Loss: 0.0714\n",
      "Epoch [30/200], Train Loss: 0.0673\n",
      "Epoch [31/200], Train Loss: 0.0709\n",
      "Validation Loss: 0.1761\n",
      "Epoch [32/200], Train Loss: 0.0904\n",
      "Epoch [33/200], Train Loss: 0.0870\n",
      "Epoch [34/200], Train Loss: 0.0953\n",
      "Epoch [35/200], Train Loss: 0.0832\n",
      "Epoch [36/200], Train Loss: 0.0823\n",
      "Validation Loss: 0.1749\n",
      "Epoch [37/200], Train Loss: 0.0787\n",
      "Epoch [38/200], Train Loss: 0.0659\n",
      "Epoch [39/200], Train Loss: 0.0564\n",
      "Epoch [40/200], Train Loss: 0.0584\n",
      "Epoch [41/200], Train Loss: 0.0538\n",
      "Validation Loss: 0.1172\n",
      "Epoch [42/200], Train Loss: 0.0513\n",
      "Epoch [43/200], Train Loss: 0.0511\n",
      "Epoch [44/200], Train Loss: 0.0459\n",
      "Epoch [45/200], Train Loss: 0.0439\n",
      "Epoch [46/200], Train Loss: 0.0455\n",
      "Validation Loss: 0.0930\n",
      "Epoch [47/200], Train Loss: 0.0441\n",
      "Epoch [48/200], Train Loss: 0.0510\n",
      "Epoch [49/200], Train Loss: 0.0534\n",
      "Epoch [50/200], Train Loss: 0.0540\n",
      "Epoch [51/200], Train Loss: 0.0510\n",
      "Validation Loss: 0.1038\n",
      "Epoch [52/200], Train Loss: 0.0470\n",
      "Epoch [53/200], Train Loss: 0.0503\n",
      "Epoch [54/200], Train Loss: 0.0456\n",
      "Epoch [55/200], Train Loss: 0.0439\n",
      "Epoch [56/200], Train Loss: 0.0448\n",
      "Validation Loss: 0.1476\n",
      "Epoch [57/200], Train Loss: 0.0486\n",
      "Epoch [58/200], Train Loss: 0.0440\n",
      "Epoch [59/200], Train Loss: 0.0478\n",
      "Epoch [60/200], Train Loss: 0.0552\n",
      "Epoch [61/200], Train Loss: 0.0503\n",
      "Validation Loss: 0.1117\n",
      "Epoch [62/200], Train Loss: 0.0489\n",
      "Epoch [63/200], Train Loss: 0.0525\n",
      "Epoch [64/200], Train Loss: 0.0474\n",
      "Epoch [65/200], Train Loss: 0.0501\n",
      "Epoch [66/200], Train Loss: 0.0487\n",
      "Validation Loss: 0.1392\n",
      "Epoch [67/200], Train Loss: 0.0483\n",
      "Epoch [68/200], Train Loss: 0.0433\n",
      "Epoch [69/200], Train Loss: 0.0422\n",
      "Epoch [70/200], Train Loss: 0.0386\n",
      "Epoch [71/200], Train Loss: 0.0350\n",
      "Early stopping after 70 epochs.\n",
      "Epoch [1/200], Train Loss: 1.3383\n",
      "Validation Loss: 0.7591\n",
      "Epoch [2/200], Train Loss: 0.5663\n",
      "Epoch [3/200], Train Loss: 0.3767\n",
      "Epoch [4/200], Train Loss: 0.2984\n",
      "Epoch [5/200], Train Loss: 0.2444\n",
      "Epoch [6/200], Train Loss: 0.2229\n",
      "Validation Loss: 0.2308\n",
      "Epoch [7/200], Train Loss: 0.1719\n",
      "Epoch [8/200], Train Loss: 0.1388\n",
      "Epoch [9/200], Train Loss: 0.1133\n",
      "Epoch [10/200], Train Loss: 0.0981\n",
      "Epoch [11/200], Train Loss: 0.0850\n",
      "Validation Loss: 0.1109\n",
      "Epoch [12/200], Train Loss: 0.0794\n",
      "Epoch [13/200], Train Loss: 0.0729\n",
      "Epoch [14/200], Train Loss: 0.0684\n",
      "Epoch [15/200], Train Loss: 0.0625\n",
      "Epoch [16/200], Train Loss: 0.0608\n",
      "Validation Loss: 0.0813\n",
      "Epoch [17/200], Train Loss: 0.0591\n",
      "Epoch [18/200], Train Loss: 0.0555\n",
      "Epoch [19/200], Train Loss: 0.0525\n",
      "Epoch [20/200], Train Loss: 0.0508\n",
      "Epoch [21/200], Train Loss: 0.0491\n",
      "Validation Loss: 0.0658\n",
      "Epoch [22/200], Train Loss: 0.0469\n",
      "Epoch [23/200], Train Loss: 0.0472\n",
      "Epoch [24/200], Train Loss: 0.0467\n",
      "Epoch [25/200], Train Loss: 0.0451\n",
      "Epoch [26/200], Train Loss: 0.0434\n",
      "Validation Loss: 0.0565\n",
      "Epoch [27/200], Train Loss: 0.0404\n",
      "Epoch [28/200], Train Loss: 0.0396\n",
      "Epoch [29/200], Train Loss: 0.0381\n",
      "Epoch [30/200], Train Loss: 0.0381\n",
      "Epoch [31/200], Train Loss: 0.0356\n",
      "Validation Loss: 0.0509\n",
      "Epoch [32/200], Train Loss: 0.0338\n",
      "Epoch [33/200], Train Loss: 0.0346\n",
      "Epoch [34/200], Train Loss: 0.0346\n",
      "Epoch [35/200], Train Loss: 0.0356\n",
      "Epoch [36/200], Train Loss: 0.0305\n",
      "Validation Loss: 0.0452\n",
      "Epoch [37/200], Train Loss: 0.0305\n",
      "Epoch [38/200], Train Loss: 0.0298\n",
      "Epoch [39/200], Train Loss: 0.0303\n",
      "Epoch [40/200], Train Loss: 0.0296\n",
      "Epoch [41/200], Train Loss: 0.0321\n",
      "Validation Loss: 0.0453\n",
      "Epoch [42/200], Train Loss: 0.0317\n",
      "Epoch [43/200], Train Loss: 0.0305\n",
      "Epoch [44/200], Train Loss: 0.0295\n",
      "Epoch [45/200], Train Loss: 0.0318\n",
      "Epoch [46/200], Train Loss: 0.0292\n",
      "Validation Loss: 0.0486\n",
      "Epoch [47/200], Train Loss: 0.0275\n",
      "Epoch [48/200], Train Loss: 0.0250\n",
      "Epoch [49/200], Train Loss: 0.0241\n",
      "Epoch [50/200], Train Loss: 0.0248\n",
      "Epoch [51/200], Train Loss: 0.0242\n",
      "Validation Loss: 0.0406\n",
      "Epoch [52/200], Train Loss: 0.0246\n",
      "Epoch [53/200], Train Loss: 0.0228\n",
      "Epoch [54/200], Train Loss: 0.0240\n",
      "Epoch [55/200], Train Loss: 0.0237\n",
      "Epoch [56/200], Train Loss: 0.0248\n",
      "Validation Loss: 0.0338\n",
      "Epoch [57/200], Train Loss: 0.0239\n",
      "Epoch [58/200], Train Loss: 0.0222\n",
      "Epoch [59/200], Train Loss: 0.0228\n",
      "Epoch [60/200], Train Loss: 0.0216\n",
      "Epoch [61/200], Train Loss: 0.0221\n",
      "Validation Loss: 0.0318\n",
      "Epoch [62/200], Train Loss: 0.0218\n",
      "Epoch [63/200], Train Loss: 0.0229\n",
      "Epoch [64/200], Train Loss: 0.0216\n",
      "Epoch [65/200], Train Loss: 0.0217\n",
      "Epoch [66/200], Train Loss: 0.0199\n",
      "Validation Loss: 0.0294\n",
      "Epoch [67/200], Train Loss: 0.0201\n",
      "Epoch [68/200], Train Loss: 0.0192\n",
      "Epoch [69/200], Train Loss: 0.0191\n",
      "Epoch [70/200], Train Loss: 0.0205\n",
      "Epoch [71/200], Train Loss: 0.0207\n",
      "Validation Loss: 0.0304\n",
      "Epoch [72/200], Train Loss: 0.0200\n",
      "Epoch [73/200], Train Loss: 0.0194\n",
      "Epoch [74/200], Train Loss: 0.0195\n",
      "Epoch [75/200], Train Loss: 0.0203\n",
      "Epoch [76/200], Train Loss: 0.0185\n",
      "Validation Loss: 0.0286\n",
      "Epoch [77/200], Train Loss: 0.0179\n",
      "Epoch [78/200], Train Loss: 0.0183\n",
      "Epoch [79/200], Train Loss: 0.0179\n",
      "Epoch [80/200], Train Loss: 0.0179\n",
      "Epoch [81/200], Train Loss: 0.0180\n",
      "Validation Loss: 0.0298\n",
      "Epoch [82/200], Train Loss: 0.0174\n",
      "Epoch [83/200], Train Loss: 0.0176\n",
      "Epoch [84/200], Train Loss: 0.0176\n",
      "Epoch [85/200], Train Loss: 0.0171\n",
      "Epoch [86/200], Train Loss: 0.0165\n",
      "Validation Loss: 0.0274\n",
      "Epoch [87/200], Train Loss: 0.0171\n",
      "Epoch [88/200], Train Loss: 0.0175\n",
      "Epoch [89/200], Train Loss: 0.0173\n",
      "Epoch [90/200], Train Loss: 0.0185\n",
      "Epoch [91/200], Train Loss: 0.0194\n",
      "Validation Loss: 0.0332\n",
      "Epoch [92/200], Train Loss: 0.0197\n",
      "Epoch [93/200], Train Loss: 0.0172\n",
      "Epoch [94/200], Train Loss: 0.0174\n",
      "Epoch [95/200], Train Loss: 0.0173\n",
      "Epoch [96/200], Train Loss: 0.0178\n",
      "Validation Loss: 0.0289\n",
      "Epoch [97/200], Train Loss: 0.0176\n",
      "Epoch [98/200], Train Loss: 0.0172\n",
      "Epoch [99/200], Train Loss: 0.0159\n",
      "Epoch [100/200], Train Loss: 0.0157\n",
      "Epoch [101/200], Train Loss: 0.0165\n",
      "Validation Loss: 0.0311\n",
      "Epoch [102/200], Train Loss: 0.0159\n",
      "Epoch [103/200], Train Loss: 0.0164\n",
      "Epoch [104/200], Train Loss: 0.0154\n",
      "Epoch [105/200], Train Loss: 0.0157\n",
      "Epoch [106/200], Train Loss: 0.0157\n",
      "Validation Loss: 0.0267\n",
      "Epoch [107/200], Train Loss: 0.0155\n",
      "Epoch [108/200], Train Loss: 0.0156\n",
      "Epoch [109/200], Train Loss: 0.0151\n",
      "Epoch [110/200], Train Loss: 0.0157\n",
      "Epoch [111/200], Train Loss: 0.0157\n",
      "Early stopping after 110 epochs.\n",
      "Epoch [1/200], Train Loss: 1.9017\n",
      "Validation Loss: 2.5815\n",
      "Epoch [2/200], Train Loss: 1.9154\n",
      "Epoch [3/200], Train Loss: 1.8941\n",
      "Epoch [4/200], Train Loss: 1.9175\n",
      "Epoch [5/200], Train Loss: 1.9097\n",
      "Epoch [6/200], Train Loss: 1.9515\n",
      "Validation Loss: 2.5815\n",
      "Epoch [7/200], Train Loss: 1.8896\n",
      "Epoch [8/200], Train Loss: 1.9247\n",
      "Epoch [9/200], Train Loss: 1.9345\n",
      "Epoch [10/200], Train Loss: 1.9361\n",
      "Epoch [11/200], Train Loss: 1.8979\n",
      "Validation Loss: 2.5815\n",
      "Epoch [12/200], Train Loss: 1.8889\n",
      "Epoch [13/200], Train Loss: 1.9415\n",
      "Epoch [14/200], Train Loss: 1.8784\n",
      "Epoch [15/200], Train Loss: 1.9060\n",
      "Epoch [16/200], Train Loss: 1.9109\n",
      "Validation Loss: 2.5815\n",
      "Epoch [17/200], Train Loss: 1.9135\n",
      "Epoch [18/200], Train Loss: 1.9093\n",
      "Epoch [19/200], Train Loss: 1.9068\n",
      "Epoch [20/200], Train Loss: 1.9266\n",
      "Epoch [21/200], Train Loss: 1.8927\n",
      "Validation Loss: 2.5815\n",
      "Epoch [22/200], Train Loss: 1.9050\n",
      "Epoch [23/200], Train Loss: 1.8841\n",
      "Epoch [24/200], Train Loss: 1.8950\n",
      "Epoch [25/200], Train Loss: 1.9104\n",
      "Epoch [26/200], Train Loss: 1.8890\n",
      "Early stopping after 25 epochs.\n",
      "Epoch [1/200], Train Loss: 1.3087\n",
      "Validation Loss: 0.7665\n",
      "Epoch [2/200], Train Loss: 0.4945\n",
      "Epoch [3/200], Train Loss: 0.3490\n",
      "Epoch [4/200], Train Loss: 0.2676\n",
      "Epoch [5/200], Train Loss: 0.2303\n",
      "Epoch [6/200], Train Loss: 0.1991\n",
      "Validation Loss: 0.2396\n",
      "Epoch [7/200], Train Loss: 0.1632\n",
      "Epoch [8/200], Train Loss: 0.1268\n",
      "Epoch [9/200], Train Loss: 0.1019\n",
      "Epoch [10/200], Train Loss: 0.0854\n",
      "Epoch [11/200], Train Loss: 0.0739\n",
      "Validation Loss: 0.0932\n",
      "Epoch [12/200], Train Loss: 0.0695\n",
      "Epoch [13/200], Train Loss: 0.0632\n",
      "Epoch [14/200], Train Loss: 0.0616\n",
      "Epoch [15/200], Train Loss: 0.0569\n",
      "Epoch [16/200], Train Loss: 0.0537\n",
      "Validation Loss: 0.0793\n",
      "Epoch [17/200], Train Loss: 0.0518\n",
      "Epoch [18/200], Train Loss: 0.0485\n",
      "Epoch [19/200], Train Loss: 0.0482\n",
      "Epoch [20/200], Train Loss: 0.0454\n",
      "Epoch [21/200], Train Loss: 0.0419\n",
      "Validation Loss: 0.0565\n",
      "Epoch [22/200], Train Loss: 0.0402\n",
      "Epoch [23/200], Train Loss: 0.0407\n",
      "Epoch [24/200], Train Loss: 0.0402\n",
      "Epoch [25/200], Train Loss: 0.0394\n",
      "Epoch [26/200], Train Loss: 0.0371\n",
      "Validation Loss: 0.0522\n",
      "Epoch [27/200], Train Loss: 0.0380\n",
      "Epoch [28/200], Train Loss: 0.0338\n",
      "Epoch [29/200], Train Loss: 0.0330\n",
      "Epoch [30/200], Train Loss: 0.0342\n",
      "Epoch [31/200], Train Loss: 0.0347\n",
      "Validation Loss: 0.0456\n",
      "Epoch [32/200], Train Loss: 0.0339\n",
      "Epoch [33/200], Train Loss: 0.0339\n",
      "Epoch [34/200], Train Loss: 0.0328\n",
      "Epoch [35/200], Train Loss: 0.0311\n",
      "Epoch [36/200], Train Loss: 0.0310\n",
      "Validation Loss: 0.0460\n",
      "Epoch [37/200], Train Loss: 0.0325\n",
      "Epoch [38/200], Train Loss: 0.0326\n",
      "Epoch [39/200], Train Loss: 0.0318\n",
      "Epoch [40/200], Train Loss: 0.0298\n",
      "Epoch [41/200], Train Loss: 0.0297\n",
      "Validation Loss: 0.0406\n",
      "Epoch [42/200], Train Loss: 0.0276\n",
      "Epoch [43/200], Train Loss: 0.0274\n",
      "Epoch [44/200], Train Loss: 0.0278\n",
      "Epoch [45/200], Train Loss: 0.0296\n",
      "Epoch [46/200], Train Loss: 0.0275\n",
      "Validation Loss: 0.0407\n",
      "Epoch [47/200], Train Loss: 0.0264\n",
      "Epoch [48/200], Train Loss: 0.0262\n",
      "Epoch [49/200], Train Loss: 0.0273\n",
      "Epoch [50/200], Train Loss: 0.0276\n",
      "Epoch [51/200], Train Loss: 0.0276\n",
      "Validation Loss: 0.0384\n",
      "Epoch [52/200], Train Loss: 0.0289\n",
      "Epoch [53/200], Train Loss: 0.0264\n",
      "Epoch [54/200], Train Loss: 0.0255\n",
      "Epoch [55/200], Train Loss: 0.0253\n",
      "Epoch [56/200], Train Loss: 0.0275\n",
      "Validation Loss: 0.0403\n",
      "Epoch [57/200], Train Loss: 0.0287\n",
      "Epoch [58/200], Train Loss: 0.0303\n",
      "Epoch [59/200], Train Loss: 0.0314\n",
      "Epoch [60/200], Train Loss: 0.0305\n",
      "Epoch [61/200], Train Loss: 0.0295\n",
      "Validation Loss: 0.0463\n",
      "Epoch [62/200], Train Loss: 0.0280\n",
      "Epoch [63/200], Train Loss: 0.0276\n",
      "Epoch [64/200], Train Loss: 0.0277\n",
      "Epoch [65/200], Train Loss: 0.0245\n",
      "Epoch [66/200], Train Loss: 0.0245\n",
      "Validation Loss: 0.0354\n",
      "Epoch [67/200], Train Loss: 0.0228\n",
      "Epoch [68/200], Train Loss: 0.0238\n",
      "Epoch [69/200], Train Loss: 0.0246\n",
      "Epoch [70/200], Train Loss: 0.0244\n",
      "Epoch [71/200], Train Loss: 0.0249\n",
      "Validation Loss: 0.0361\n",
      "Epoch [72/200], Train Loss: 0.0225\n",
      "Epoch [73/200], Train Loss: 0.0223\n",
      "Epoch [74/200], Train Loss: 0.0243\n",
      "Epoch [75/200], Train Loss: 0.0249\n",
      "Epoch [76/200], Train Loss: 0.0258\n",
      "Validation Loss: 0.0372\n",
      "Epoch [77/200], Train Loss: 0.0254\n",
      "Epoch [78/200], Train Loss: 0.0227\n",
      "Epoch [79/200], Train Loss: 0.0234\n",
      "Epoch [80/200], Train Loss: 0.0225\n",
      "Epoch [81/200], Train Loss: 0.0218\n",
      "Validation Loss: 0.0313\n",
      "Epoch [82/200], Train Loss: 0.0222\n",
      "Epoch [83/200], Train Loss: 0.0207\n",
      "Epoch [84/200], Train Loss: 0.0217\n",
      "Epoch [85/200], Train Loss: 0.0211\n",
      "Epoch [86/200], Train Loss: 0.0206\n",
      "Validation Loss: 0.0315\n",
      "Epoch [87/200], Train Loss: 0.0207\n",
      "Epoch [88/200], Train Loss: 0.0203\n",
      "Epoch [89/200], Train Loss: 0.0202\n",
      "Epoch [90/200], Train Loss: 0.0211\n",
      "Epoch [91/200], Train Loss: 0.0233\n",
      "Validation Loss: 0.0439\n",
      "Epoch [92/200], Train Loss: 0.0255\n",
      "Epoch [93/200], Train Loss: 0.0248\n",
      "Epoch [94/200], Train Loss: 0.0208\n",
      "Epoch [95/200], Train Loss: 0.0206\n",
      "Epoch [96/200], Train Loss: 0.0199\n",
      "Validation Loss: 0.0344\n",
      "Epoch [97/200], Train Loss: 0.0198\n",
      "Epoch [98/200], Train Loss: 0.0195\n",
      "Epoch [99/200], Train Loss: 0.0189\n",
      "Epoch [100/200], Train Loss: 0.0207\n",
      "Epoch [101/200], Train Loss: 0.0253\n",
      "Validation Loss: 0.0465\n",
      "Epoch [102/200], Train Loss: 0.0237\n",
      "Epoch [103/200], Train Loss: 0.0208\n",
      "Epoch [104/200], Train Loss: 0.0232\n",
      "Epoch [105/200], Train Loss: 0.0226\n",
      "Epoch [106/200], Train Loss: 0.0265\n",
      "Early stopping after 105 epochs.\n",
      "Epoch [1/200], Train Loss: 1.4880\n",
      "Validation Loss: 1.2188\n",
      "Epoch [2/200], Train Loss: 0.7427\n",
      "Epoch [3/200], Train Loss: 0.4498\n",
      "Epoch [4/200], Train Loss: 0.2732\n",
      "Epoch [5/200], Train Loss: 0.2364\n",
      "Epoch [6/200], Train Loss: 0.1874\n",
      "Validation Loss: 0.2253\n",
      "Epoch [7/200], Train Loss: 0.1601\n",
      "Epoch [8/200], Train Loss: 0.1255\n",
      "Epoch [9/200], Train Loss: 0.0974\n",
      "Epoch [10/200], Train Loss: 0.0883\n",
      "Epoch [11/200], Train Loss: 0.0820\n",
      "Validation Loss: 0.1025\n",
      "Epoch [12/200], Train Loss: 0.0763\n",
      "Epoch [13/200], Train Loss: 0.0719\n",
      "Epoch [14/200], Train Loss: 0.0698\n",
      "Epoch [15/200], Train Loss: 0.0637\n",
      "Epoch [16/200], Train Loss: 0.0632\n",
      "Validation Loss: 0.0887\n",
      "Epoch [17/200], Train Loss: 0.0637\n",
      "Epoch [18/200], Train Loss: 0.0641\n",
      "Epoch [19/200], Train Loss: 0.0617\n",
      "Epoch [20/200], Train Loss: 0.0594\n",
      "Epoch [21/200], Train Loss: 0.0547\n",
      "Validation Loss: 0.0712\n",
      "Epoch [22/200], Train Loss: 0.0533\n",
      "Epoch [23/200], Train Loss: 0.0506\n",
      "Epoch [24/200], Train Loss: 0.0487\n",
      "Epoch [25/200], Train Loss: 0.0474\n",
      "Epoch [26/200], Train Loss: 0.0468\n",
      "Validation Loss: 0.0670\n",
      "Epoch [27/200], Train Loss: 0.0453\n",
      "Epoch [28/200], Train Loss: 0.0438\n",
      "Epoch [29/200], Train Loss: 0.0423\n",
      "Epoch [30/200], Train Loss: 0.0418\n",
      "Epoch [31/200], Train Loss: 0.0416\n",
      "Validation Loss: 0.0585\n",
      "Epoch [32/200], Train Loss: 0.0421\n",
      "Epoch [33/200], Train Loss: 0.0407\n",
      "Epoch [34/200], Train Loss: 0.0377\n",
      "Epoch [35/200], Train Loss: 0.0369\n",
      "Epoch [36/200], Train Loss: 0.0388\n",
      "Validation Loss: 0.0508\n",
      "Epoch [37/200], Train Loss: 0.0366\n",
      "Epoch [38/200], Train Loss: 0.0351\n",
      "Epoch [39/200], Train Loss: 0.0354\n",
      "Epoch [40/200], Train Loss: 0.0345\n",
      "Epoch [41/200], Train Loss: 0.0355\n",
      "Validation Loss: 0.0504\n",
      "Epoch [42/200], Train Loss: 0.0355\n",
      "Epoch [43/200], Train Loss: 0.0331\n",
      "Epoch [44/200], Train Loss: 0.0331\n",
      "Epoch [45/200], Train Loss: 0.0320\n",
      "Epoch [46/200], Train Loss: 0.0308\n",
      "Validation Loss: 0.0445\n",
      "Epoch [47/200], Train Loss: 0.0312\n",
      "Epoch [48/200], Train Loss: 0.0308\n",
      "Epoch [49/200], Train Loss: 0.0302\n",
      "Epoch [50/200], Train Loss: 0.0338\n",
      "Epoch [51/200], Train Loss: 0.0300\n",
      "Validation Loss: 0.0475\n",
      "Epoch [52/200], Train Loss: 0.0308\n",
      "Epoch [53/200], Train Loss: 0.0283\n",
      "Epoch [54/200], Train Loss: 0.0281\n",
      "Epoch [55/200], Train Loss: 0.0286\n",
      "Epoch [56/200], Train Loss: 0.0301\n",
      "Validation Loss: 0.0450\n",
      "Epoch [57/200], Train Loss: 0.0271\n",
      "Epoch [58/200], Train Loss: 0.0279\n",
      "Epoch [59/200], Train Loss: 0.0273\n",
      "Epoch [60/200], Train Loss: 0.0274\n",
      "Epoch [61/200], Train Loss: 0.0262\n",
      "Validation Loss: 0.0393\n",
      "Epoch [62/200], Train Loss: 0.0261\n",
      "Epoch [63/200], Train Loss: 0.0269\n",
      "Epoch [64/200], Train Loss: 0.0267\n",
      "Epoch [65/200], Train Loss: 0.0249\n",
      "Epoch [66/200], Train Loss: 0.0242\n",
      "Validation Loss: 0.0390\n",
      "Epoch [67/200], Train Loss: 0.0256\n",
      "Epoch [68/200], Train Loss: 0.0279\n",
      "Epoch [69/200], Train Loss: 0.0277\n",
      "Epoch [70/200], Train Loss: 0.0277\n",
      "Epoch [71/200], Train Loss: 0.0266\n",
      "Validation Loss: 0.0375\n",
      "Epoch [72/200], Train Loss: 0.0240\n",
      "Epoch [73/200], Train Loss: 0.0237\n",
      "Epoch [74/200], Train Loss: 0.0251\n",
      "Epoch [75/200], Train Loss: 0.0241\n",
      "Epoch [76/200], Train Loss: 0.0245\n",
      "Validation Loss: 0.0430\n",
      "Epoch [77/200], Train Loss: 0.0241\n",
      "Epoch [78/200], Train Loss: 0.0233\n",
      "Epoch [79/200], Train Loss: 0.0241\n",
      "Epoch [80/200], Train Loss: 0.0258\n",
      "Epoch [81/200], Train Loss: 0.0295\n",
      "Validation Loss: 0.0462\n",
      "Epoch [82/200], Train Loss: 0.0245\n",
      "Epoch [83/200], Train Loss: 0.0285\n",
      "Epoch [84/200], Train Loss: 0.0260\n",
      "Epoch [85/200], Train Loss: 0.0309\n",
      "Epoch [86/200], Train Loss: 0.0253\n",
      "Validation Loss: 0.0386\n",
      "Epoch [87/200], Train Loss: 0.0252\n",
      "Epoch [88/200], Train Loss: 0.0237\n",
      "Epoch [89/200], Train Loss: 0.0220\n",
      "Epoch [90/200], Train Loss: 0.0215\n",
      "Epoch [91/200], Train Loss: 0.0218\n",
      "Validation Loss: 0.0359\n",
      "Epoch [92/200], Train Loss: 0.0220\n",
      "Epoch [93/200], Train Loss: 0.0211\n",
      "Epoch [94/200], Train Loss: 0.0215\n",
      "Epoch [95/200], Train Loss: 0.0221\n",
      "Epoch [96/200], Train Loss: 0.0259\n",
      "Validation Loss: 0.0355\n",
      "Epoch [97/200], Train Loss: 0.0227\n",
      "Epoch [98/200], Train Loss: 0.0204\n",
      "Epoch [99/200], Train Loss: 0.0211\n",
      "Epoch [100/200], Train Loss: 0.0204\n",
      "Epoch [101/200], Train Loss: 0.0216\n",
      "Validation Loss: 0.0373\n",
      "Epoch [102/200], Train Loss: 0.0219\n",
      "Epoch [103/200], Train Loss: 0.0210\n",
      "Epoch [104/200], Train Loss: 0.0225\n",
      "Epoch [105/200], Train Loss: 0.0217\n",
      "Epoch [106/200], Train Loss: 0.0212\n",
      "Validation Loss: 0.0348\n",
      "Epoch [107/200], Train Loss: 0.0206\n",
      "Epoch [108/200], Train Loss: 0.0213\n",
      "Epoch [109/200], Train Loss: 0.0240\n",
      "Epoch [110/200], Train Loss: 0.0213\n",
      "Epoch [111/200], Train Loss: 0.0213\n",
      "Validation Loss: 0.0399\n",
      "Epoch [112/200], Train Loss: 0.0220\n",
      "Epoch [113/200], Train Loss: 0.0224\n",
      "Epoch [114/200], Train Loss: 0.0198\n",
      "Epoch [115/200], Train Loss: 0.0213\n",
      "Epoch [116/200], Train Loss: 0.0209\n",
      "Validation Loss: 0.0472\n",
      "Epoch [117/200], Train Loss: 0.0217\n",
      "Epoch [118/200], Train Loss: 0.0204\n",
      "Epoch [119/200], Train Loss: 0.0192\n",
      "Epoch [120/200], Train Loss: 0.0201\n",
      "Epoch [121/200], Train Loss: 0.0207\n",
      "Validation Loss: 0.0465\n",
      "Epoch [122/200], Train Loss: 0.0219\n",
      "Epoch [123/200], Train Loss: 0.0224\n",
      "Epoch [124/200], Train Loss: 0.0207\n",
      "Epoch [125/200], Train Loss: 0.0189\n",
      "Epoch [126/200], Train Loss: 0.0208\n",
      "Validation Loss: 0.0326\n",
      "Epoch [127/200], Train Loss: 0.0180\n",
      "Epoch [128/200], Train Loss: 0.0182\n",
      "Epoch [129/200], Train Loss: 0.0180\n",
      "Epoch [130/200], Train Loss: 0.0181\n",
      "Epoch [131/200], Train Loss: 0.0218\n",
      "Validation Loss: 0.0326\n",
      "Epoch [132/200], Train Loss: 0.0180\n",
      "Epoch [133/200], Train Loss: 0.0198\n",
      "Epoch [134/200], Train Loss: 0.0191\n",
      "Epoch [135/200], Train Loss: 0.0196\n",
      "Epoch [136/200], Train Loss: 0.0184\n",
      "Validation Loss: 0.0311\n",
      "Epoch [137/200], Train Loss: 0.0176\n",
      "Epoch [138/200], Train Loss: 0.0181\n",
      "Epoch [139/200], Train Loss: 0.0181\n",
      "Epoch [140/200], Train Loss: 0.0206\n",
      "Epoch [141/200], Train Loss: 0.0196\n",
      "Validation Loss: 0.0332\n",
      "Epoch [142/200], Train Loss: 0.0188\n",
      "Epoch [143/200], Train Loss: 0.0194\n",
      "Epoch [144/200], Train Loss: 0.0191\n",
      "Epoch [145/200], Train Loss: 0.0191\n",
      "Epoch [146/200], Train Loss: 0.0207\n",
      "Validation Loss: 0.0334\n",
      "Epoch [147/200], Train Loss: 0.0193\n",
      "Epoch [148/200], Train Loss: 0.0219\n",
      "Epoch [149/200], Train Loss: 0.0200\n",
      "Epoch [150/200], Train Loss: 0.0223\n",
      "Epoch [151/200], Train Loss: 0.0193\n",
      "Validation Loss: 0.0385\n",
      "Epoch [152/200], Train Loss: 0.0195\n",
      "Epoch [153/200], Train Loss: 0.0203\n",
      "Epoch [154/200], Train Loss: 0.0206\n",
      "Epoch [155/200], Train Loss: 0.0186\n",
      "Epoch [156/200], Train Loss: 0.0181\n",
      "Validation Loss: 0.0409\n",
      "Epoch [157/200], Train Loss: 0.0199\n",
      "Epoch [158/200], Train Loss: 0.0193\n",
      "Epoch [159/200], Train Loss: 0.0177\n",
      "Epoch [160/200], Train Loss: 0.0177\n",
      "Epoch [161/200], Train Loss: 0.0164\n",
      "Early stopping after 160 epochs.\n",
      "Epoch [1/200], Train Loss: 1.7667\n",
      "Validation Loss: 2.3603\n",
      "Epoch [2/200], Train Loss: 1.7749\n",
      "Epoch [3/200], Train Loss: 1.7432\n",
      "Epoch [4/200], Train Loss: 1.7526\n",
      "Epoch [5/200], Train Loss: 1.7648\n",
      "Epoch [6/200], Train Loss: 1.7304\n",
      "Validation Loss: 2.3603\n",
      "Epoch [7/200], Train Loss: 1.7727\n",
      "Epoch [8/200], Train Loss: 1.7351\n",
      "Epoch [9/200], Train Loss: 1.7557\n",
      "Epoch [10/200], Train Loss: 1.7507\n",
      "Epoch [11/200], Train Loss: 1.7332\n",
      "Validation Loss: 2.3603\n",
      "Epoch [12/200], Train Loss: 1.7670\n",
      "Epoch [13/200], Train Loss: 1.7187\n",
      "Epoch [14/200], Train Loss: 1.7812\n",
      "Epoch [15/200], Train Loss: 1.7314\n",
      "Epoch [16/200], Train Loss: 1.7603\n",
      "Validation Loss: 2.3603\n",
      "Epoch [17/200], Train Loss: 1.7255\n",
      "Epoch [18/200], Train Loss: 1.7471\n",
      "Epoch [19/200], Train Loss: 1.7828\n",
      "Epoch [20/200], Train Loss: 1.7805\n",
      "Epoch [21/200], Train Loss: 1.7561\n",
      "Validation Loss: 2.3603\n",
      "Epoch [22/200], Train Loss: 1.7606\n",
      "Epoch [23/200], Train Loss: 1.7708\n",
      "Epoch [24/200], Train Loss: 1.7439\n",
      "Epoch [25/200], Train Loss: 1.7303\n",
      "Epoch [26/200], Train Loss: 1.7486\n",
      "Early stopping after 25 epochs.\n",
      "Epoch [1/200], Train Loss: 1.4479\n",
      "Validation Loss: 1.8863\n",
      "Epoch [2/200], Train Loss: 1.4064\n",
      "Epoch [3/200], Train Loss: 1.4414\n",
      "Epoch [4/200], Train Loss: 1.4318\n",
      "Epoch [5/200], Train Loss: 1.4306\n",
      "Epoch [6/200], Train Loss: 1.4239\n",
      "Validation Loss: 1.8863\n",
      "Epoch [7/200], Train Loss: 1.4177\n",
      "Epoch [8/200], Train Loss: 1.4287\n",
      "Epoch [9/200], Train Loss: 1.4334\n",
      "Epoch [10/200], Train Loss: 1.4416\n",
      "Epoch [11/200], Train Loss: 1.4212\n",
      "Validation Loss: 1.8863\n",
      "Epoch [12/200], Train Loss: 1.4241\n",
      "Epoch [13/200], Train Loss: 1.4290\n",
      "Epoch [14/200], Train Loss: 1.4485\n",
      "Epoch [15/200], Train Loss: 1.4614\n",
      "Epoch [16/200], Train Loss: 1.4198\n",
      "Validation Loss: 1.8863\n",
      "Epoch [17/200], Train Loss: 1.4235\n",
      "Epoch [18/200], Train Loss: 1.4064\n",
      "Epoch [19/200], Train Loss: 1.4322\n",
      "Epoch [20/200], Train Loss: 1.4290\n",
      "Epoch [21/200], Train Loss: 1.4295\n",
      "Validation Loss: 1.8863\n",
      "Epoch [22/200], Train Loss: 1.4340\n",
      "Epoch [23/200], Train Loss: 1.4370\n",
      "Epoch [24/200], Train Loss: 1.4357\n",
      "Epoch [25/200], Train Loss: 1.4139\n",
      "Epoch [26/200], Train Loss: 1.4311\n",
      "Early stopping after 25 epochs.\n",
      "Epoch [1/200], Train Loss: 1.2267\n",
      "Validation Loss: 1.6074\n",
      "Epoch [2/200], Train Loss: 1.2422\n",
      "Epoch [3/200], Train Loss: 1.2483\n",
      "Epoch [4/200], Train Loss: 1.2412\n",
      "Epoch [5/200], Train Loss: 1.2450\n",
      "Epoch [6/200], Train Loss: 1.2336\n",
      "Validation Loss: 1.6074\n",
      "Epoch [7/200], Train Loss: 1.2243\n",
      "Epoch [8/200], Train Loss: 1.2394\n",
      "Epoch [9/200], Train Loss: 1.2274\n",
      "Epoch [10/200], Train Loss: 1.2284\n",
      "Epoch [11/200], Train Loss: 1.2345\n",
      "Validation Loss: 1.6074\n",
      "Epoch [12/200], Train Loss: 1.2372\n",
      "Epoch [13/200], Train Loss: 1.2382\n",
      "Epoch [14/200], Train Loss: 1.2386\n",
      "Epoch [15/200], Train Loss: 1.2314\n",
      "Epoch [16/200], Train Loss: 1.2406\n",
      "Validation Loss: 1.6074\n",
      "Epoch [17/200], Train Loss: 1.2447\n",
      "Epoch [18/200], Train Loss: 1.2609\n",
      "Epoch [19/200], Train Loss: 1.2290\n",
      "Epoch [20/200], Train Loss: 1.2140\n",
      "Epoch [21/200], Train Loss: 1.2255\n",
      "Validation Loss: 1.6074\n",
      "Epoch [22/200], Train Loss: 1.2223\n",
      "Epoch [23/200], Train Loss: 1.2352\n",
      "Epoch [24/200], Train Loss: 1.2445\n",
      "Epoch [25/200], Train Loss: 1.2163\n",
      "Epoch [26/200], Train Loss: 1.2446\n",
      "Early stopping after 25 epochs.\n",
      "Epoch [1/200], Train Loss: 1.8846\n",
      "Validation Loss: 2.5816\n",
      "Epoch [2/200], Train Loss: 1.9248\n",
      "Epoch [3/200], Train Loss: 1.8606\n",
      "Epoch [4/200], Train Loss: 1.8971\n",
      "Epoch [5/200], Train Loss: 1.9329\n",
      "Epoch [6/200], Train Loss: 1.9409\n",
      "Validation Loss: 2.5816\n",
      "Epoch [7/200], Train Loss: 1.9092\n",
      "Epoch [8/200], Train Loss: 1.8881\n",
      "Epoch [9/200], Train Loss: 1.8825\n",
      "Epoch [10/200], Train Loss: 1.8926\n",
      "Epoch [11/200], Train Loss: 1.8857\n",
      "Validation Loss: 2.5816\n",
      "Epoch [12/200], Train Loss: 1.8946\n",
      "Epoch [13/200], Train Loss: 1.8909\n",
      "Epoch [14/200], Train Loss: 1.9034\n",
      "Epoch [15/200], Train Loss: 1.8738\n",
      "Epoch [16/200], Train Loss: 1.9045\n",
      "Validation Loss: 2.5816\n",
      "Epoch [17/200], Train Loss: 1.8990\n",
      "Epoch [18/200], Train Loss: 1.8691\n",
      "Epoch [19/200], Train Loss: 1.8893\n",
      "Epoch [20/200], Train Loss: 1.8950\n",
      "Epoch [21/200], Train Loss: 1.9110\n",
      "Validation Loss: 2.5816\n",
      "Epoch [22/200], Train Loss: 1.8912\n",
      "Epoch [23/200], Train Loss: 1.8894\n",
      "Epoch [24/200], Train Loss: 1.9268\n",
      "Epoch [25/200], Train Loss: 1.9033\n",
      "Epoch [26/200], Train Loss: 1.9097\n",
      "Early stopping after 25 epochs.\n",
      "Epoch [1/200], Train Loss: 1.4336\n",
      "Validation Loss: 0.8476\n",
      "Epoch [2/200], Train Loss: 0.5485\n",
      "Epoch [3/200], Train Loss: 0.3863\n",
      "Epoch [4/200], Train Loss: 0.2796\n",
      "Epoch [5/200], Train Loss: 0.2339\n",
      "Epoch [6/200], Train Loss: 0.2221\n",
      "Validation Loss: 0.2731\n",
      "Epoch [7/200], Train Loss: 0.2001\n",
      "Epoch [8/200], Train Loss: 0.1890\n",
      "Epoch [9/200], Train Loss: 0.1604\n",
      "Epoch [10/200], Train Loss: 0.1365\n",
      "Epoch [11/200], Train Loss: 0.1167\n",
      "Validation Loss: 0.1723\n",
      "Epoch [12/200], Train Loss: 0.1119\n",
      "Epoch [13/200], Train Loss: 0.1001\n",
      "Epoch [14/200], Train Loss: 0.0890\n",
      "Epoch [15/200], Train Loss: 0.0833\n",
      "Epoch [16/200], Train Loss: 0.0785\n",
      "Validation Loss: 0.1069\n",
      "Epoch [17/200], Train Loss: 0.0762\n",
      "Epoch [18/200], Train Loss: 0.0741\n",
      "Epoch [19/200], Train Loss: 0.0731\n",
      "Epoch [20/200], Train Loss: 0.0682\n",
      "Epoch [21/200], Train Loss: 0.0681\n",
      "Validation Loss: 0.0933\n",
      "Epoch [22/200], Train Loss: 0.0687\n",
      "Epoch [23/200], Train Loss: 0.0647\n",
      "Epoch [24/200], Train Loss: 0.0631\n",
      "Epoch [25/200], Train Loss: 0.0613\n",
      "Epoch [26/200], Train Loss: 0.0594\n",
      "Validation Loss: 0.0808\n",
      "Epoch [27/200], Train Loss: 0.0596\n",
      "Epoch [28/200], Train Loss: 0.0593\n",
      "Epoch [29/200], Train Loss: 0.0577\n",
      "Epoch [30/200], Train Loss: 0.0577\n",
      "Epoch [31/200], Train Loss: 0.0559\n",
      "Validation Loss: 0.0762\n",
      "Epoch [32/200], Train Loss: 0.0538\n",
      "Epoch [33/200], Train Loss: 0.0520\n",
      "Epoch [34/200], Train Loss: 0.0513\n",
      "Epoch [35/200], Train Loss: 0.0532\n",
      "Epoch [36/200], Train Loss: 0.0493\n",
      "Validation Loss: 0.0724\n",
      "Epoch [37/200], Train Loss: 0.0506\n",
      "Epoch [38/200], Train Loss: 0.0498\n",
      "Epoch [39/200], Train Loss: 0.0471\n",
      "Epoch [40/200], Train Loss: 0.0469\n",
      "Epoch [41/200], Train Loss: 0.0445\n",
      "Validation Loss: 0.0649\n",
      "Epoch [42/200], Train Loss: 0.0442\n",
      "Epoch [43/200], Train Loss: 0.0430\n",
      "Epoch [44/200], Train Loss: 0.0429\n",
      "Epoch [45/200], Train Loss: 0.0415\n",
      "Epoch [46/200], Train Loss: 0.0420\n",
      "Validation Loss: 0.0643\n",
      "Epoch [47/200], Train Loss: 0.0418\n",
      "Epoch [48/200], Train Loss: 0.0429\n",
      "Epoch [49/200], Train Loss: 0.0418\n",
      "Epoch [50/200], Train Loss: 0.0410\n",
      "Epoch [51/200], Train Loss: 0.0434\n",
      "Validation Loss: 0.0590\n",
      "Epoch [52/200], Train Loss: 0.0409\n",
      "Epoch [53/200], Train Loss: 0.0403\n",
      "Epoch [54/200], Train Loss: 0.0392\n",
      "Epoch [55/200], Train Loss: 0.0376\n",
      "Epoch [56/200], Train Loss: 0.0378\n",
      "Validation Loss: 0.0618\n",
      "Epoch [57/200], Train Loss: 0.0376\n",
      "Epoch [58/200], Train Loss: 0.0363\n",
      "Epoch [59/200], Train Loss: 0.0354\n",
      "Epoch [60/200], Train Loss: 0.0364\n",
      "Epoch [61/200], Train Loss: 0.0355\n",
      "Validation Loss: 0.0528\n",
      "Epoch [62/200], Train Loss: 0.0354\n",
      "Epoch [63/200], Train Loss: 0.0377\n",
      "Epoch [64/200], Train Loss: 0.0372\n",
      "Epoch [65/200], Train Loss: 0.0352\n",
      "Epoch [66/200], Train Loss: 0.0356\n",
      "Validation Loss: 0.0533\n",
      "Epoch [67/200], Train Loss: 0.0358\n",
      "Epoch [68/200], Train Loss: 0.0359\n",
      "Epoch [69/200], Train Loss: 0.0365\n",
      "Epoch [70/200], Train Loss: 0.0355\n",
      "Epoch [71/200], Train Loss: 0.0356\n",
      "Validation Loss: 0.0560\n",
      "Epoch [72/200], Train Loss: 0.0356\n",
      "Epoch [73/200], Train Loss: 0.0337\n",
      "Epoch [74/200], Train Loss: 0.0320\n",
      "Epoch [75/200], Train Loss: 0.0344\n",
      "Epoch [76/200], Train Loss: 0.0322\n",
      "Validation Loss: 0.0508\n",
      "Epoch [77/200], Train Loss: 0.0321\n",
      "Epoch [78/200], Train Loss: 0.0321\n",
      "Epoch [79/200], Train Loss: 0.0321\n",
      "Epoch [80/200], Train Loss: 0.0322\n",
      "Epoch [81/200], Train Loss: 0.0315\n",
      "Validation Loss: 0.0508\n",
      "Epoch [82/200], Train Loss: 0.0328\n",
      "Epoch [83/200], Train Loss: 0.0326\n",
      "Epoch [84/200], Train Loss: 0.0336\n",
      "Epoch [85/200], Train Loss: 0.0327\n",
      "Epoch [86/200], Train Loss: 0.0318\n",
      "Validation Loss: 0.0483\n",
      "Epoch [87/200], Train Loss: 0.0304\n",
      "Epoch [88/200], Train Loss: 0.0296\n",
      "Epoch [89/200], Train Loss: 0.0308\n",
      "Epoch [90/200], Train Loss: 0.0309\n",
      "Epoch [91/200], Train Loss: 0.0300\n",
      "Validation Loss: 0.0472\n",
      "Epoch [92/200], Train Loss: 0.0307\n",
      "Epoch [93/200], Train Loss: 0.0296\n",
      "Epoch [94/200], Train Loss: 0.0331\n",
      "Epoch [95/200], Train Loss: 0.0330\n",
      "Epoch [96/200], Train Loss: 0.0363\n",
      "Validation Loss: 0.0469\n",
      "Epoch [97/200], Train Loss: 0.0321\n",
      "Epoch [98/200], Train Loss: 0.0357\n",
      "Epoch [99/200], Train Loss: 0.0335\n",
      "Epoch [100/200], Train Loss: 0.0331\n",
      "Epoch [101/200], Train Loss: 0.0306\n",
      "Validation Loss: 0.0465\n",
      "Epoch [102/200], Train Loss: 0.0294\n",
      "Epoch [103/200], Train Loss: 0.0288\n",
      "Epoch [104/200], Train Loss: 0.0285\n",
      "Epoch [105/200], Train Loss: 0.0289\n",
      "Epoch [106/200], Train Loss: 0.0304\n",
      "Validation Loss: 0.0464\n",
      "Epoch [107/200], Train Loss: 0.0284\n",
      "Epoch [108/200], Train Loss: 0.0294\n",
      "Epoch [109/200], Train Loss: 0.0315\n",
      "Epoch [110/200], Train Loss: 0.0320\n",
      "Epoch [111/200], Train Loss: 0.0284\n",
      "Validation Loss: 0.0466\n",
      "Epoch [112/200], Train Loss: 0.0286\n",
      "Epoch [113/200], Train Loss: 0.0301\n",
      "Epoch [114/200], Train Loss: 0.0321\n",
      "Epoch [115/200], Train Loss: 0.0328\n",
      "Epoch [116/200], Train Loss: 0.0334\n",
      "Early stopping after 115 epochs.\n",
      "Epoch [1/200], Train Loss: 1.8877\n",
      "Validation Loss: 2.5815\n",
      "Epoch [2/200], Train Loss: 1.8998\n",
      "Epoch [3/200], Train Loss: 1.8855\n",
      "Epoch [4/200], Train Loss: 1.8938\n",
      "Epoch [5/200], Train Loss: 1.8803\n",
      "Epoch [6/200], Train Loss: 1.8858\n",
      "Validation Loss: 2.5815\n",
      "Epoch [7/200], Train Loss: 1.8955\n",
      "Epoch [8/200], Train Loss: 1.8911\n",
      "Epoch [9/200], Train Loss: 1.9000\n",
      "Epoch [10/200], Train Loss: 1.8956\n",
      "Epoch [11/200], Train Loss: 1.8961\n",
      "Validation Loss: 2.5815\n",
      "Epoch [12/200], Train Loss: 1.8872\n",
      "Epoch [13/200], Train Loss: 1.9041\n",
      "Epoch [14/200], Train Loss: 1.8791\n",
      "Epoch [15/200], Train Loss: 1.8927\n",
      "Epoch [16/200], Train Loss: 1.8789\n",
      "Validation Loss: 2.5815\n",
      "Epoch [17/200], Train Loss: 1.8939\n",
      "Epoch [18/200], Train Loss: 1.9445\n",
      "Epoch [19/200], Train Loss: 1.8793\n",
      "Epoch [20/200], Train Loss: 1.8735\n",
      "Epoch [21/200], Train Loss: 1.9094\n",
      "Validation Loss: 2.5815\n",
      "Epoch [22/200], Train Loss: 1.9090\n",
      "Epoch [23/200], Train Loss: 1.9292\n",
      "Epoch [24/200], Train Loss: 1.9086\n",
      "Epoch [25/200], Train Loss: 1.9002\n",
      "Epoch [26/200], Train Loss: 1.8758\n",
      "Early stopping after 25 epochs.\n",
      "Epoch [1/200], Train Loss: 1.8881\n",
      "Validation Loss: 2.5494\n",
      "Epoch [2/200], Train Loss: 1.8915\n",
      "Epoch [3/200], Train Loss: 1.8628\n",
      "Epoch [4/200], Train Loss: 1.8892\n",
      "Epoch [5/200], Train Loss: 1.8619\n",
      "Epoch [6/200], Train Loss: 1.9266\n",
      "Validation Loss: 2.5494\n",
      "Epoch [7/200], Train Loss: 1.8377\n",
      "Epoch [8/200], Train Loss: 1.8703\n",
      "Epoch [9/200], Train Loss: 1.8819\n",
      "Epoch [10/200], Train Loss: 1.9051\n",
      "Epoch [11/200], Train Loss: 1.8528\n",
      "Validation Loss: 2.5494\n",
      "Epoch [12/200], Train Loss: 1.8910\n",
      "Epoch [13/200], Train Loss: 1.8699\n",
      "Epoch [14/200], Train Loss: 1.8803\n",
      "Epoch [15/200], Train Loss: 1.8724\n",
      "Epoch [16/200], Train Loss: 1.8755\n",
      "Validation Loss: 2.5494\n",
      "Epoch [17/200], Train Loss: 1.8580\n",
      "Epoch [18/200], Train Loss: 1.8813\n",
      "Epoch [19/200], Train Loss: 1.8605\n",
      "Epoch [20/200], Train Loss: 1.8857\n",
      "Epoch [21/200], Train Loss: 1.8683\n",
      "Validation Loss: 2.5494\n",
      "Epoch [22/200], Train Loss: 1.8797\n",
      "Epoch [23/200], Train Loss: 1.8529\n",
      "Epoch [24/200], Train Loss: 1.8700\n",
      "Epoch [25/200], Train Loss: 1.8868\n",
      "Epoch [26/200], Train Loss: 1.8916\n",
      "Early stopping after 25 epochs.\n",
      "Epoch [1/200], Train Loss: 1.6120\n",
      "Validation Loss: 1.7608\n",
      "Epoch [2/200], Train Loss: 0.8576\n",
      "Epoch [3/200], Train Loss: 0.4276\n",
      "Epoch [4/200], Train Loss: 0.3184\n",
      "Epoch [5/200], Train Loss: 0.2491\n",
      "Epoch [6/200], Train Loss: 0.2159\n",
      "Validation Loss: 0.2949\n",
      "Epoch [7/200], Train Loss: 0.1922\n",
      "Epoch [8/200], Train Loss: 0.1631\n",
      "Epoch [9/200], Train Loss: 0.1422\n",
      "Epoch [10/200], Train Loss: 0.1299\n",
      "Epoch [11/200], Train Loss: 0.1213\n",
      "Validation Loss: 0.1711\n",
      "Epoch [12/200], Train Loss: 0.1114\n",
      "Epoch [13/200], Train Loss: 0.1074\n",
      "Epoch [14/200], Train Loss: 0.1074\n",
      "Epoch [15/200], Train Loss: 0.1060\n",
      "Epoch [16/200], Train Loss: 0.1047\n",
      "Validation Loss: 0.1481\n",
      "Epoch [17/200], Train Loss: 0.1044\n",
      "Epoch [18/200], Train Loss: 0.1047\n",
      "Epoch [19/200], Train Loss: 0.1011\n",
      "Epoch [20/200], Train Loss: 0.1005\n",
      "Epoch [21/200], Train Loss: 0.1011\n",
      "Validation Loss: 0.1374\n",
      "Epoch [22/200], Train Loss: 0.1004\n",
      "Epoch [23/200], Train Loss: 0.1007\n",
      "Epoch [24/200], Train Loss: 0.0976\n",
      "Epoch [25/200], Train Loss: 0.0995\n",
      "Epoch [26/200], Train Loss: 0.0985\n",
      "Validation Loss: 0.1360\n",
      "Epoch [27/200], Train Loss: 0.0966\n",
      "Epoch [28/200], Train Loss: 0.0955\n",
      "Epoch [29/200], Train Loss: 0.0974\n",
      "Epoch [30/200], Train Loss: 0.0957\n",
      "Epoch [31/200], Train Loss: 0.0958\n",
      "Validation Loss: 0.1334\n",
      "Epoch [32/200], Train Loss: 0.0949\n",
      "Epoch [33/200], Train Loss: 0.0963\n",
      "Epoch [34/200], Train Loss: 0.0965\n",
      "Epoch [35/200], Train Loss: 0.0945\n",
      "Epoch [36/200], Train Loss: 0.0989\n",
      "Validation Loss: 0.1439\n",
      "Epoch [37/200], Train Loss: 0.1017\n",
      "Epoch [38/200], Train Loss: 0.0966\n",
      "Epoch [39/200], Train Loss: 0.0956\n",
      "Epoch [40/200], Train Loss: 0.0910\n",
      "Epoch [41/200], Train Loss: 0.0918\n",
      "Validation Loss: 0.1313\n",
      "Epoch [42/200], Train Loss: 0.0927\n",
      "Epoch [43/200], Train Loss: 0.0926\n",
      "Epoch [44/200], Train Loss: 0.0936\n",
      "Epoch [45/200], Train Loss: 0.0923\n",
      "Epoch [46/200], Train Loss: 0.0894\n",
      "Validation Loss: 0.1302\n",
      "Epoch [47/200], Train Loss: 0.0914\n",
      "Epoch [48/200], Train Loss: 0.0899\n",
      "Epoch [49/200], Train Loss: 0.0924\n",
      "Epoch [50/200], Train Loss: 0.0928\n",
      "Epoch [51/200], Train Loss: 0.0875\n",
      "Validation Loss: 0.1260\n",
      "Epoch [52/200], Train Loss: 0.0910\n",
      "Epoch [53/200], Train Loss: 0.0894\n",
      "Epoch [54/200], Train Loss: 0.0915\n",
      "Epoch [55/200], Train Loss: 0.0911\n",
      "Epoch [56/200], Train Loss: 0.0876\n",
      "Validation Loss: 0.1329\n",
      "Epoch [57/200], Train Loss: 0.0872\n",
      "Epoch [58/200], Train Loss: 0.0869\n",
      "Epoch [59/200], Train Loss: 0.0867\n",
      "Epoch [60/200], Train Loss: 0.0906\n",
      "Epoch [61/200], Train Loss: 0.0911\n",
      "Validation Loss: 0.1393\n",
      "Epoch [62/200], Train Loss: 0.0901\n",
      "Epoch [63/200], Train Loss: 0.0865\n",
      "Epoch [64/200], Train Loss: 0.0858\n",
      "Epoch [65/200], Train Loss: 0.0855\n",
      "Epoch [66/200], Train Loss: 0.0912\n",
      "Validation Loss: 0.1302\n",
      "Epoch [67/200], Train Loss: 0.0915\n",
      "Epoch [68/200], Train Loss: 0.0885\n",
      "Epoch [69/200], Train Loss: 0.0856\n",
      "Epoch [70/200], Train Loss: 0.0859\n",
      "Epoch [71/200], Train Loss: 0.0881\n",
      "Validation Loss: 0.1278\n",
      "Epoch [72/200], Train Loss: 0.0869\n",
      "Epoch [73/200], Train Loss: 0.0848\n",
      "Epoch [74/200], Train Loss: 0.0851\n",
      "Epoch [75/200], Train Loss: 0.0849\n",
      "Epoch [76/200], Train Loss: 0.0837\n",
      "Early stopping after 75 epochs.\n",
      "Epoch [1/200], Train Loss: 1.2710\n",
      "Validation Loss: 1.5816\n",
      "Epoch [2/200], Train Loss: 0.6900\n",
      "Epoch [3/200], Train Loss: 0.3749\n",
      "Epoch [4/200], Train Loss: 0.2934\n",
      "Epoch [5/200], Train Loss: 0.2528\n",
      "Epoch [6/200], Train Loss: 0.2161\n",
      "Validation Loss: 0.2553\n",
      "Epoch [7/200], Train Loss: 0.1839\n",
      "Epoch [8/200], Train Loss: 0.1756\n",
      "Epoch [9/200], Train Loss: 0.1574\n",
      "Epoch [10/200], Train Loss: 0.1543\n",
      "Epoch [11/200], Train Loss: 0.1505\n",
      "Validation Loss: 0.2225\n",
      "Epoch [12/200], Train Loss: 0.1479\n",
      "Epoch [13/200], Train Loss: 0.1470\n",
      "Epoch [14/200], Train Loss: 0.1399\n",
      "Epoch [15/200], Train Loss: 0.1334\n",
      "Epoch [16/200], Train Loss: 0.1258\n",
      "Validation Loss: 0.1739\n",
      "Epoch [17/200], Train Loss: 0.1130\n",
      "Epoch [18/200], Train Loss: 0.1027\n",
      "Epoch [19/200], Train Loss: 0.0925\n",
      "Epoch [20/200], Train Loss: 0.0895\n",
      "Epoch [21/200], Train Loss: 0.0847\n",
      "Validation Loss: 0.1258\n",
      "Epoch [22/200], Train Loss: 0.0787\n",
      "Epoch [23/200], Train Loss: 0.0736\n",
      "Epoch [24/200], Train Loss: 0.0710\n",
      "Epoch [25/200], Train Loss: 0.0695\n",
      "Epoch [26/200], Train Loss: 0.0666\n",
      "Validation Loss: 0.1055\n",
      "Epoch [27/200], Train Loss: 0.0662\n",
      "Epoch [28/200], Train Loss: 0.0636\n",
      "Epoch [29/200], Train Loss: 0.0626\n",
      "Epoch [30/200], Train Loss: 0.0624\n",
      "Epoch [31/200], Train Loss: 0.0589\n",
      "Validation Loss: 0.0916\n",
      "Epoch [32/200], Train Loss: 0.0550\n",
      "Epoch [33/200], Train Loss: 0.0541\n",
      "Epoch [34/200], Train Loss: 0.0584\n",
      "Epoch [35/200], Train Loss: 0.0557\n",
      "Epoch [36/200], Train Loss: 0.0564\n",
      "Validation Loss: 0.0794\n",
      "Epoch [37/200], Train Loss: 0.0512\n",
      "Epoch [38/200], Train Loss: 0.0522\n",
      "Epoch [39/200], Train Loss: 0.0498\n",
      "Epoch [40/200], Train Loss: 0.0525\n",
      "Epoch [41/200], Train Loss: 0.0500\n",
      "Validation Loss: 0.0807\n",
      "Epoch [42/200], Train Loss: 0.0506\n",
      "Epoch [43/200], Train Loss: 0.0502\n",
      "Epoch [44/200], Train Loss: 0.0492\n",
      "Epoch [45/200], Train Loss: 0.0484\n",
      "Epoch [46/200], Train Loss: 0.0467\n",
      "Validation Loss: 0.0808\n",
      "Epoch [47/200], Train Loss: 0.0478\n",
      "Epoch [48/200], Train Loss: 0.0484\n",
      "Epoch [49/200], Train Loss: 0.0509\n",
      "Epoch [50/200], Train Loss: 0.0497\n",
      "Epoch [51/200], Train Loss: 0.0474\n",
      "Validation Loss: 0.0867\n",
      "Epoch [52/200], Train Loss: 0.0516\n",
      "Epoch [53/200], Train Loss: 0.0481\n",
      "Epoch [54/200], Train Loss: 0.0471\n",
      "Epoch [55/200], Train Loss: 0.0490\n",
      "Epoch [56/200], Train Loss: 0.0474\n",
      "Validation Loss: 0.0770\n",
      "Epoch [57/200], Train Loss: 0.0462\n",
      "Epoch [58/200], Train Loss: 0.0473\n",
      "Epoch [59/200], Train Loss: 0.0449\n",
      "Epoch [60/200], Train Loss: 0.0445\n",
      "Epoch [61/200], Train Loss: 0.0426\n",
      "Validation Loss: 0.0703\n",
      "Epoch [62/200], Train Loss: 0.0429\n",
      "Epoch [63/200], Train Loss: 0.0447\n",
      "Epoch [64/200], Train Loss: 0.0430\n",
      "Epoch [65/200], Train Loss: 0.0440\n",
      "Epoch [66/200], Train Loss: 0.0418\n",
      "Validation Loss: 0.0677\n",
      "Epoch [67/200], Train Loss: 0.0423\n",
      "Epoch [68/200], Train Loss: 0.0431\n",
      "Epoch [69/200], Train Loss: 0.0412\n",
      "Epoch [70/200], Train Loss: 0.0413\n",
      "Epoch [71/200], Train Loss: 0.0424\n",
      "Validation Loss: 0.0688\n",
      "Epoch [72/200], Train Loss: 0.0443\n",
      "Epoch [73/200], Train Loss: 0.0447\n",
      "Epoch [74/200], Train Loss: 0.0424\n",
      "Epoch [75/200], Train Loss: 0.0425\n",
      "Epoch [76/200], Train Loss: 0.0406\n",
      "Validation Loss: 0.0665\n",
      "Epoch [77/200], Train Loss: 0.0395\n",
      "Epoch [78/200], Train Loss: 0.0389\n",
      "Epoch [79/200], Train Loss: 0.0383\n",
      "Epoch [80/200], Train Loss: 0.0388\n",
      "Epoch [81/200], Train Loss: 0.0392\n",
      "Validation Loss: 0.0684\n",
      "Epoch [82/200], Train Loss: 0.0383\n",
      "Epoch [83/200], Train Loss: 0.0389\n",
      "Epoch [84/200], Train Loss: 0.0381\n",
      "Epoch [85/200], Train Loss: 0.0385\n",
      "Epoch [86/200], Train Loss: 0.0388\n",
      "Validation Loss: 0.0724\n",
      "Epoch [87/200], Train Loss: 0.0408\n",
      "Epoch [88/200], Train Loss: 0.0402\n",
      "Epoch [89/200], Train Loss: 0.0393\n",
      "Epoch [90/200], Train Loss: 0.0394\n",
      "Epoch [91/200], Train Loss: 0.0385\n",
      "Validation Loss: 0.0727\n",
      "Epoch [92/200], Train Loss: 0.0394\n",
      "Epoch [93/200], Train Loss: 0.0384\n",
      "Epoch [94/200], Train Loss: 0.0377\n",
      "Epoch [95/200], Train Loss: 0.0357\n",
      "Epoch [96/200], Train Loss: 0.0365\n",
      "Validation Loss: 0.0713\n",
      "Epoch [97/200], Train Loss: 0.0379\n",
      "Epoch [98/200], Train Loss: 0.0414\n",
      "Epoch [99/200], Train Loss: 0.0403\n",
      "Epoch [100/200], Train Loss: 0.0397\n",
      "Epoch [101/200], Train Loss: 0.0393\n",
      "Early stopping after 100 epochs.\n",
      "Epoch [1/200], Train Loss: 1.2229\n",
      "Validation Loss: 1.6074\n",
      "Epoch [2/200], Train Loss: 1.2480\n",
      "Epoch [3/200], Train Loss: 1.2417\n",
      "Epoch [4/200], Train Loss: 1.2303\n",
      "Epoch [5/200], Train Loss: 1.2401\n",
      "Epoch [6/200], Train Loss: 1.2321\n",
      "Validation Loss: 1.6074\n",
      "Epoch [7/200], Train Loss: 1.2391\n",
      "Epoch [8/200], Train Loss: 1.2334\n",
      "Epoch [9/200], Train Loss: 1.2477\n",
      "Epoch [10/200], Train Loss: 1.2306\n",
      "Epoch [11/200], Train Loss: 1.2327\n",
      "Validation Loss: 1.6074\n",
      "Epoch [12/200], Train Loss: 1.2468\n",
      "Epoch [13/200], Train Loss: 1.2269\n",
      "Epoch [14/200], Train Loss: 1.2530\n",
      "Epoch [15/200], Train Loss: 1.2545\n",
      "Epoch [16/200], Train Loss: 1.2321\n",
      "Validation Loss: 1.6074\n",
      "Epoch [17/200], Train Loss: 1.2229\n",
      "Epoch [18/200], Train Loss: 1.2453\n",
      "Epoch [19/200], Train Loss: 1.2479\n",
      "Epoch [20/200], Train Loss: 1.2304\n",
      "Epoch [21/200], Train Loss: 1.2444\n",
      "Validation Loss: 1.6074\n",
      "Epoch [22/200], Train Loss: 1.2514\n",
      "Epoch [23/200], Train Loss: 1.2437\n",
      "Epoch [24/200], Train Loss: 1.2348\n",
      "Epoch [25/200], Train Loss: 1.2336\n",
      "Epoch [26/200], Train Loss: 1.2322\n",
      "Early stopping after 25 epochs.\n",
      "Epoch [1/200], Train Loss: 1.9370\n",
      "Validation Loss: 2.5816\n",
      "Epoch [2/200], Train Loss: 1.8927\n",
      "Epoch [3/200], Train Loss: 1.8663\n",
      "Epoch [4/200], Train Loss: 1.9021\n",
      "Epoch [5/200], Train Loss: 1.9077\n",
      "Epoch [6/200], Train Loss: 1.8960\n",
      "Validation Loss: 2.5816\n",
      "Epoch [7/200], Train Loss: 1.8756\n",
      "Epoch [8/200], Train Loss: 1.8923\n",
      "Epoch [9/200], Train Loss: 1.8834\n",
      "Epoch [10/200], Train Loss: 1.8719\n",
      "Epoch [11/200], Train Loss: 1.8850\n",
      "Validation Loss: 2.5816\n",
      "Epoch [12/200], Train Loss: 1.8992\n",
      "Epoch [13/200], Train Loss: 1.9294\n",
      "Epoch [14/200], Train Loss: 1.9084\n",
      "Epoch [15/200], Train Loss: 1.8957\n",
      "Epoch [16/200], Train Loss: 1.9186\n",
      "Validation Loss: 2.5816\n",
      "Epoch [17/200], Train Loss: 1.9304\n",
      "Epoch [18/200], Train Loss: 1.9450\n",
      "Epoch [19/200], Train Loss: 1.9335\n",
      "Epoch [20/200], Train Loss: 1.9165\n",
      "Epoch [21/200], Train Loss: 1.9089\n",
      "Validation Loss: 2.5816\n",
      "Epoch [22/200], Train Loss: 1.8843\n",
      "Epoch [23/200], Train Loss: 1.8950\n",
      "Epoch [24/200], Train Loss: 1.8881\n",
      "Epoch [25/200], Train Loss: 1.9327\n",
      "Epoch [26/200], Train Loss: 1.8816\n",
      "Early stopping after 25 epochs.\n",
      "Epoch [1/200], Train Loss: 1.5877\n",
      "Validation Loss: 0.9212\n",
      "Epoch [2/200], Train Loss: 0.5947\n",
      "Epoch [3/200], Train Loss: 0.4733\n",
      "Epoch [4/200], Train Loss: 0.3329\n",
      "Epoch [5/200], Train Loss: 0.2714\n",
      "Epoch [6/200], Train Loss: 0.2448\n",
      "Validation Loss: 0.2937\n",
      "Epoch [7/200], Train Loss: 0.2182\n",
      "Epoch [8/200], Train Loss: 0.2125\n",
      "Epoch [9/200], Train Loss: 0.1968\n",
      "Epoch [10/200], Train Loss: 0.1853\n",
      "Epoch [11/200], Train Loss: 0.1704\n",
      "Validation Loss: 0.2145\n",
      "Epoch [12/200], Train Loss: 0.1518\n",
      "Epoch [13/200], Train Loss: 0.1342\n",
      "Epoch [14/200], Train Loss: 0.1247\n",
      "Epoch [15/200], Train Loss: 0.1247\n",
      "Epoch [16/200], Train Loss: 0.1191\n",
      "Validation Loss: 0.1683\n",
      "Epoch [17/200], Train Loss: 0.1153\n",
      "Epoch [18/200], Train Loss: 0.1143\n",
      "Epoch [19/200], Train Loss: 0.1117\n",
      "Epoch [20/200], Train Loss: 0.1124\n",
      "Epoch [21/200], Train Loss: 0.1093\n",
      "Validation Loss: 0.1513\n",
      "Epoch [22/200], Train Loss: 0.1032\n",
      "Epoch [23/200], Train Loss: 0.0995\n",
      "Epoch [24/200], Train Loss: 0.0950\n",
      "Epoch [25/200], Train Loss: 0.0918\n",
      "Epoch [26/200], Train Loss: 0.0800\n",
      "Validation Loss: 0.1018\n",
      "Epoch [27/200], Train Loss: 0.0767\n",
      "Epoch [28/200], Train Loss: 0.0706\n",
      "Epoch [29/200], Train Loss: 0.0702\n",
      "Epoch [30/200], Train Loss: 0.0670\n",
      "Epoch [31/200], Train Loss: 0.0664\n",
      "Validation Loss: 0.0883\n",
      "Epoch [32/200], Train Loss: 0.0639\n",
      "Epoch [33/200], Train Loss: 0.0650\n",
      "Epoch [34/200], Train Loss: 0.0618\n",
      "Epoch [35/200], Train Loss: 0.0601\n",
      "Epoch [36/200], Train Loss: 0.0581\n",
      "Validation Loss: 0.0814\n",
      "Epoch [37/200], Train Loss: 0.0579\n",
      "Epoch [38/200], Train Loss: 0.0579\n",
      "Epoch [39/200], Train Loss: 0.0594\n",
      "Epoch [40/200], Train Loss: 0.0559\n",
      "Epoch [41/200], Train Loss: 0.0538\n",
      "Validation Loss: 0.0760\n",
      "Epoch [42/200], Train Loss: 0.0534\n",
      "Epoch [43/200], Train Loss: 0.0500\n",
      "Epoch [44/200], Train Loss: 0.0506\n",
      "Epoch [45/200], Train Loss: 0.0497\n",
      "Epoch [46/200], Train Loss: 0.0479\n",
      "Validation Loss: 0.0802\n",
      "Epoch [47/200], Train Loss: 0.0491\n",
      "Epoch [48/200], Train Loss: 0.0471\n",
      "Epoch [49/200], Train Loss: 0.0457\n",
      "Epoch [50/200], Train Loss: 0.0471\n",
      "Epoch [51/200], Train Loss: 0.0494\n",
      "Validation Loss: 0.0614\n",
      "Epoch [52/200], Train Loss: 0.0434\n",
      "Epoch [53/200], Train Loss: 0.0463\n",
      "Epoch [54/200], Train Loss: 0.0434\n",
      "Epoch [55/200], Train Loss: 0.0413\n",
      "Epoch [56/200], Train Loss: 0.0414\n",
      "Validation Loss: 0.0579\n",
      "Epoch [57/200], Train Loss: 0.0416\n",
      "Epoch [58/200], Train Loss: 0.0421\n",
      "Epoch [59/200], Train Loss: 0.0398\n",
      "Epoch [60/200], Train Loss: 0.0404\n",
      "Epoch [61/200], Train Loss: 0.0424\n",
      "Validation Loss: 0.0710\n",
      "Epoch [62/200], Train Loss: 0.0439\n",
      "Epoch [63/200], Train Loss: 0.0419\n",
      "Epoch [64/200], Train Loss: 0.0414\n",
      "Epoch [65/200], Train Loss: 0.0397\n",
      "Epoch [66/200], Train Loss: 0.0377\n",
      "Validation Loss: 0.0560\n",
      "Epoch [67/200], Train Loss: 0.0381\n",
      "Epoch [68/200], Train Loss: 0.0384\n",
      "Epoch [69/200], Train Loss: 0.0377\n",
      "Epoch [70/200], Train Loss: 0.0370\n",
      "Epoch [71/200], Train Loss: 0.0370\n",
      "Validation Loss: 0.0549\n",
      "Epoch [72/200], Train Loss: 0.0362\n",
      "Epoch [73/200], Train Loss: 0.0364\n",
      "Epoch [74/200], Train Loss: 0.0370\n",
      "Epoch [75/200], Train Loss: 0.0380\n",
      "Epoch [76/200], Train Loss: 0.0382\n",
      "Validation Loss: 0.0529\n",
      "Epoch [77/200], Train Loss: 0.0362\n",
      "Epoch [78/200], Train Loss: 0.0371\n",
      "Epoch [79/200], Train Loss: 0.0363\n",
      "Epoch [80/200], Train Loss: 0.0360\n",
      "Epoch [81/200], Train Loss: 0.0359\n",
      "Validation Loss: 0.0531\n",
      "Epoch [82/200], Train Loss: 0.0356\n",
      "Epoch [83/200], Train Loss: 0.0384\n",
      "Epoch [84/200], Train Loss: 0.0413\n",
      "Epoch [85/200], Train Loss: 0.0415\n",
      "Epoch [86/200], Train Loss: 0.0396\n",
      "Validation Loss: 0.0581\n",
      "Epoch [87/200], Train Loss: 0.0359\n",
      "Epoch [88/200], Train Loss: 0.0371\n",
      "Epoch [89/200], Train Loss: 0.0357\n",
      "Epoch [90/200], Train Loss: 0.0357\n",
      "Epoch [91/200], Train Loss: 0.0363\n",
      "Validation Loss: 0.0546\n",
      "Epoch [92/200], Train Loss: 0.0346\n",
      "Epoch [93/200], Train Loss: 0.0340\n",
      "Epoch [94/200], Train Loss: 0.0350\n",
      "Epoch [95/200], Train Loss: 0.0350\n",
      "Epoch [96/200], Train Loss: 0.0361\n",
      "Validation Loss: 0.0534\n",
      "Epoch [97/200], Train Loss: 0.0349\n",
      "Epoch [98/200], Train Loss: 0.0341\n",
      "Epoch [99/200], Train Loss: 0.0351\n",
      "Epoch [100/200], Train Loss: 0.0343\n",
      "Epoch [101/200], Train Loss: 0.0360\n",
      "Early stopping after 100 epochs.\n",
      "Epoch [1/200], Train Loss: 1.4840\n",
      "Validation Loss: 0.5473\n",
      "Epoch [2/200], Train Loss: 0.5262\n",
      "Epoch [3/200], Train Loss: 0.4337\n",
      "Epoch [4/200], Train Loss: 0.3071\n",
      "Epoch [5/200], Train Loss: 0.2723\n",
      "Epoch [6/200], Train Loss: 0.2424\n",
      "Validation Loss: 0.3243\n",
      "Epoch [7/200], Train Loss: 0.2200\n",
      "Epoch [8/200], Train Loss: 0.2045\n",
      "Epoch [9/200], Train Loss: 0.1872\n",
      "Epoch [10/200], Train Loss: 0.1670\n",
      "Epoch [11/200], Train Loss: 0.1458\n",
      "Validation Loss: 0.1935\n",
      "Epoch [12/200], Train Loss: 0.1320\n",
      "Epoch [13/200], Train Loss: 0.1318\n",
      "Epoch [14/200], Train Loss: 0.1165\n",
      "Epoch [15/200], Train Loss: 0.1059\n",
      "Epoch [16/200], Train Loss: 0.0950\n",
      "Validation Loss: 0.1351\n",
      "Epoch [17/200], Train Loss: 0.0872\n",
      "Epoch [18/200], Train Loss: 0.0800\n",
      "Epoch [19/200], Train Loss: 0.0753\n",
      "Epoch [20/200], Train Loss: 0.0728\n",
      "Epoch [21/200], Train Loss: 0.0713\n",
      "Validation Loss: 0.1062\n",
      "Epoch [22/200], Train Loss: 0.0721\n",
      "Epoch [23/200], Train Loss: 0.0716\n",
      "Epoch [24/200], Train Loss: 0.0698\n",
      "Epoch [25/200], Train Loss: 0.0682\n",
      "Epoch [26/200], Train Loss: 0.0675\n",
      "Validation Loss: 0.0998\n",
      "Epoch [27/200], Train Loss: 0.0642\n",
      "Epoch [28/200], Train Loss: 0.0641\n",
      "Epoch [29/200], Train Loss: 0.0615\n",
      "Epoch [30/200], Train Loss: 0.0620\n",
      "Epoch [31/200], Train Loss: 0.0601\n",
      "Validation Loss: 0.0827\n",
      "Epoch [32/200], Train Loss: 0.0589\n",
      "Epoch [33/200], Train Loss: 0.0615\n",
      "Epoch [34/200], Train Loss: 0.0589\n",
      "Epoch [35/200], Train Loss: 0.0601\n",
      "Epoch [36/200], Train Loss: 0.0632\n",
      "Validation Loss: 0.0854\n",
      "Epoch [37/200], Train Loss: 0.0544\n",
      "Epoch [38/200], Train Loss: 0.0538\n",
      "Epoch [39/200], Train Loss: 0.0512\n",
      "Epoch [40/200], Train Loss: 0.0502\n",
      "Epoch [41/200], Train Loss: 0.0512\n",
      "Validation Loss: 0.0711\n",
      "Epoch [42/200], Train Loss: 0.0501\n",
      "Epoch [43/200], Train Loss: 0.0519\n",
      "Epoch [44/200], Train Loss: 0.0499\n",
      "Epoch [45/200], Train Loss: 0.0489\n",
      "Epoch [46/200], Train Loss: 0.0460\n",
      "Validation Loss: 0.0672\n",
      "Epoch [47/200], Train Loss: 0.0461\n",
      "Epoch [48/200], Train Loss: 0.0453\n",
      "Epoch [49/200], Train Loss: 0.0439\n",
      "Epoch [50/200], Train Loss: 0.0411\n",
      "Epoch [51/200], Train Loss: 0.0411\n",
      "Validation Loss: 0.0559\n",
      "Epoch [52/200], Train Loss: 0.0406\n",
      "Epoch [53/200], Train Loss: 0.0389\n",
      "Epoch [54/200], Train Loss: 0.0396\n",
      "Epoch [55/200], Train Loss: 0.0390\n",
      "Epoch [56/200], Train Loss: 0.0394\n",
      "Validation Loss: 0.0532\n",
      "Epoch [57/200], Train Loss: 0.0388\n",
      "Epoch [58/200], Train Loss: 0.0379\n",
      "Epoch [59/200], Train Loss: 0.0406\n",
      "Epoch [60/200], Train Loss: 0.0396\n",
      "Epoch [61/200], Train Loss: 0.0427\n",
      "Validation Loss: 0.0572\n",
      "Epoch [62/200], Train Loss: 0.0412\n",
      "Epoch [63/200], Train Loss: 0.0375\n",
      "Epoch [64/200], Train Loss: 0.0391\n",
      "Epoch [65/200], Train Loss: 0.0415\n",
      "Epoch [66/200], Train Loss: 0.0393\n",
      "Validation Loss: 0.0509\n",
      "Epoch [67/200], Train Loss: 0.0353\n",
      "Epoch [68/200], Train Loss: 0.0361\n",
      "Epoch [69/200], Train Loss: 0.0347\n",
      "Epoch [70/200], Train Loss: 0.0356\n",
      "Epoch [71/200], Train Loss: 0.0358\n",
      "Validation Loss: 0.0490\n",
      "Epoch [72/200], Train Loss: 0.0360\n",
      "Epoch [73/200], Train Loss: 0.0363\n",
      "Epoch [74/200], Train Loss: 0.0342\n",
      "Epoch [75/200], Train Loss: 0.0349\n",
      "Epoch [76/200], Train Loss: 0.0343\n",
      "Validation Loss: 0.0487\n",
      "Epoch [77/200], Train Loss: 0.0339\n",
      "Epoch [78/200], Train Loss: 0.0340\n",
      "Epoch [79/200], Train Loss: 0.0335\n",
      "Epoch [80/200], Train Loss: 0.0356\n",
      "Epoch [81/200], Train Loss: 0.0362\n",
      "Validation Loss: 0.0502\n",
      "Epoch [82/200], Train Loss: 0.0381\n",
      "Epoch [83/200], Train Loss: 0.0381\n",
      "Epoch [84/200], Train Loss: 0.0346\n",
      "Epoch [85/200], Train Loss: 0.0352\n",
      "Epoch [86/200], Train Loss: 0.0349\n",
      "Validation Loss: 0.0544\n",
      "Epoch [87/200], Train Loss: 0.0373\n",
      "Epoch [88/200], Train Loss: 0.0356\n",
      "Epoch [89/200], Train Loss: 0.0341\n",
      "Epoch [90/200], Train Loss: 0.0324\n",
      "Epoch [91/200], Train Loss: 0.0334\n",
      "Validation Loss: 0.0472\n",
      "Epoch [92/200], Train Loss: 0.0336\n",
      "Epoch [93/200], Train Loss: 0.0341\n",
      "Epoch [94/200], Train Loss: 0.0346\n",
      "Epoch [95/200], Train Loss: 0.0318\n",
      "Epoch [96/200], Train Loss: 0.0321\n",
      "Validation Loss: 0.0524\n",
      "Epoch [97/200], Train Loss: 0.0333\n",
      "Epoch [98/200], Train Loss: 0.0345\n",
      "Epoch [99/200], Train Loss: 0.0334\n",
      "Epoch [100/200], Train Loss: 0.0333\n",
      "Epoch [101/200], Train Loss: 0.0335\n",
      "Validation Loss: 0.0467\n",
      "Epoch [102/200], Train Loss: 0.0334\n",
      "Epoch [103/200], Train Loss: 0.0341\n",
      "Epoch [104/200], Train Loss: 0.0326\n",
      "Epoch [105/200], Train Loss: 0.0331\n",
      "Epoch [106/200], Train Loss: 0.0344\n",
      "Validation Loss: 0.0514\n",
      "Epoch [107/200], Train Loss: 0.0341\n",
      "Epoch [108/200], Train Loss: 0.0328\n",
      "Epoch [109/200], Train Loss: 0.0315\n",
      "Epoch [110/200], Train Loss: 0.0310\n",
      "Epoch [111/200], Train Loss: 0.0313\n",
      "Validation Loss: 0.0533\n",
      "Epoch [112/200], Train Loss: 0.0311\n",
      "Epoch [113/200], Train Loss: 0.0308\n",
      "Epoch [114/200], Train Loss: 0.0310\n",
      "Epoch [115/200], Train Loss: 0.0313\n",
      "Epoch [116/200], Train Loss: 0.0328\n",
      "Early stopping after 115 epochs.\n",
      "Epoch [1/200], Train Loss: 1.8783\n",
      "Validation Loss: 2.5494\n",
      "Epoch [2/200], Train Loss: 1.8499\n",
      "Epoch [3/200], Train Loss: 1.8795\n",
      "Epoch [4/200], Train Loss: 1.8831\n",
      "Epoch [5/200], Train Loss: 1.8796\n",
      "Epoch [6/200], Train Loss: 1.8798\n",
      "Validation Loss: 2.5494\n",
      "Epoch [7/200], Train Loss: 1.8929\n",
      "Epoch [8/200], Train Loss: 1.8884\n",
      "Epoch [9/200], Train Loss: 1.8581\n",
      "Epoch [10/200], Train Loss: 1.8960\n",
      "Epoch [11/200], Train Loss: 1.8636\n",
      "Validation Loss: 2.5494\n",
      "Epoch [12/200], Train Loss: 1.8867\n",
      "Epoch [13/200], Train Loss: 1.8684\n",
      "Epoch [14/200], Train Loss: 1.8459\n",
      "Epoch [15/200], Train Loss: 1.8863\n",
      "Epoch [16/200], Train Loss: 1.8545\n",
      "Validation Loss: 2.5494\n",
      "Epoch [17/200], Train Loss: 1.8973\n",
      "Epoch [18/200], Train Loss: 1.8914\n",
      "Epoch [19/200], Train Loss: 1.8938\n",
      "Epoch [20/200], Train Loss: 1.8628\n",
      "Epoch [21/200], Train Loss: 1.8869\n",
      "Validation Loss: 2.5494\n",
      "Epoch [22/200], Train Loss: 1.8558\n",
      "Epoch [23/200], Train Loss: 1.8499\n",
      "Epoch [24/200], Train Loss: 1.9011\n",
      "Epoch [25/200], Train Loss: 1.8636\n",
      "Epoch [26/200], Train Loss: 1.8988\n",
      "Early stopping after 25 epochs.\n",
      "Epoch [1/200], Train Loss: 1.4712\n",
      "Validation Loss: 1.3675\n",
      "Epoch [2/200], Train Loss: 0.6484\n",
      "Epoch [3/200], Train Loss: 0.3433\n",
      "Epoch [4/200], Train Loss: 0.2243\n",
      "Epoch [5/200], Train Loss: 0.1767\n",
      "Epoch [6/200], Train Loss: 0.1538\n",
      "Validation Loss: 0.1839\n",
      "Epoch [7/200], Train Loss: 0.1248\n",
      "Epoch [8/200], Train Loss: 0.1066\n",
      "Epoch [9/200], Train Loss: 0.0930\n",
      "Epoch [10/200], Train Loss: 0.0839\n",
      "Epoch [11/200], Train Loss: 0.0779\n",
      "Validation Loss: 0.1045\n",
      "Epoch [12/200], Train Loss: 0.0752\n",
      "Epoch [13/200], Train Loss: 0.0714\n",
      "Epoch [14/200], Train Loss: 0.0694\n",
      "Epoch [15/200], Train Loss: 0.0652\n",
      "Epoch [16/200], Train Loss: 0.0643\n",
      "Validation Loss: 0.0932\n",
      "Epoch [17/200], Train Loss: 0.0627\n",
      "Epoch [18/200], Train Loss: 0.0587\n",
      "Epoch [19/200], Train Loss: 0.0581\n",
      "Epoch [20/200], Train Loss: 0.0551\n",
      "Epoch [21/200], Train Loss: 0.0512\n",
      "Validation Loss: 0.0701\n",
      "Epoch [22/200], Train Loss: 0.0492\n",
      "Epoch [23/200], Train Loss: 0.0473\n",
      "Epoch [24/200], Train Loss: 0.0456\n",
      "Epoch [25/200], Train Loss: 0.0453\n",
      "Epoch [26/200], Train Loss: 0.0455\n",
      "Validation Loss: 0.0596\n",
      "Epoch [27/200], Train Loss: 0.0418\n",
      "Epoch [28/200], Train Loss: 0.0398\n",
      "Epoch [29/200], Train Loss: 0.0388\n",
      "Epoch [30/200], Train Loss: 0.0404\n",
      "Epoch [31/200], Train Loss: 0.0385\n",
      "Validation Loss: 0.0531\n",
      "Epoch [32/200], Train Loss: 0.0381\n",
      "Epoch [33/200], Train Loss: 0.0397\n",
      "Epoch [34/200], Train Loss: 0.0362\n",
      "Epoch [35/200], Train Loss: 0.0353\n",
      "Epoch [36/200], Train Loss: 0.0323\n",
      "Validation Loss: 0.0535\n",
      "Epoch [37/200], Train Loss: 0.0337\n",
      "Epoch [38/200], Train Loss: 0.0328\n",
      "Epoch [39/200], Train Loss: 0.0352\n",
      "Epoch [40/200], Train Loss: 0.0344\n",
      "Epoch [41/200], Train Loss: 0.0322\n",
      "Validation Loss: 0.0504\n",
      "Epoch [42/200], Train Loss: 0.0343\n",
      "Epoch [43/200], Train Loss: 0.0382\n",
      "Epoch [44/200], Train Loss: 0.0372\n",
      "Epoch [45/200], Train Loss: 0.0314\n",
      "Epoch [46/200], Train Loss: 0.0308\n",
      "Validation Loss: 0.0424\n",
      "Epoch [47/200], Train Loss: 0.0293\n",
      "Epoch [48/200], Train Loss: 0.0296\n",
      "Epoch [49/200], Train Loss: 0.0275\n",
      "Epoch [50/200], Train Loss: 0.0284\n",
      "Epoch [51/200], Train Loss: 0.0283\n",
      "Validation Loss: 0.0407\n",
      "Epoch [52/200], Train Loss: 0.0284\n",
      "Epoch [53/200], Train Loss: 0.0291\n",
      "Epoch [54/200], Train Loss: 0.0285\n",
      "Epoch [55/200], Train Loss: 0.0270\n",
      "Epoch [56/200], Train Loss: 0.0270\n",
      "Validation Loss: 0.0408\n",
      "Epoch [57/200], Train Loss: 0.0267\n",
      "Epoch [58/200], Train Loss: 0.0267\n",
      "Epoch [59/200], Train Loss: 0.0261\n",
      "Epoch [60/200], Train Loss: 0.0264\n",
      "Epoch [61/200], Train Loss: 0.0256\n",
      "Validation Loss: 0.0372\n",
      "Epoch [62/200], Train Loss: 0.0259\n",
      "Epoch [63/200], Train Loss: 0.0260\n",
      "Epoch [64/200], Train Loss: 0.0269\n",
      "Epoch [65/200], Train Loss: 0.0243\n",
      "Epoch [66/200], Train Loss: 0.0259\n",
      "Validation Loss: 0.0366\n",
      "Epoch [67/200], Train Loss: 0.0270\n",
      "Epoch [68/200], Train Loss: 0.0278\n",
      "Epoch [69/200], Train Loss: 0.0264\n",
      "Epoch [70/200], Train Loss: 0.0253\n",
      "Epoch [71/200], Train Loss: 0.0248\n",
      "Validation Loss: 0.0361\n",
      "Epoch [72/200], Train Loss: 0.0249\n",
      "Epoch [73/200], Train Loss: 0.0238\n",
      "Epoch [74/200], Train Loss: 0.0237\n",
      "Epoch [75/200], Train Loss: 0.0236\n",
      "Epoch [76/200], Train Loss: 0.0243\n",
      "Validation Loss: 0.0357\n",
      "Epoch [77/200], Train Loss: 0.0244\n",
      "Epoch [78/200], Train Loss: 0.0260\n",
      "Epoch [79/200], Train Loss: 0.0269\n",
      "Epoch [80/200], Train Loss: 0.0264\n",
      "Epoch [81/200], Train Loss: 0.0246\n",
      "Validation Loss: 0.0349\n",
      "Epoch [82/200], Train Loss: 0.0225\n",
      "Epoch [83/200], Train Loss: 0.0231\n",
      "Epoch [84/200], Train Loss: 0.0229\n",
      "Epoch [85/200], Train Loss: 0.0218\n",
      "Epoch [86/200], Train Loss: 0.0222\n",
      "Validation Loss: 0.0381\n",
      "Epoch [87/200], Train Loss: 0.0239\n",
      "Epoch [88/200], Train Loss: 0.0254\n",
      "Epoch [89/200], Train Loss: 0.0267\n",
      "Epoch [90/200], Train Loss: 0.0268\n",
      "Epoch [91/200], Train Loss: 0.0270\n",
      "Validation Loss: 0.0371\n",
      "Epoch [92/200], Train Loss: 0.0234\n",
      "Epoch [93/200], Train Loss: 0.0231\n",
      "Epoch [94/200], Train Loss: 0.0221\n",
      "Epoch [95/200], Train Loss: 0.0226\n",
      "Epoch [96/200], Train Loss: 0.0216\n",
      "Validation Loss: 0.0360\n",
      "Epoch [97/200], Train Loss: 0.0215\n",
      "Epoch [98/200], Train Loss: 0.0211\n",
      "Epoch [99/200], Train Loss: 0.0238\n",
      "Epoch [100/200], Train Loss: 0.0225\n",
      "Epoch [101/200], Train Loss: 0.0219\n",
      "Validation Loss: 0.0411\n",
      "Epoch [102/200], Train Loss: 0.0217\n",
      "Epoch [103/200], Train Loss: 0.0205\n",
      "Epoch [104/200], Train Loss: 0.0214\n",
      "Epoch [105/200], Train Loss: 0.0224\n",
      "Epoch [106/200], Train Loss: 0.0206\n",
      "Early stopping after 105 epochs.\n",
      "Epoch [1/200], Train Loss: 1.1807\n",
      "Validation Loss: 1.1104\n",
      "Epoch [2/200], Train Loss: 0.6074\n",
      "Epoch [3/200], Train Loss: 0.4039\n",
      "Epoch [4/200], Train Loss: 0.2918\n",
      "Epoch [5/200], Train Loss: 0.2683\n",
      "Epoch [6/200], Train Loss: 0.2153\n",
      "Validation Loss: 0.2854\n",
      "Epoch [7/200], Train Loss: 0.1905\n",
      "Epoch [8/200], Train Loss: 0.1734\n",
      "Epoch [9/200], Train Loss: 0.1580\n",
      "Epoch [10/200], Train Loss: 0.1405\n",
      "Epoch [11/200], Train Loss: 0.1235\n",
      "Validation Loss: 0.1729\n",
      "Epoch [12/200], Train Loss: 0.1142\n",
      "Epoch [13/200], Train Loss: 0.1085\n",
      "Epoch [14/200], Train Loss: 0.1024\n",
      "Epoch [15/200], Train Loss: 0.0991\n",
      "Epoch [16/200], Train Loss: 0.0934\n",
      "Validation Loss: 0.1341\n",
      "Epoch [17/200], Train Loss: 0.0899\n",
      "Epoch [18/200], Train Loss: 0.0914\n",
      "Epoch [19/200], Train Loss: 0.0860\n",
      "Epoch [20/200], Train Loss: 0.0784\n",
      "Epoch [21/200], Train Loss: 0.0739\n",
      "Validation Loss: 0.1098\n",
      "Epoch [22/200], Train Loss: 0.0709\n",
      "Epoch [23/200], Train Loss: 0.0670\n",
      "Epoch [24/200], Train Loss: 0.0650\n",
      "Epoch [25/200], Train Loss: 0.0634\n",
      "Epoch [26/200], Train Loss: 0.0625\n",
      "Validation Loss: 0.0975\n",
      "Epoch [27/200], Train Loss: 0.0582\n",
      "Epoch [28/200], Train Loss: 0.0572\n",
      "Epoch [29/200], Train Loss: 0.0581\n",
      "Epoch [30/200], Train Loss: 0.0575\n",
      "Epoch [31/200], Train Loss: 0.0556\n",
      "Validation Loss: 0.0854\n",
      "Epoch [32/200], Train Loss: 0.0542\n",
      "Epoch [33/200], Train Loss: 0.0542\n",
      "Epoch [34/200], Train Loss: 0.0514\n",
      "Epoch [35/200], Train Loss: 0.0529\n",
      "Epoch [36/200], Train Loss: 0.0518\n",
      "Validation Loss: 0.0855\n",
      "Epoch [37/200], Train Loss: 0.0513\n",
      "Epoch [38/200], Train Loss: 0.0517\n",
      "Epoch [39/200], Train Loss: 0.0504\n",
      "Epoch [40/200], Train Loss: 0.0511\n",
      "Epoch [41/200], Train Loss: 0.0515\n",
      "Validation Loss: 0.0811\n",
      "Epoch [42/200], Train Loss: 0.0509\n",
      "Epoch [43/200], Train Loss: 0.0483\n",
      "Epoch [44/200], Train Loss: 0.0503\n",
      "Epoch [45/200], Train Loss: 0.0537\n",
      "Epoch [46/200], Train Loss: 0.0483\n",
      "Validation Loss: 0.0876\n",
      "Epoch [47/200], Train Loss: 0.0494\n",
      "Epoch [48/200], Train Loss: 0.0459\n",
      "Epoch [49/200], Train Loss: 0.0445\n",
      "Epoch [50/200], Train Loss: 0.0448\n",
      "Epoch [51/200], Train Loss: 0.0454\n",
      "Validation Loss: 0.0788\n",
      "Epoch [52/200], Train Loss: 0.0445\n",
      "Epoch [53/200], Train Loss: 0.0446\n",
      "Epoch [54/200], Train Loss: 0.0447\n",
      "Epoch [55/200], Train Loss: 0.0446\n",
      "Epoch [56/200], Train Loss: 0.0435\n",
      "Validation Loss: 0.0874\n",
      "Epoch [57/200], Train Loss: 0.0437\n",
      "Epoch [58/200], Train Loss: 0.0425\n",
      "Epoch [59/200], Train Loss: 0.0433\n",
      "Epoch [60/200], Train Loss: 0.0444\n",
      "Epoch [61/200], Train Loss: 0.0423\n",
      "Validation Loss: 0.0939\n",
      "Epoch [62/200], Train Loss: 0.0449\n",
      "Epoch [63/200], Train Loss: 0.0434\n",
      "Epoch [64/200], Train Loss: 0.0430\n",
      "Epoch [65/200], Train Loss: 0.0417\n",
      "Epoch [66/200], Train Loss: 0.0421\n",
      "Validation Loss: 0.0787\n",
      "Epoch [67/200], Train Loss: 0.0435\n",
      "Epoch [68/200], Train Loss: 0.0432\n",
      "Epoch [69/200], Train Loss: 0.0450\n",
      "Epoch [70/200], Train Loss: 0.0443\n",
      "Epoch [71/200], Train Loss: 0.0430\n",
      "Validation Loss: 0.0820\n",
      "Epoch [72/200], Train Loss: 0.0434\n",
      "Epoch [73/200], Train Loss: 0.0428\n",
      "Epoch [74/200], Train Loss: 0.0418\n",
      "Epoch [75/200], Train Loss: 0.0433\n",
      "Epoch [76/200], Train Loss: 0.0466\n",
      "Early stopping after 75 epochs.\n",
      "Epoch [1/200], Train Loss: 1.2207\n",
      "Validation Loss: 1.5780\n",
      "Epoch [2/200], Train Loss: 1.2193\n",
      "Epoch [3/200], Train Loss: 1.2057\n",
      "Epoch [4/200], Train Loss: 1.1922\n",
      "Epoch [5/200], Train Loss: 1.1873\n",
      "Epoch [6/200], Train Loss: 1.1979\n",
      "Validation Loss: 1.5552\n",
      "Epoch [7/200], Train Loss: 1.1825\n",
      "Epoch [8/200], Train Loss: 1.1820\n",
      "Epoch [9/200], Train Loss: 1.1747\n",
      "Epoch [10/200], Train Loss: 1.1765\n",
      "Epoch [11/200], Train Loss: 1.1980\n",
      "Validation Loss: 1.5507\n",
      "Epoch [12/200], Train Loss: 1.1885\n",
      "Epoch [13/200], Train Loss: 1.1630\n",
      "Epoch [14/200], Train Loss: 1.1761\n",
      "Epoch [15/200], Train Loss: 1.1854\n",
      "Epoch [16/200], Train Loss: 1.1841\n",
      "Validation Loss: 1.5652\n",
      "Epoch [17/200], Train Loss: 1.1759\n",
      "Epoch [18/200], Train Loss: 1.1784\n",
      "Epoch [19/200], Train Loss: 1.1682\n",
      "Epoch [20/200], Train Loss: 1.1708\n",
      "Epoch [21/200], Train Loss: 1.1680\n",
      "Validation Loss: 1.5406\n",
      "Epoch [22/200], Train Loss: 1.1569\n",
      "Epoch [23/200], Train Loss: 1.1640\n",
      "Epoch [24/200], Train Loss: 1.1524\n",
      "Epoch [25/200], Train Loss: 1.1622\n",
      "Epoch [26/200], Train Loss: 1.1624\n",
      "Validation Loss: 1.5381\n",
      "Epoch [27/200], Train Loss: 1.1519\n",
      "Epoch [28/200], Train Loss: 1.1620\n",
      "Epoch [29/200], Train Loss: 1.1546\n",
      "Epoch [30/200], Train Loss: 1.1497\n",
      "Epoch [31/200], Train Loss: 1.1533\n",
      "Validation Loss: 1.5374\n",
      "Epoch [32/200], Train Loss: 1.1391\n",
      "Epoch [33/200], Train Loss: 1.1452\n",
      "Epoch [34/200], Train Loss: 1.1472\n",
      "Epoch [35/200], Train Loss: 1.1502\n",
      "Epoch [36/200], Train Loss: 1.1216\n",
      "Validation Loss: 1.5705\n",
      "Epoch [37/200], Train Loss: 1.0931\n",
      "Epoch [38/200], Train Loss: 1.0809\n",
      "Epoch [39/200], Train Loss: 1.0841\n",
      "Epoch [40/200], Train Loss: 1.0545\n",
      "Epoch [41/200], Train Loss: 0.9593\n",
      "Validation Loss: 1.7300\n",
      "Epoch [42/200], Train Loss: 0.9180\n",
      "Epoch [43/200], Train Loss: 0.7097\n",
      "Epoch [44/200], Train Loss: 0.5033\n",
      "Epoch [45/200], Train Loss: 0.3928\n",
      "Epoch [46/200], Train Loss: 0.3504\n",
      "Validation Loss: 0.6334\n",
      "Epoch [47/200], Train Loss: 0.3282\n",
      "Epoch [48/200], Train Loss: 0.2873\n",
      "Epoch [49/200], Train Loss: 0.2664\n",
      "Epoch [50/200], Train Loss: 0.2455\n",
      "Epoch [51/200], Train Loss: 0.2227\n",
      "Validation Loss: 0.4089\n",
      "Epoch [52/200], Train Loss: 0.2011\n",
      "Epoch [53/200], Train Loss: 0.1785\n",
      "Epoch [54/200], Train Loss: 0.1574\n",
      "Epoch [55/200], Train Loss: 0.1439\n",
      "Epoch [56/200], Train Loss: 0.1485\n",
      "Validation Loss: 0.2208\n",
      "Epoch [57/200], Train Loss: 0.1520\n",
      "Epoch [58/200], Train Loss: 0.1491\n",
      "Epoch [59/200], Train Loss: 0.1382\n",
      "Epoch [60/200], Train Loss: 0.1361\n",
      "Epoch [61/200], Train Loss: 0.1322\n",
      "Validation Loss: 0.1939\n",
      "Epoch [62/200], Train Loss: 0.1295\n",
      "Epoch [63/200], Train Loss: 0.1242\n",
      "Epoch [64/200], Train Loss: 0.1279\n",
      "Epoch [65/200], Train Loss: 0.1081\n",
      "Epoch [66/200], Train Loss: 0.1186\n",
      "Validation Loss: 0.2034\n",
      "Epoch [67/200], Train Loss: 0.1292\n",
      "Epoch [68/200], Train Loss: 0.1183\n",
      "Epoch [69/200], Train Loss: 0.1101\n",
      "Epoch [70/200], Train Loss: 0.1014\n",
      "Epoch [71/200], Train Loss: 0.1007\n",
      "Validation Loss: 0.1730\n",
      "Epoch [72/200], Train Loss: 0.0975\n",
      "Epoch [73/200], Train Loss: 0.0954\n",
      "Epoch [74/200], Train Loss: 0.0911\n",
      "Epoch [75/200], Train Loss: 0.0890\n",
      "Epoch [76/200], Train Loss: 0.0886\n",
      "Validation Loss: 0.1475\n",
      "Epoch [77/200], Train Loss: 0.0881\n",
      "Epoch [78/200], Train Loss: 0.0886\n",
      "Epoch [79/200], Train Loss: 0.0845\n",
      "Epoch [80/200], Train Loss: 0.0851\n",
      "Epoch [81/200], Train Loss: 0.0833\n",
      "Validation Loss: 0.1663\n",
      "Epoch [82/200], Train Loss: 0.0866\n",
      "Epoch [83/200], Train Loss: 0.0873\n",
      "Epoch [84/200], Train Loss: 0.0892\n",
      "Epoch [85/200], Train Loss: 0.0882\n",
      "Epoch [86/200], Train Loss: 0.0878\n",
      "Validation Loss: 0.1491\n",
      "Epoch [87/200], Train Loss: 0.0907\n",
      "Epoch [88/200], Train Loss: 0.0824\n",
      "Epoch [89/200], Train Loss: 0.0838\n",
      "Epoch [90/200], Train Loss: 0.0798\n",
      "Epoch [91/200], Train Loss: 0.0803\n",
      "Validation Loss: 0.1404\n",
      "Epoch [92/200], Train Loss: 0.0744\n",
      "Epoch [93/200], Train Loss: 0.0773\n",
      "Epoch [94/200], Train Loss: 0.0806\n",
      "Epoch [95/200], Train Loss: 0.0758\n",
      "Epoch [96/200], Train Loss: 0.0771\n",
      "Validation Loss: 0.1349\n",
      "Epoch [97/200], Train Loss: 0.0745\n",
      "Epoch [98/200], Train Loss: 0.0742\n",
      "Epoch [99/200], Train Loss: 0.0740\n",
      "Epoch [100/200], Train Loss: 0.0751\n",
      "Epoch [101/200], Train Loss: 0.0734\n",
      "Validation Loss: 0.1481\n",
      "Epoch [102/200], Train Loss: 0.0806\n",
      "Epoch [103/200], Train Loss: 0.0799\n",
      "Epoch [104/200], Train Loss: 0.0738\n",
      "Epoch [105/200], Train Loss: 0.0769\n",
      "Epoch [106/200], Train Loss: 0.0776\n",
      "Validation Loss: 0.1405\n",
      "Epoch [107/200], Train Loss: 0.0791\n",
      "Epoch [108/200], Train Loss: 0.0742\n",
      "Epoch [109/200], Train Loss: 0.0753\n",
      "Epoch [110/200], Train Loss: 0.0730\n",
      "Epoch [111/200], Train Loss: 0.0758\n",
      "Validation Loss: 0.1516\n",
      "Epoch [112/200], Train Loss: 0.0776\n",
      "Epoch [113/200], Train Loss: 0.0748\n",
      "Epoch [114/200], Train Loss: 0.0727\n",
      "Epoch [115/200], Train Loss: 0.0749\n",
      "Epoch [116/200], Train Loss: 0.0723\n",
      "Validation Loss: 0.1294\n",
      "Epoch [117/200], Train Loss: 0.0709\n",
      "Epoch [118/200], Train Loss: 0.0690\n",
      "Epoch [119/200], Train Loss: 0.0707\n",
      "Epoch [120/200], Train Loss: 0.0704\n",
      "Epoch [121/200], Train Loss: 0.0681\n",
      "Validation Loss: 0.1416\n",
      "Epoch [122/200], Train Loss: 0.0685\n",
      "Epoch [123/200], Train Loss: 0.0671\n",
      "Epoch [124/200], Train Loss: 0.0682\n",
      "Epoch [125/200], Train Loss: 0.0658\n",
      "Epoch [126/200], Train Loss: 0.0705\n",
      "Validation Loss: 0.1234\n",
      "Epoch [127/200], Train Loss: 0.0663\n",
      "Epoch [128/200], Train Loss: 0.0672\n",
      "Epoch [129/200], Train Loss: 0.0678\n",
      "Epoch [130/200], Train Loss: 0.0741\n",
      "Epoch [131/200], Train Loss: 0.0788\n",
      "Validation Loss: 0.1285\n",
      "Epoch [132/200], Train Loss: 0.0721\n",
      "Epoch [133/200], Train Loss: 0.0710\n",
      "Epoch [134/200], Train Loss: 0.0695\n",
      "Epoch [135/200], Train Loss: 0.0684\n",
      "Epoch [136/200], Train Loss: 0.0650\n",
      "Validation Loss: 0.1389\n",
      "Epoch [137/200], Train Loss: 0.0650\n",
      "Epoch [138/200], Train Loss: 0.0636\n",
      "Epoch [139/200], Train Loss: 0.0613\n",
      "Epoch [140/200], Train Loss: 0.0617\n",
      "Epoch [141/200], Train Loss: 0.0635\n",
      "Validation Loss: 0.1370\n",
      "Epoch [142/200], Train Loss: 0.0650\n",
      "Epoch [143/200], Train Loss: 0.0677\n",
      "Epoch [144/200], Train Loss: 0.0656\n",
      "Epoch [145/200], Train Loss: 0.0634\n",
      "Epoch [146/200], Train Loss: 0.0646\n",
      "Validation Loss: 0.1702\n",
      "Epoch [147/200], Train Loss: 0.0639\n",
      "Epoch [148/200], Train Loss: 0.0629\n",
      "Epoch [149/200], Train Loss: 0.0642\n",
      "Epoch [150/200], Train Loss: 0.0592\n",
      "Epoch [151/200], Train Loss: 0.0598\n",
      "Early stopping after 150 epochs.\n",
      "Epoch [1/200], Train Loss: 1.8895\n",
      "Validation Loss: 2.5816\n",
      "Epoch [2/200], Train Loss: 1.8905\n",
      "Epoch [3/200], Train Loss: 1.9079\n",
      "Epoch [4/200], Train Loss: 1.8879\n",
      "Epoch [5/200], Train Loss: 1.9099\n",
      "Epoch [6/200], Train Loss: 1.8775\n",
      "Validation Loss: 2.5816\n",
      "Epoch [7/200], Train Loss: 1.8882\n",
      "Epoch [8/200], Train Loss: 1.9104\n",
      "Epoch [9/200], Train Loss: 1.8972\n",
      "Epoch [10/200], Train Loss: 1.8973\n",
      "Epoch [11/200], Train Loss: 1.9079\n",
      "Validation Loss: 2.5816\n",
      "Epoch [12/200], Train Loss: 1.9077\n",
      "Epoch [13/200], Train Loss: 1.9115\n",
      "Epoch [14/200], Train Loss: 1.8694\n",
      "Epoch [15/200], Train Loss: 1.9103\n",
      "Epoch [16/200], Train Loss: 1.9228\n",
      "Validation Loss: 2.5816\n",
      "Epoch [17/200], Train Loss: 1.8897\n",
      "Epoch [18/200], Train Loss: 1.8837\n",
      "Epoch [19/200], Train Loss: 1.9058\n",
      "Epoch [20/200], Train Loss: 1.8923\n",
      "Epoch [21/200], Train Loss: 1.8928\n",
      "Validation Loss: 2.5816\n",
      "Epoch [22/200], Train Loss: 1.9037\n",
      "Epoch [23/200], Train Loss: 1.8869\n",
      "Epoch [24/200], Train Loss: 1.9091\n",
      "Epoch [25/200], Train Loss: 1.9174\n",
      "Epoch [26/200], Train Loss: 1.8963\n",
      "Early stopping after 25 epochs.\n",
      "Epoch [1/200], Train Loss: 1.4596\n",
      "Validation Loss: 1.0266\n",
      "Epoch [2/200], Train Loss: 0.5897\n",
      "Epoch [3/200], Train Loss: 0.4326\n",
      "Epoch [4/200], Train Loss: 0.2873\n",
      "Epoch [5/200], Train Loss: 0.2549\n",
      "Epoch [6/200], Train Loss: 0.2242\n",
      "Validation Loss: 0.2879\n",
      "Epoch [7/200], Train Loss: 0.2104\n",
      "Epoch [8/200], Train Loss: 0.1853\n",
      "Epoch [9/200], Train Loss: 0.1519\n",
      "Epoch [10/200], Train Loss: 0.1188\n",
      "Epoch [11/200], Train Loss: 0.1027\n",
      "Validation Loss: 0.1238\n",
      "Epoch [12/200], Train Loss: 0.0869\n",
      "Epoch [13/200], Train Loss: 0.0825\n",
      "Epoch [14/200], Train Loss: 0.0737\n",
      "Epoch [15/200], Train Loss: 0.0687\n",
      "Epoch [16/200], Train Loss: 0.0637\n",
      "Validation Loss: 0.0825\n",
      "Epoch [17/200], Train Loss: 0.0620\n",
      "Epoch [18/200], Train Loss: 0.0579\n",
      "Epoch [19/200], Train Loss: 0.0558\n",
      "Epoch [20/200], Train Loss: 0.0589\n",
      "Epoch [21/200], Train Loss: 0.0558\n",
      "Validation Loss: 0.0702\n",
      "Epoch [22/200], Train Loss: 0.0531\n",
      "Epoch [23/200], Train Loss: 0.0542\n",
      "Epoch [24/200], Train Loss: 0.0526\n",
      "Epoch [25/200], Train Loss: 0.0480\n",
      "Epoch [26/200], Train Loss: 0.0440\n",
      "Validation Loss: 0.0601\n",
      "Epoch [27/200], Train Loss: 0.0435\n",
      "Epoch [28/200], Train Loss: 0.0431\n",
      "Epoch [29/200], Train Loss: 0.0418\n",
      "Epoch [30/200], Train Loss: 0.0393\n",
      "Epoch [31/200], Train Loss: 0.0397\n",
      "Validation Loss: 0.0500\n",
      "Epoch [32/200], Train Loss: 0.0364\n",
      "Epoch [33/200], Train Loss: 0.0364\n",
      "Epoch [34/200], Train Loss: 0.0352\n",
      "Epoch [35/200], Train Loss: 0.0346\n",
      "Epoch [36/200], Train Loss: 0.0340\n",
      "Validation Loss: 0.0468\n",
      "Epoch [37/200], Train Loss: 0.0347\n",
      "Epoch [38/200], Train Loss: 0.0339\n",
      "Epoch [39/200], Train Loss: 0.0334\n",
      "Epoch [40/200], Train Loss: 0.0321\n",
      "Epoch [41/200], Train Loss: 0.0315\n",
      "Validation Loss: 0.0471\n",
      "Epoch [42/200], Train Loss: 0.0326\n",
      "Epoch [43/200], Train Loss: 0.0372\n",
      "Epoch [44/200], Train Loss: 0.0358\n",
      "Epoch [45/200], Train Loss: 0.0340\n",
      "Epoch [46/200], Train Loss: 0.0299\n",
      "Validation Loss: 0.0478\n",
      "Epoch [47/200], Train Loss: 0.0293\n",
      "Epoch [48/200], Train Loss: 0.0278\n",
      "Epoch [49/200], Train Loss: 0.0286\n",
      "Epoch [50/200], Train Loss: 0.0285\n",
      "Epoch [51/200], Train Loss: 0.0296\n",
      "Validation Loss: 0.0449\n",
      "Epoch [52/200], Train Loss: 0.0295\n",
      "Epoch [53/200], Train Loss: 0.0282\n",
      "Epoch [54/200], Train Loss: 0.0291\n",
      "Epoch [55/200], Train Loss: 0.0303\n",
      "Epoch [56/200], Train Loss: 0.0287\n",
      "Validation Loss: 0.0431\n",
      "Epoch [57/200], Train Loss: 0.0277\n",
      "Epoch [58/200], Train Loss: 0.0268\n",
      "Epoch [59/200], Train Loss: 0.0261\n",
      "Epoch [60/200], Train Loss: 0.0268\n",
      "Epoch [61/200], Train Loss: 0.0261\n",
      "Validation Loss: 0.0424\n",
      "Epoch [62/200], Train Loss: 0.0267\n",
      "Epoch [63/200], Train Loss: 0.0273\n",
      "Epoch [64/200], Train Loss: 0.0270\n",
      "Epoch [65/200], Train Loss: 0.0276\n",
      "Epoch [66/200], Train Loss: 0.0262\n",
      "Validation Loss: 0.0416\n",
      "Epoch [67/200], Train Loss: 0.0266\n",
      "Epoch [68/200], Train Loss: 0.0280\n",
      "Epoch [69/200], Train Loss: 0.0276\n",
      "Epoch [70/200], Train Loss: 0.0272\n",
      "Epoch [71/200], Train Loss: 0.0290\n",
      "Validation Loss: 0.0470\n",
      "Epoch [72/200], Train Loss: 0.0312\n",
      "Epoch [73/200], Train Loss: 0.0339\n",
      "Epoch [74/200], Train Loss: 0.0279\n",
      "Epoch [75/200], Train Loss: 0.0267\n",
      "Epoch [76/200], Train Loss: 0.0278\n",
      "Validation Loss: 0.0438\n",
      "Epoch [77/200], Train Loss: 0.0254\n",
      "Epoch [78/200], Train Loss: 0.0244\n",
      "Epoch [79/200], Train Loss: 0.0246\n",
      "Epoch [80/200], Train Loss: 0.0244\n",
      "Epoch [81/200], Train Loss: 0.0270\n",
      "Validation Loss: 0.0367\n",
      "Epoch [82/200], Train Loss: 0.0238\n",
      "Epoch [83/200], Train Loss: 0.0238\n",
      "Epoch [84/200], Train Loss: 0.0233\n",
      "Epoch [85/200], Train Loss: 0.0236\n",
      "Epoch [86/200], Train Loss: 0.0236\n",
      "Validation Loss: 0.0359\n",
      "Epoch [87/200], Train Loss: 0.0259\n",
      "Epoch [88/200], Train Loss: 0.0277\n",
      "Epoch [89/200], Train Loss: 0.0278\n",
      "Epoch [90/200], Train Loss: 0.0258\n",
      "Epoch [91/200], Train Loss: 0.0236\n",
      "Validation Loss: 0.0374\n",
      "Epoch [92/200], Train Loss: 0.0236\n",
      "Epoch [93/200], Train Loss: 0.0233\n",
      "Epoch [94/200], Train Loss: 0.0236\n",
      "Epoch [95/200], Train Loss: 0.0245\n",
      "Epoch [96/200], Train Loss: 0.0236\n",
      "Validation Loss: 0.0386\n",
      "Epoch [97/200], Train Loss: 0.0222\n",
      "Epoch [98/200], Train Loss: 0.0251\n",
      "Epoch [99/200], Train Loss: 0.0234\n",
      "Epoch [100/200], Train Loss: 0.0236\n",
      "Epoch [101/200], Train Loss: 0.0233\n",
      "Validation Loss: 0.0404\n",
      "Epoch [102/200], Train Loss: 0.0229\n",
      "Epoch [103/200], Train Loss: 0.0251\n",
      "Epoch [104/200], Train Loss: 0.0229\n",
      "Epoch [105/200], Train Loss: 0.0231\n",
      "Epoch [106/200], Train Loss: 0.0217\n",
      "Early stopping after 105 epochs.\n",
      "Epoch [1/200], Train Loss: 1.8759\n",
      "Validation Loss: 2.5815\n",
      "Epoch [2/200], Train Loss: 1.8648\n",
      "Epoch [3/200], Train Loss: 1.9233\n",
      "Epoch [4/200], Train Loss: 1.8965\n",
      "Epoch [5/200], Train Loss: 1.9285\n",
      "Epoch [6/200], Train Loss: 1.9230\n",
      "Validation Loss: 2.5815\n",
      "Epoch [7/200], Train Loss: 1.8820\n",
      "Epoch [8/200], Train Loss: 1.8868\n",
      "Epoch [9/200], Train Loss: 1.8965\n",
      "Epoch [10/200], Train Loss: 1.9168\n",
      "Epoch [11/200], Train Loss: 1.8867\n",
      "Validation Loss: 2.5815\n",
      "Epoch [12/200], Train Loss: 1.9403\n",
      "Epoch [13/200], Train Loss: 1.8789\n",
      "Epoch [14/200], Train Loss: 1.8770\n",
      "Epoch [15/200], Train Loss: 1.9391\n",
      "Epoch [16/200], Train Loss: 1.8687\n",
      "Validation Loss: 2.5815\n",
      "Epoch [17/200], Train Loss: 1.9256\n",
      "Epoch [18/200], Train Loss: 1.8627\n",
      "Epoch [19/200], Train Loss: 1.8954\n",
      "Epoch [20/200], Train Loss: 1.8779\n",
      "Epoch [21/200], Train Loss: 1.8935\n",
      "Validation Loss: 2.5815\n",
      "Epoch [22/200], Train Loss: 1.8928\n",
      "Epoch [23/200], Train Loss: 1.9107\n",
      "Epoch [24/200], Train Loss: 1.9008\n",
      "Epoch [25/200], Train Loss: 1.8895\n",
      "Epoch [26/200], Train Loss: 1.9089\n",
      "Early stopping after 25 epochs.\n",
      "Epoch [1/200], Train Loss: 1.8320\n",
      "Validation Loss: 2.0156\n",
      "Epoch [2/200], Train Loss: 1.3287\n",
      "Epoch [3/200], Train Loss: 1.1655\n",
      "Epoch [4/200], Train Loss: 1.0902\n",
      "Epoch [5/200], Train Loss: 1.0706\n",
      "Epoch [6/200], Train Loss: 1.0350\n",
      "Validation Loss: 1.4112\n",
      "Epoch [7/200], Train Loss: 1.0464\n",
      "Epoch [8/200], Train Loss: 1.0534\n",
      "Epoch [9/200], Train Loss: 1.0255\n",
      "Epoch [10/200], Train Loss: 1.0416\n",
      "Epoch [11/200], Train Loss: 1.0124\n",
      "Validation Loss: 1.3859\n",
      "Epoch [12/200], Train Loss: 0.9977\n",
      "Epoch [13/200], Train Loss: 1.0231\n",
      "Epoch [14/200], Train Loss: 0.9672\n",
      "Epoch [15/200], Train Loss: 0.6334\n",
      "Epoch [16/200], Train Loss: 0.3807\n",
      "Validation Loss: 0.1722\n",
      "Epoch [17/200], Train Loss: 0.1557\n",
      "Epoch [18/200], Train Loss: 0.1327\n",
      "Epoch [19/200], Train Loss: 0.1074\n",
      "Epoch [20/200], Train Loss: 0.0956\n",
      "Epoch [21/200], Train Loss: 0.0894\n",
      "Validation Loss: 0.1125\n",
      "Epoch [22/200], Train Loss: 0.0850\n",
      "Epoch [23/200], Train Loss: 0.0807\n",
      "Epoch [24/200], Train Loss: 0.0811\n",
      "Epoch [25/200], Train Loss: 0.0769\n",
      "Epoch [26/200], Train Loss: 0.0768\n",
      "Validation Loss: 0.1033\n",
      "Epoch [27/200], Train Loss: 0.0765\n",
      "Epoch [28/200], Train Loss: 0.0753\n",
      "Epoch [29/200], Train Loss: 0.0730\n",
      "Epoch [30/200], Train Loss: 0.0701\n",
      "Epoch [31/200], Train Loss: 0.0692\n",
      "Validation Loss: 0.1050\n",
      "Epoch [32/200], Train Loss: 0.0685\n",
      "Epoch [33/200], Train Loss: 0.0680\n",
      "Epoch [34/200], Train Loss: 0.0656\n",
      "Epoch [35/200], Train Loss: 0.0638\n",
      "Epoch [36/200], Train Loss: 0.0626\n",
      "Validation Loss: 0.0858\n",
      "Epoch [37/200], Train Loss: 0.0608\n",
      "Epoch [38/200], Train Loss: 0.0587\n",
      "Epoch [39/200], Train Loss: 0.0585\n",
      "Epoch [40/200], Train Loss: 0.0578\n",
      "Epoch [41/200], Train Loss: 0.0550\n",
      "Validation Loss: 0.0767\n",
      "Epoch [42/200], Train Loss: 0.0538\n",
      "Epoch [43/200], Train Loss: 0.0526\n",
      "Epoch [44/200], Train Loss: 0.0519\n",
      "Epoch [45/200], Train Loss: 0.0485\n",
      "Epoch [46/200], Train Loss: 0.0486\n",
      "Validation Loss: 0.0737\n",
      "Epoch [47/200], Train Loss: 0.0484\n",
      "Epoch [48/200], Train Loss: 0.0502\n",
      "Epoch [49/200], Train Loss: 0.0480\n",
      "Epoch [50/200], Train Loss: 0.0471\n",
      "Epoch [51/200], Train Loss: 0.0451\n",
      "Validation Loss: 0.0628\n",
      "Epoch [52/200], Train Loss: 0.0460\n",
      "Epoch [53/200], Train Loss: 0.0439\n",
      "Epoch [54/200], Train Loss: 0.0433\n",
      "Epoch [55/200], Train Loss: 0.0429\n",
      "Epoch [56/200], Train Loss: 0.0443\n",
      "Validation Loss: 0.0619\n",
      "Epoch [57/200], Train Loss: 0.0423\n",
      "Epoch [58/200], Train Loss: 0.0421\n",
      "Epoch [59/200], Train Loss: 0.0419\n",
      "Epoch [60/200], Train Loss: 0.0436\n",
      "Epoch [61/200], Train Loss: 0.0416\n",
      "Validation Loss: 0.0603\n",
      "Epoch [62/200], Train Loss: 0.0412\n",
      "Epoch [63/200], Train Loss: 0.0411\n",
      "Epoch [64/200], Train Loss: 0.0408\n",
      "Epoch [65/200], Train Loss: 0.0399\n",
      "Epoch [66/200], Train Loss: 0.0417\n",
      "Validation Loss: 0.0591\n",
      "Epoch [67/200], Train Loss: 0.0414\n",
      "Epoch [68/200], Train Loss: 0.0400\n",
      "Epoch [69/200], Train Loss: 0.0404\n",
      "Epoch [70/200], Train Loss: 0.0398\n",
      "Epoch [71/200], Train Loss: 0.0385\n",
      "Validation Loss: 0.0561\n",
      "Epoch [72/200], Train Loss: 0.0394\n",
      "Epoch [73/200], Train Loss: 0.0396\n",
      "Epoch [74/200], Train Loss: 0.0394\n",
      "Epoch [75/200], Train Loss: 0.0403\n",
      "Epoch [76/200], Train Loss: 0.0376\n",
      "Validation Loss: 0.0566\n",
      "Epoch [77/200], Train Loss: 0.0373\n",
      "Epoch [78/200], Train Loss: 0.0382\n",
      "Epoch [79/200], Train Loss: 0.0406\n",
      "Epoch [80/200], Train Loss: 0.0387\n",
      "Epoch [81/200], Train Loss: 0.0372\n",
      "Validation Loss: 0.0581\n",
      "Epoch [82/200], Train Loss: 0.0365\n",
      "Epoch [83/200], Train Loss: 0.0376\n",
      "Epoch [84/200], Train Loss: 0.0372\n",
      "Epoch [85/200], Train Loss: 0.0372\n",
      "Epoch [86/200], Train Loss: 0.0390\n",
      "Validation Loss: 0.0665\n",
      "Epoch [87/200], Train Loss: 0.0385\n",
      "Epoch [88/200], Train Loss: 0.0367\n",
      "Epoch [89/200], Train Loss: 0.0406\n",
      "Epoch [90/200], Train Loss: 0.0392\n",
      "Epoch [91/200], Train Loss: 0.0399\n",
      "Validation Loss: 0.0551\n",
      "Epoch [92/200], Train Loss: 0.0393\n",
      "Epoch [93/200], Train Loss: 0.0367\n",
      "Epoch [94/200], Train Loss: 0.0349\n",
      "Epoch [95/200], Train Loss: 0.0344\n",
      "Epoch [96/200], Train Loss: 0.0336\n",
      "Validation Loss: 0.0495\n",
      "Epoch [97/200], Train Loss: 0.0345\n",
      "Epoch [98/200], Train Loss: 0.0335\n",
      "Epoch [99/200], Train Loss: 0.0337\n",
      "Epoch [100/200], Train Loss: 0.0326\n",
      "Epoch [101/200], Train Loss: 0.0315\n",
      "Validation Loss: 0.0483\n",
      "Epoch [102/200], Train Loss: 0.0328\n",
      "Epoch [103/200], Train Loss: 0.0330\n",
      "Epoch [104/200], Train Loss: 0.0333\n",
      "Epoch [105/200], Train Loss: 0.0337\n",
      "Epoch [106/200], Train Loss: 0.0316\n",
      "Validation Loss: 0.0469\n",
      "Epoch [107/200], Train Loss: 0.0320\n",
      "Epoch [108/200], Train Loss: 0.0326\n",
      "Epoch [109/200], Train Loss: 0.0356\n",
      "Epoch [110/200], Train Loss: 0.0339\n",
      "Epoch [111/200], Train Loss: 0.0306\n",
      "Validation Loss: 0.0538\n",
      "Epoch [112/200], Train Loss: 0.0311\n",
      "Epoch [113/200], Train Loss: 0.0314\n",
      "Epoch [114/200], Train Loss: 0.0311\n",
      "Epoch [115/200], Train Loss: 0.0309\n",
      "Epoch [116/200], Train Loss: 0.0311\n",
      "Validation Loss: 0.0480\n",
      "Epoch [117/200], Train Loss: 0.0307\n",
      "Epoch [118/200], Train Loss: 0.0292\n",
      "Epoch [119/200], Train Loss: 0.0288\n",
      "Epoch [120/200], Train Loss: 0.0288\n",
      "Epoch [121/200], Train Loss: 0.0300\n",
      "Validation Loss: 0.0485\n",
      "Epoch [122/200], Train Loss: 0.0297\n",
      "Epoch [123/200], Train Loss: 0.0306\n",
      "Epoch [124/200], Train Loss: 0.0303\n",
      "Epoch [125/200], Train Loss: 0.0297\n",
      "Epoch [126/200], Train Loss: 0.0290\n",
      "Validation Loss: 0.0434\n",
      "Epoch [127/200], Train Loss: 0.0284\n",
      "Epoch [128/200], Train Loss: 0.0283\n",
      "Epoch [129/200], Train Loss: 0.0288\n",
      "Epoch [130/200], Train Loss: 0.0277\n",
      "Epoch [131/200], Train Loss: 0.0296\n",
      "Validation Loss: 0.0433\n",
      "Epoch [132/200], Train Loss: 0.0295\n",
      "Epoch [133/200], Train Loss: 0.0288\n",
      "Epoch [134/200], Train Loss: 0.0306\n",
      "Epoch [135/200], Train Loss: 0.0319\n",
      "Epoch [136/200], Train Loss: 0.0291\n",
      "Validation Loss: 0.0468\n",
      "Epoch [137/200], Train Loss: 0.0284\n",
      "Epoch [138/200], Train Loss: 0.0279\n",
      "Epoch [139/200], Train Loss: 0.0299\n",
      "Epoch [140/200], Train Loss: 0.0299\n",
      "Epoch [141/200], Train Loss: 0.0317\n",
      "Validation Loss: 0.0469\n",
      "Epoch [142/200], Train Loss: 0.0329\n",
      "Epoch [143/200], Train Loss: 0.0308\n",
      "Epoch [144/200], Train Loss: 0.0277\n",
      "Epoch [145/200], Train Loss: 0.0284\n",
      "Epoch [146/200], Train Loss: 0.0271\n",
      "Validation Loss: 0.0441\n",
      "Epoch [147/200], Train Loss: 0.0275\n",
      "Epoch [148/200], Train Loss: 0.0281\n",
      "Epoch [149/200], Train Loss: 0.0276\n",
      "Epoch [150/200], Train Loss: 0.0296\n",
      "Epoch [151/200], Train Loss: 0.0300\n",
      "Early stopping after 150 epochs.\n",
      "Epoch [1/200], Train Loss: 1.7599\n",
      "Validation Loss: 2.3603\n",
      "Epoch [2/200], Train Loss: 1.7861\n",
      "Epoch [3/200], Train Loss: 1.7172\n",
      "Epoch [4/200], Train Loss: 1.7411\n",
      "Epoch [5/200], Train Loss: 1.7617\n",
      "Epoch [6/200], Train Loss: 1.7359\n",
      "Validation Loss: 2.3603\n",
      "Epoch [7/200], Train Loss: 1.7634\n",
      "Epoch [8/200], Train Loss: 1.7465\n",
      "Epoch [9/200], Train Loss: 1.7670\n",
      "Epoch [10/200], Train Loss: 1.7240\n",
      "Epoch [11/200], Train Loss: 1.7838\n",
      "Validation Loss: 2.3603\n",
      "Epoch [12/200], Train Loss: 1.7432\n",
      "Epoch [13/200], Train Loss: 1.7581\n",
      "Epoch [14/200], Train Loss: 1.7602\n",
      "Epoch [15/200], Train Loss: 1.7540\n",
      "Epoch [16/200], Train Loss: 1.7583\n",
      "Validation Loss: 2.3603\n",
      "Epoch [17/200], Train Loss: 1.7339\n",
      "Epoch [18/200], Train Loss: 1.7612\n",
      "Epoch [19/200], Train Loss: 1.7561\n",
      "Epoch [20/200], Train Loss: 1.7729\n",
      "Epoch [21/200], Train Loss: 1.7605\n",
      "Validation Loss: 2.3603\n",
      "Epoch [22/200], Train Loss: 1.7568\n",
      "Epoch [23/200], Train Loss: 1.7659\n",
      "Epoch [24/200], Train Loss: 1.7391\n",
      "Epoch [25/200], Train Loss: 1.7465\n",
      "Epoch [26/200], Train Loss: 1.7251\n",
      "Early stopping after 25 epochs.\n",
      "Epoch [1/200], Train Loss: 1.4344\n",
      "Validation Loss: 1.8863\n",
      "Epoch [2/200], Train Loss: 1.4465\n",
      "Epoch [3/200], Train Loss: 1.4181\n",
      "Epoch [4/200], Train Loss: 1.4398\n",
      "Epoch [5/200], Train Loss: 1.4290\n",
      "Epoch [6/200], Train Loss: 1.4299\n",
      "Validation Loss: 1.8863\n",
      "Epoch [7/200], Train Loss: 1.4391\n",
      "Epoch [8/200], Train Loss: 1.4451\n",
      "Epoch [9/200], Train Loss: 1.4514\n",
      "Epoch [10/200], Train Loss: 1.4417\n",
      "Epoch [11/200], Train Loss: 1.4262\n",
      "Validation Loss: 1.8863\n",
      "Epoch [12/200], Train Loss: 1.4346\n",
      "Epoch [13/200], Train Loss: 1.4178\n",
      "Epoch [14/200], Train Loss: 1.4322\n",
      "Epoch [15/200], Train Loss: 1.3969\n",
      "Epoch [16/200], Train Loss: 1.4224\n",
      "Validation Loss: 1.8863\n",
      "Epoch [17/200], Train Loss: 1.4299\n",
      "Epoch [18/200], Train Loss: 1.4095\n",
      "Epoch [19/200], Train Loss: 1.4211\n",
      "Epoch [20/200], Train Loss: 1.4294\n",
      "Epoch [21/200], Train Loss: 1.4352\n",
      "Validation Loss: 1.8863\n",
      "Epoch [22/200], Train Loss: 1.4348\n",
      "Epoch [23/200], Train Loss: 1.4236\n",
      "Epoch [24/200], Train Loss: 1.4323\n",
      "Epoch [25/200], Train Loss: 1.4298\n",
      "Epoch [26/200], Train Loss: 1.4141\n",
      "Early stopping after 25 epochs.\n",
      "Epoch [1/200], Train Loss: 1.0068\n",
      "Validation Loss: 0.9243\n",
      "Epoch [2/200], Train Loss: 0.5693\n",
      "Epoch [3/200], Train Loss: 0.5090\n",
      "Epoch [4/200], Train Loss: 0.4556\n",
      "Epoch [5/200], Train Loss: 0.3762\n",
      "Epoch [6/200], Train Loss: 0.3277\n",
      "Validation Loss: 0.4443\n",
      "Epoch [7/200], Train Loss: 0.2829\n",
      "Epoch [8/200], Train Loss: 0.2429\n",
      "Epoch [9/200], Train Loss: 0.2117\n",
      "Epoch [10/200], Train Loss: 0.1892\n",
      "Epoch [11/200], Train Loss: 0.1767\n",
      "Validation Loss: 0.2672\n",
      "Epoch [12/200], Train Loss: 0.1713\n",
      "Epoch [13/200], Train Loss: 0.1603\n",
      "Epoch [14/200], Train Loss: 0.1538\n",
      "Epoch [15/200], Train Loss: 0.1470\n",
      "Epoch [16/200], Train Loss: 0.1374\n",
      "Validation Loss: 0.2689\n",
      "Epoch [17/200], Train Loss: 0.1504\n",
      "Epoch [18/200], Train Loss: 0.1622\n",
      "Epoch [19/200], Train Loss: 0.1569\n",
      "Epoch [20/200], Train Loss: 0.1377\n",
      "Epoch [21/200], Train Loss: 0.1324\n",
      "Validation Loss: 0.2216\n",
      "Epoch [22/200], Train Loss: 0.1165\n",
      "Epoch [23/200], Train Loss: 0.1127\n",
      "Epoch [24/200], Train Loss: 0.1233\n",
      "Epoch [25/200], Train Loss: 0.1263\n",
      "Epoch [26/200], Train Loss: 0.1193\n",
      "Validation Loss: 0.1941\n",
      "Epoch [27/200], Train Loss: 0.1025\n",
      "Epoch [28/200], Train Loss: 0.1282\n",
      "Epoch [29/200], Train Loss: 0.1257\n",
      "Epoch [30/200], Train Loss: 0.1229\n",
      "Epoch [31/200], Train Loss: 0.1061\n",
      "Validation Loss: 0.2047\n",
      "Epoch [32/200], Train Loss: 0.1198\n",
      "Epoch [33/200], Train Loss: 0.1196\n",
      "Epoch [34/200], Train Loss: 0.0986\n",
      "Epoch [35/200], Train Loss: 0.0922\n",
      "Epoch [36/200], Train Loss: 0.0884\n",
      "Validation Loss: 0.1565\n",
      "Epoch [37/200], Train Loss: 0.0921\n",
      "Epoch [38/200], Train Loss: 0.0926\n",
      "Epoch [39/200], Train Loss: 0.0912\n",
      "Epoch [40/200], Train Loss: 0.0889\n",
      "Epoch [41/200], Train Loss: 0.0871\n",
      "Validation Loss: 0.1402\n",
      "Epoch [42/200], Train Loss: 0.0862\n",
      "Epoch [43/200], Train Loss: 0.0831\n",
      "Epoch [44/200], Train Loss: 0.0790\n",
      "Epoch [45/200], Train Loss: 0.0838\n",
      "Epoch [46/200], Train Loss: 0.0927\n",
      "Validation Loss: 0.1522\n",
      "Epoch [47/200], Train Loss: 0.0898\n",
      "Epoch [48/200], Train Loss: 0.0920\n",
      "Epoch [49/200], Train Loss: 0.0994\n",
      "Epoch [50/200], Train Loss: 0.0859\n",
      "Epoch [51/200], Train Loss: 0.0788\n",
      "Validation Loss: 0.1339\n",
      "Epoch [52/200], Train Loss: 0.0751\n",
      "Epoch [53/200], Train Loss: 0.0720\n",
      "Epoch [54/200], Train Loss: 0.0706\n",
      "Epoch [55/200], Train Loss: 0.0701\n",
      "Epoch [56/200], Train Loss: 0.0714\n",
      "Validation Loss: 0.1270\n",
      "Epoch [57/200], Train Loss: 0.0705\n",
      "Epoch [58/200], Train Loss: 0.0723\n",
      "Epoch [59/200], Train Loss: 0.0757\n",
      "Epoch [60/200], Train Loss: 0.0774\n",
      "Epoch [61/200], Train Loss: 0.0761\n",
      "Validation Loss: 0.1486\n",
      "Epoch [62/200], Train Loss: 0.0825\n",
      "Epoch [63/200], Train Loss: 0.0853\n",
      "Epoch [64/200], Train Loss: 0.0837\n",
      "Epoch [65/200], Train Loss: 0.0785\n",
      "Epoch [66/200], Train Loss: 0.0779\n",
      "Validation Loss: 0.1254\n",
      "Epoch [67/200], Train Loss: 0.0773\n",
      "Epoch [68/200], Train Loss: 0.0801\n",
      "Epoch [69/200], Train Loss: 0.0788\n",
      "Epoch [70/200], Train Loss: 0.0777\n",
      "Epoch [71/200], Train Loss: 0.0764\n",
      "Validation Loss: 0.1377\n",
      "Epoch [72/200], Train Loss: 0.0697\n",
      "Epoch [73/200], Train Loss: 0.0697\n",
      "Epoch [74/200], Train Loss: 0.0719\n",
      "Epoch [75/200], Train Loss: 0.0698\n",
      "Epoch [76/200], Train Loss: 0.0705\n",
      "Validation Loss: 0.1386\n",
      "Epoch [77/200], Train Loss: 0.0723\n",
      "Epoch [78/200], Train Loss: 0.0728\n",
      "Epoch [79/200], Train Loss: 0.0760\n",
      "Epoch [80/200], Train Loss: 0.0742\n",
      "Epoch [81/200], Train Loss: 0.0722\n",
      "Validation Loss: 0.1336\n",
      "Epoch [82/200], Train Loss: 0.0778\n",
      "Epoch [83/200], Train Loss: 0.0701\n",
      "Epoch [84/200], Train Loss: 0.0723\n",
      "Epoch [85/200], Train Loss: 0.0710\n",
      "Epoch [86/200], Train Loss: 0.0654\n",
      "Validation Loss: 0.1513\n",
      "Epoch [87/200], Train Loss: 0.0695\n",
      "Epoch [88/200], Train Loss: 0.0707\n",
      "Epoch [89/200], Train Loss: 0.0681\n",
      "Epoch [90/200], Train Loss: 0.0618\n",
      "Epoch [91/200], Train Loss: 0.0660\n",
      "Validation Loss: 0.1073\n",
      "Epoch [92/200], Train Loss: 0.0683\n",
      "Epoch [93/200], Train Loss: 0.0761\n",
      "Epoch [94/200], Train Loss: 0.0688\n",
      "Epoch [95/200], Train Loss: 0.0705\n",
      "Epoch [96/200], Train Loss: 0.0752\n",
      "Validation Loss: 0.1412\n",
      "Epoch [97/200], Train Loss: 0.0765\n",
      "Epoch [98/200], Train Loss: 0.0685\n",
      "Epoch [99/200], Train Loss: 0.0686\n",
      "Epoch [100/200], Train Loss: 0.0777\n",
      "Epoch [101/200], Train Loss: 0.0729\n",
      "Validation Loss: 0.1501\n",
      "Epoch [102/200], Train Loss: 0.0852\n",
      "Epoch [103/200], Train Loss: 0.0808\n",
      "Epoch [104/200], Train Loss: 0.0705\n",
      "Epoch [105/200], Train Loss: 0.0648\n",
      "Epoch [106/200], Train Loss: 0.0670\n",
      "Validation Loss: 0.1248\n",
      "Epoch [107/200], Train Loss: 0.0681\n",
      "Epoch [108/200], Train Loss: 0.0750\n",
      "Epoch [109/200], Train Loss: 0.0891\n",
      "Epoch [110/200], Train Loss: 0.0752\n",
      "Epoch [111/200], Train Loss: 0.0709\n",
      "Validation Loss: 0.1713\n",
      "Epoch [112/200], Train Loss: 0.0739\n",
      "Epoch [113/200], Train Loss: 0.0667\n",
      "Epoch [114/200], Train Loss: 0.0684\n",
      "Epoch [115/200], Train Loss: 0.0680\n",
      "Epoch [116/200], Train Loss: 0.0665\n",
      "Early stopping after 115 epochs.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_loss_npp_true' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Run and save the pipeline data\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m loss_vs_sigma_data \u001b[38;5;241m=\u001b[39m \u001b[43mrun_and_save_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_channel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_every_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Plot and save the plot using the saved data\u001b[39;00m\n\u001b[1;32m     16\u001b[0m plot_and_save(loss_vs_sigma_data, sigmas, dataset, learning_rate)\n",
      "Cell \u001b[0;32mIn[14], line 80\u001b[0m, in \u001b[0;36mrun_and_save_pipeline\u001b[0;34m(input_channel, train_dataloader, val_dataloader, num_epochs, val_every_epoch, learning_rate, device)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_and_save_pipeline\u001b[39m(input_channel, train_dataloader, val_dataloader, num_epochs, val_every_epoch, learning_rate, device):\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;66;03m# Run the pipeline\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m     test_loss_npp_true, test_loss_npp_false\u001b[38;5;241m=\u001b[39m \u001b[43mrun_pipeline_ci\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_channel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_every_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;66;03m# Save the data\u001b[39;00m\n\u001b[1;32m     83\u001b[0m     save_data(test_loss_npp_true, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_loss_npp_true.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[14], line 68\u001b[0m, in \u001b[0;36mrun_pipeline_ci\u001b[0;34m(input_channel, train_dataloader, val_dataloader, num_epochs, val_every_epoch, learning_rate, device, num_runs)\u001b[0m\n\u001b[1;32m     65\u001b[0m         test_losses_vs_sigma_npp_true\u001b[38;5;241m.\u001b[39mappend(test_loss)\n\u001b[1;32m     67\u001b[0m     test_losses_npp_true\u001b[38;5;241m.\u001b[39mappend(test_losses_npp_true)\n\u001b[0;32m---> 68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtest_loss_npp_true\u001b[49m, test_loss_npp_false_runs\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_loss_npp_true' is not defined"
     ]
    }
   ],
   "source": [
    "# Set your hyperparameters\n",
    "dataset = \"MNIST\"\n",
    "input_channel = 1 if dataset == \"MNIST\" else 3\n",
    "num_epochs = 200\n",
    "sigmas = [0.1, 0.2, 0.5, 1, 2, 3]  # Set the sigma values you want to test\n",
    "num_kernels_encoder = [16, 8]\n",
    "num_kernels_decoder = [16]\n",
    "learning_rate = 0.03\n",
    "val_every_epoch = 5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Run and save the pipeline data\n",
    "loss_vs_sigma_data = run_and_save_pipeline(input_channel, train_dataloader, val_dataloader, num_epochs, val_every_epoch, learning_rate, device)\n",
    "\n",
    "# Plot and save the plot using the saved data\n",
    "plot_and_save(loss_vs_sigma_data, sigmas, dataset, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527b04d7-2308-48e9-9a75-d8071dd3d0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss_npp_true, test_loss_npp_false= loss_vs_sigma_data\n",
    "test_loss_npp_false = [test_loss_npp_false for i in range((len(sigmas)))]\n",
    "test_loss_npp_true,test_loss_npp_false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8bf364-d010-40de-ad6c-ce837b5dee2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_and_save(loss_vs_sigma_data, sigmas, dataset, learning_rate, model_name=\"Auto encoder\"):\n",
    "    # Unpack the data\n",
    "    test_loss_npp_true, test_loss_npp_false = loss_vs_sigma_data\n",
    "    test_loss_npp_false = [test_loss_npp_false for i in range(len(sigmas))]\n",
    "\n",
    "    # Calculate mean and confidence intervals for NPP=True runs\n",
    "    mean_test_loss_npp_true = np.mean(test_loss_npp_true, axis=0)\n",
    "    ci_test_loss_npp_true = 1.96 * np.std(test_loss_npp_true, axis=0) / np.sqrt(len(test_loss_npp_true))\n",
    "\n",
    "    # Duplicate NPP=False values for plotting\n",
    "    mean_test_loss_npp_false = np.mean(test_loss_npp_false, axis=1)\n",
    "    ci_test_loss_npp_false = 1.96 * np.std(test_loss_npp_false, axis=1) / np.sqrt(len(test_loss_npp_false))\n",
    "\n",
    "    # Plot mean and confidence intervals for NPP=True\n",
    "    plt.plot(sigmas, mean_test_loss_npp_true, marker='o', label='NPP=True', color='blue')\n",
    "\n",
    "    # Plot mean and confidence intervals for duplicated NPP=False\n",
    "    plt.plot(sigmas, mean_test_loss_npp_false, marker='o', color='red', linestyle='--', label='NPP=False')\n",
    "\n",
    "    # Fill between for NPP=True with blue color\n",
    "    plt.fill_between(sigmas, mean_test_loss_npp_true - ci_test_loss_npp_true, mean_test_loss_npp_true + ci_test_loss_npp_true, color='blue', alpha=0.2)\n",
    "\n",
    "    # Fill between for NPP=False with red color\n",
    "    plt.fill_between(sigmas, mean_test_loss_npp_false - ci_test_loss_npp_false, mean_test_loss_npp_false + ci_test_loss_npp_false, color='red', alpha=0.2)\n",
    "\n",
    "    plt.xlabel('Sigma')\n",
    "    plt.ylabel('Test Loss')\n",
    "    plt.title(f'Test Loss vs. Sigma:{dataset} dataset with {model_name}')\n",
    "    plt.legend()\n",
    "\n",
    "    # Create a directory to save the results if it doesn't exist\n",
    "    results_dir = './results'\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "    # Generate a filename based on parameters in the title\n",
    "    filename = f\"test_loss_vs_sigma_{dataset}_{model_name}_lr_{learning_rate}.png\"\n",
    "    filepath = os.path.join(results_dir, filename)\n",
    "\n",
    "    # Save the plot\n",
    "    plt.savefig(filepath)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# plot_and_save(loss_vs_sigma_data, sigmas, dataset, learning_rate)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "# Plot and save the plot using the saved data\n",
    "plot_and_save(loss_vs_sigma_data, sigmas, dataset, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26a5d16-5dbd-4a42-917e-72b80b9fd9b2",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e275dea-f73a-4c94-a6dd-f2b5def8f4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(x, p, y, outputs, r):\n",
    "    plt.figure(figsize=(14, 14))  # Adjust the figure size\n",
    "    n_shows = 4\n",
    "\n",
    "    for i in range(n_shows):\n",
    "        # Original Images\n",
    "        img = x[i].squeeze().detach().cpu() / 255\n",
    "\n",
    "        plt.subplot(4, 4, 4*i+1 )\n",
    "        plt.imshow(img)\n",
    "\n",
    "        plt.subplot(4, 4, 4*i + 2)  # Transposed the rows and columns\n",
    "        count_all_image = plot_all(img, r=r)\n",
    "        plt.imshow(count_all_image)\n",
    "\n",
    "        # Reconstructed Images (switched with row 3)\n",
    "        plt.subplot(4, 4, 4*i + 3)  # Transposed the rows and columns\n",
    "        count_image = plot_label_pin(img, p[i], y[i])\n",
    "        plt.imshow(count_image)\n",
    "\n",
    "        plt.subplot(4, 4, 4*i + 4)\n",
    "        plt.imshow(outputs[i].squeeze().detach().cpu())\n",
    "\n",
    "    # Add an overall color bar\n",
    "    plt.subplots_adjust(bottom=0.2, hspace=0.4)  # Increase the vertical spacing\n",
    "    cbar_ax = plt.gcf().add_axes([0.15, 0.1, 0.7, 0.03])  # Define the position and size of the color bar\n",
    "    cbar = plt.colorbar(cax=cbar_ax, orientation='horizontal')\n",
    "\n",
    "    # Add titles in the middle of the entire row\n",
    "    plt.subplot(4, 4, 1).set_title(\"images\", position=(0.5, 1.05))\n",
    "    plt.subplot(4, 4, 2).set_title(\"Label all map\", position=(0.5, 1.05))\n",
    "    plt.subplot(4, 4, 3).set_title(\"Label pins\", position=(0.5, 1.05))\n",
    "    plt.subplot(4, 4, 4).set_title(\"Predicted\", position=(0.5, 1.05))\n",
    "\n",
    "    # Define the output folder based on the dataset\n",
    "    output_folder = f\"results/{dataset}\"\n",
    "    os.makedirs(output_folder, exist_ok=True)  # Create the folder if it doesn't exist\n",
    "\n",
    "    # Define the image filename based on parameters\n",
    "    image_filename = f\"NPP{NPP}_LR={learning_rate}_n={n}_mesh={mesh}_d={d}_n_pins={n_pins}_fixedpins={fixed_pins}_r={r}.png\"\n",
    "\n",
    "    # Save the figure as a high-resolution PNG in the specified folder\n",
    "    plt.savefig(os.path.join(output_folder, image_filename), dpi=100)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd6564e-cd65-4496-ba82-4784f2cdc03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_samples(dataloader, autoencoder):\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            x = batch['image'][:, :input_channel, :, :].to(device)\n",
    "            p = [tensor.to(device) for tensor in batch['pins']]\n",
    "            y = [tensor.to(device) for tensor in batch['outputs']]\n",
    "\n",
    "            outputs = autoencoder(x.float())\n",
    "            break\n",
    "    plot_results(x, p, y, outputs, r=r)\n",
    "    \n",
    "\n",
    "# visualize_samples(test_dataloader, autoencoder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pycox",
   "language": "python",
   "name": "pycox"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
