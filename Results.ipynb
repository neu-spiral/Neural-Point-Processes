{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7be8cd88-49d4-4db2-9a32-6f68fab28150",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4e8794b-47eb-4e9a-ade6-606e891c779d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_experiment_data(base_directory):\n",
    "    combined_data = []\n",
    "    \n",
    "    # Loop through each experiment directory\n",
    "    for experiment_id in os.listdir(base_directory):\n",
    "        experiment_path = os.path.join(base_directory, experiment_id)\n",
    "        \n",
    "        if os.path.isdir(experiment_path):\n",
    "            # Paths to the expected config.json and results.txt\n",
    "            config_path = os.path.join(experiment_path, 'config.json')\n",
    "            results_path = os.path.join(experiment_path, 'results.txt')\n",
    "            \n",
    "            # Skip this folder if config.json or results.txt does not exist\n",
    "            if not os.path.exists(config_path) or not os.path.exists(results_path):\n",
    "                continue  # Skip this folder and move to the next one\n",
    "            \n",
    "            # Read config.json\n",
    "            with open(config_path, 'r') as config_file:\n",
    "                config_data = json.load(config_file)\n",
    "            \n",
    "            # Read results.txt\n",
    "            with open(results_path, 'r') as results_file:\n",
    "                results_lines = results_file.readlines()\n",
    "\n",
    "                # Extract Best Val (checking if the line contains \"Best Val\")\n",
    "                if len(results_lines) > 0 and \"Best Val\" in results_lines[0]:\n",
    "                    best_val = float(results_lines[0].split(': ')[-1])\n",
    "                else:\n",
    "                    continue  # Skip this file if the format isn't correct\n",
    "                \n",
    "                # Check if Percent is present in the next lines\n",
    "                if len(results_lines) > 1 and \"Percent\" in results_lines[1]:\n",
    "                    # print(\"NPP data found\")\n",
    "                    # Handle the case with Percent entries\n",
    "                    for i in range(1, len(results_lines)):  # Start from line 2 for the Percent-based entries\n",
    "                        line_parts = results_lines[i].strip().split('|')\n",
    "\n",
    "                        # print(f\"Line Parts: {line_parts}\")  # Print the raw split parts of the line\n",
    "\n",
    "                        try:\n",
    "                            percent = float(line_parts[0].split(': ')[1])\n",
    "            \n",
    "                            # We split line_parts[1] by commas first, then extract Loss and R2\n",
    "                            loss_and_r2_parts = line_parts[1].split(',')\n",
    "\n",
    "                            # Extract loss and R2 using further splitting\n",
    "                            loss = float(loss_and_r2_parts[0].split(': ')[1])\n",
    "                            r2 = float(loss_and_r2_parts[1].split(': ')[1])\n",
    "                        except (IndexError, ValueError) as e:\n",
    "                            # print(f\"Error parsing line: {e}\")\n",
    "                            continue  # Skip if the line doesn't follow the expected format\n",
    "\n",
    "                        # Print the extracted values for debugging\n",
    "                        # print(f\"Percent: {percent}, Loss: {loss}, R2: {r2}\")\n",
    "                        experiment_data = {\n",
    "                            'experiment_id': experiment_id,\n",
    "                            'experiment_name': config_data.get('experiment_name'),\n",
    "                            'feature': config_data.get('feature'),\n",
    "                            'lr': config_data.get('lr'),\n",
    "                            'kernel': config_data.get('kernel'),\n",
    "                            'kernel_mode': config_data.get('kernel_mode'),\n",
    "                            'kernel_param': config_data.get('kernel_param'),\n",
    "                            'Best Val': best_val,\n",
    "                            'Percent': percent,\n",
    "                            'Loss': loss,\n",
    "                            'R2': r2\n",
    "                        }\n",
    "                        \n",
    "                        combined_data.append(experiment_data)\n",
    "                else:\n",
    "                    # Handle the case without Percent entries (single MSE and R2 result)\n",
    "                    try:\n",
    "                        mse_line = results_lines[1] if len(results_lines) > 1 else None\n",
    "                        if mse_line and \"MSE\" in mse_line and \"R2\" in mse_line:\n",
    "                            mse = float(mse_line.split('MSE: ')[1].split(',')[0])\n",
    "                            r2 = float(mse_line.split('R2: ')[1])\n",
    "                            \n",
    "                            # Create a single entry for this result\n",
    "                            experiment_data = {\n",
    "                                'experiment_id': experiment_id,\n",
    "                                'experiment_name': config_data.get('experiment_name'),\n",
    "                                'feature': config_data.get('feature'),\n",
    "                                'lr': config_data.get('lr'),\n",
    "                                'kernel': config_data.get('kernel'),\n",
    "                                'kernel_mode': config_data.get('kernel_mode'),\n",
    "                                'kernel_param': config_data.get('kernel_param'),\n",
    "                                'Best Val': best_val,\n",
    "                                'Percent': 0,  # No Percent data\n",
    "                                'Loss': mse,  # Assuming MSE is Loss here\n",
    "                                'R2': r2\n",
    "                            }\n",
    "                            \n",
    "                            combined_data.append(experiment_data)\n",
    "                    except (IndexError, ValueError):\n",
    "                        continue  # Skip if the line doesn't follow the expected format\n",
    "    \n",
    "    # Convert the combined data into a DataFrame\n",
    "    df = pd.DataFrame(combined_data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f84c154a-c656-4ac7-a056-bc4c3dcf22b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Base directory where all the experiment folders are located\n",
    "# base_directory = './history/PinMNIST_mesh_10'\n",
    "# base_directory = './history/Building_random_100'\n",
    "base_directory = '/work/DNAL/sirera.m/Satellite_Fusion/history/Synthetic_random_100'\n",
    "\n",
    "\n",
    "# Generate the combined dataframe\n",
    "experiment_df = read_experiment_data(base_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8a17a54d-d2da-4de1-ba5d-69ebb783f4d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4355/2618897621.py:37: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  final_df = final_df.applymap(lambda x: f\"{x:.3f}\" if isinstance(x, (float, int)) else x)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_86ab3_row0_col0, #T_86ab3_row0_col1, #T_86ab3_row0_col2, #T_86ab3_row0_col3, #T_86ab3_row0_col4, #T_86ab3_row0_col5, #T_86ab3_row0_col6, #T_86ab3_row0_col7, #T_86ab3_row0_col8, #T_86ab3_row0_col9, #T_86ab3_row0_col10, #T_86ab3_row1_col0, #T_86ab3_row1_col1, #T_86ab3_row1_col2, #T_86ab3_row1_col3, #T_86ab3_row1_col4, #T_86ab3_row1_col5, #T_86ab3_row1_col6, #T_86ab3_row1_col7, #T_86ab3_row1_col8, #T_86ab3_row1_col9, #T_86ab3_row1_col10 {\n",
       "  background-color: lightblue;\n",
       "}\n",
       "#T_86ab3_row2_col0, #T_86ab3_row2_col1, #T_86ab3_row2_col2, #T_86ab3_row2_col3, #T_86ab3_row2_col4, #T_86ab3_row2_col5, #T_86ab3_row2_col6, #T_86ab3_row2_col7, #T_86ab3_row2_col8, #T_86ab3_row2_col9, #T_86ab3_row2_col10, #T_86ab3_row3_col0, #T_86ab3_row3_col1, #T_86ab3_row3_col2, #T_86ab3_row3_col3, #T_86ab3_row3_col4, #T_86ab3_row3_col5, #T_86ab3_row3_col6, #T_86ab3_row3_col7, #T_86ab3_row3_col8, #T_86ab3_row3_col9, #T_86ab3_row3_col10, #T_86ab3_row4_col0, #T_86ab3_row4_col1, #T_86ab3_row4_col2, #T_86ab3_row4_col3, #T_86ab3_row4_col4, #T_86ab3_row4_col5, #T_86ab3_row4_col6, #T_86ab3_row4_col7, #T_86ab3_row4_col8, #T_86ab3_row4_col9, #T_86ab3_row4_col10, #T_86ab3_row5_col0, #T_86ab3_row5_col1, #T_86ab3_row5_col2, #T_86ab3_row5_col3, #T_86ab3_row5_col4, #T_86ab3_row5_col5, #T_86ab3_row5_col6, #T_86ab3_row5_col7, #T_86ab3_row5_col8, #T_86ab3_row5_col9, #T_86ab3_row5_col10, #T_86ab3_row6_col0, #T_86ab3_row6_col1, #T_86ab3_row6_col2, #T_86ab3_row6_col3, #T_86ab3_row6_col4, #T_86ab3_row6_col5, #T_86ab3_row6_col6, #T_86ab3_row6_col7, #T_86ab3_row6_col8, #T_86ab3_row6_col9, #T_86ab3_row6_col10, #T_86ab3_row12_col0, #T_86ab3_row12_col1, #T_86ab3_row12_col2, #T_86ab3_row12_col3, #T_86ab3_row12_col4, #T_86ab3_row12_col5, #T_86ab3_row12_col6, #T_86ab3_row12_col7, #T_86ab3_row12_col8, #T_86ab3_row12_col9, #T_86ab3_row12_col10, #T_86ab3_row13_col0, #T_86ab3_row13_col1, #T_86ab3_row13_col2, #T_86ab3_row13_col3, #T_86ab3_row13_col4, #T_86ab3_row13_col5, #T_86ab3_row13_col6, #T_86ab3_row13_col7, #T_86ab3_row13_col8, #T_86ab3_row13_col9, #T_86ab3_row13_col10, #T_86ab3_row14_col0, #T_86ab3_row14_col1, #T_86ab3_row14_col2, #T_86ab3_row14_col3, #T_86ab3_row14_col4, #T_86ab3_row14_col5, #T_86ab3_row14_col6, #T_86ab3_row14_col7, #T_86ab3_row14_col8, #T_86ab3_row14_col9, #T_86ab3_row14_col10, #T_86ab3_row15_col0, #T_86ab3_row15_col1, #T_86ab3_row15_col2, #T_86ab3_row15_col3, #T_86ab3_row15_col4, #T_86ab3_row15_col5, #T_86ab3_row15_col6, #T_86ab3_row15_col7, #T_86ab3_row15_col8, #T_86ab3_row15_col9, #T_86ab3_row15_col10, #T_86ab3_row16_col0, #T_86ab3_row16_col1, #T_86ab3_row16_col2, #T_86ab3_row16_col3, #T_86ab3_row16_col4, #T_86ab3_row16_col5, #T_86ab3_row16_col6, #T_86ab3_row16_col7, #T_86ab3_row16_col8, #T_86ab3_row16_col9, #T_86ab3_row16_col10 {\n",
       "  background-color: lightgreen;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_86ab3\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_86ab3_level0_col0\" class=\"col_heading level0 col0\" >experiment_id</th>\n",
       "      <th id=\"T_86ab3_level0_col1\" class=\"col_heading level0 col1\" >experiment_name</th>\n",
       "      <th id=\"T_86ab3_level0_col2\" class=\"col_heading level0 col2\" >feature</th>\n",
       "      <th id=\"T_86ab3_level0_col3\" class=\"col_heading level0 col3\" >lr</th>\n",
       "      <th id=\"T_86ab3_level0_col4\" class=\"col_heading level0 col4\" >kernel</th>\n",
       "      <th id=\"T_86ab3_level0_col5\" class=\"col_heading level0 col5\" >kernel_mode</th>\n",
       "      <th id=\"T_86ab3_level0_col6\" class=\"col_heading level0 col6\" >kernel_param</th>\n",
       "      <th id=\"T_86ab3_level0_col7\" class=\"col_heading level0 col7\" >Best Val</th>\n",
       "      <th id=\"T_86ab3_level0_col8\" class=\"col_heading level0 col8\" >Percent</th>\n",
       "      <th id=\"T_86ab3_level0_col9\" class=\"col_heading level0 col9\" >Loss</th>\n",
       "      <th id=\"T_86ab3_level0_col10\" class=\"col_heading level0 col10\" >R2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_86ab3_level0_row0\" class=\"row_heading level0 row0\" >4</th>\n",
       "      <td id=\"T_86ab3_row0_col0\" class=\"data row0 col0\" >1726940510</td>\n",
       "      <td id=\"T_86ab3_row0_col1\" class=\"data row0 col1\" >Synthetic_random_100</td>\n",
       "      <td id=\"T_86ab3_row0_col2\" class=\"data row0 col2\" >AE</td>\n",
       "      <td id=\"T_86ab3_row0_col3\" class=\"data row0 col3\" >0.001</td>\n",
       "      <td id=\"T_86ab3_row0_col4\" class=\"data row0 col4\" >RBF</td>\n",
       "      <td id=\"T_86ab3_row0_col5\" class=\"data row0 col5\" >learned</td>\n",
       "      <td id=\"T_86ab3_row0_col6\" class=\"data row0 col6\" >0.000</td>\n",
       "      <td id=\"T_86ab3_row0_col7\" class=\"data row0 col7\" >26.730</td>\n",
       "      <td id=\"T_86ab3_row0_col8\" class=\"data row0 col8\" >0.000</td>\n",
       "      <td id=\"T_86ab3_row0_col9\" class=\"data row0 col9\" >27.959</td>\n",
       "      <td id=\"T_86ab3_row0_col10\" class=\"data row0 col10\" >0.291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_86ab3_level0_row1\" class=\"row_heading level0 row1\" >410</th>\n",
       "      <td id=\"T_86ab3_row1_col0\" class=\"data row1 col0\" >1727172836</td>\n",
       "      <td id=\"T_86ab3_row1_col1\" class=\"data row1 col1\" >Synthetic_random_100</td>\n",
       "      <td id=\"T_86ab3_row1_col2\" class=\"data row1 col2\" >DDPM</td>\n",
       "      <td id=\"T_86ab3_row1_col3\" class=\"data row1 col3\" >0.010</td>\n",
       "      <td id=\"T_86ab3_row1_col4\" class=\"data row1 col4\" >RBF</td>\n",
       "      <td id=\"T_86ab3_row1_col5\" class=\"data row1 col5\" >predicted</td>\n",
       "      <td id=\"T_86ab3_row1_col6\" class=\"data row1 col6\" >0.000</td>\n",
       "      <td id=\"T_86ab3_row1_col7\" class=\"data row1 col7\" >26.268</td>\n",
       "      <td id=\"T_86ab3_row1_col8\" class=\"data row1 col8\" >0.000</td>\n",
       "      <td id=\"T_86ab3_row1_col9\" class=\"data row1 col9\" >27.507</td>\n",
       "      <td id=\"T_86ab3_row1_col10\" class=\"data row1 col10\" >0.298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_86ab3_level0_row2\" class=\"row_heading level0 row2\" >179</th>\n",
       "      <td id=\"T_86ab3_row2_col0\" class=\"data row2 col0\" >1727000386</td>\n",
       "      <td id=\"T_86ab3_row2_col1\" class=\"data row2 col1\" >Synthetic_random_100</td>\n",
       "      <td id=\"T_86ab3_row2_col2\" class=\"data row2 col2\" >AE</td>\n",
       "      <td id=\"T_86ab3_row2_col3\" class=\"data row2 col3\" >0.001</td>\n",
       "      <td id=\"T_86ab3_row2_col4\" class=\"data row2 col4\" >RBF</td>\n",
       "      <td id=\"T_86ab3_row2_col5\" class=\"data row2 col5\" >predicted</td>\n",
       "      <td id=\"T_86ab3_row2_col6\" class=\"data row2 col6\" >1.000</td>\n",
       "      <td id=\"T_86ab3_row2_col7\" class=\"data row2 col7\" >26.244</td>\n",
       "      <td id=\"T_86ab3_row2_col8\" class=\"data row2 col8\" >0.000</td>\n",
       "      <td id=\"T_86ab3_row2_col9\" class=\"data row2 col9\" >27.743</td>\n",
       "      <td id=\"T_86ab3_row2_col10\" class=\"data row2 col10\" >0.295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_86ab3_level0_row3\" class=\"row_heading level0 row3\" >180</th>\n",
       "      <td id=\"T_86ab3_row3_col0\" class=\"data row3 col0\" >1727000386</td>\n",
       "      <td id=\"T_86ab3_row3_col1\" class=\"data row3 col1\" >Synthetic_random_100</td>\n",
       "      <td id=\"T_86ab3_row3_col2\" class=\"data row3 col2\" >AE</td>\n",
       "      <td id=\"T_86ab3_row3_col3\" class=\"data row3 col3\" >0.001</td>\n",
       "      <td id=\"T_86ab3_row3_col4\" class=\"data row3 col4\" >RBF</td>\n",
       "      <td id=\"T_86ab3_row3_col5\" class=\"data row3 col5\" >predicted</td>\n",
       "      <td id=\"T_86ab3_row3_col6\" class=\"data row3 col6\" >1.000</td>\n",
       "      <td id=\"T_86ab3_row3_col7\" class=\"data row3 col7\" >26.244</td>\n",
       "      <td id=\"T_86ab3_row3_col8\" class=\"data row3 col8\" >0.250</td>\n",
       "      <td id=\"T_86ab3_row3_col9\" class=\"data row3 col9\" >27.700</td>\n",
       "      <td id=\"T_86ab3_row3_col10\" class=\"data row3 col10\" >0.327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_86ab3_level0_row4\" class=\"row_heading level0 row4\" >181</th>\n",
       "      <td id=\"T_86ab3_row4_col0\" class=\"data row4 col0\" >1727000386</td>\n",
       "      <td id=\"T_86ab3_row4_col1\" class=\"data row4 col1\" >Synthetic_random_100</td>\n",
       "      <td id=\"T_86ab3_row4_col2\" class=\"data row4 col2\" >AE</td>\n",
       "      <td id=\"T_86ab3_row4_col3\" class=\"data row4 col3\" >0.001</td>\n",
       "      <td id=\"T_86ab3_row4_col4\" class=\"data row4 col4\" >RBF</td>\n",
       "      <td id=\"T_86ab3_row4_col5\" class=\"data row4 col5\" >predicted</td>\n",
       "      <td id=\"T_86ab3_row4_col6\" class=\"data row4 col6\" >1.000</td>\n",
       "      <td id=\"T_86ab3_row4_col7\" class=\"data row4 col7\" >26.244</td>\n",
       "      <td id=\"T_86ab3_row4_col8\" class=\"data row4 col8\" >0.500</td>\n",
       "      <td id=\"T_86ab3_row4_col9\" class=\"data row4 col9\" >27.932</td>\n",
       "      <td id=\"T_86ab3_row4_col10\" class=\"data row4 col10\" >0.350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_86ab3_level0_row5\" class=\"row_heading level0 row5\" >182</th>\n",
       "      <td id=\"T_86ab3_row5_col0\" class=\"data row5 col0\" >1727000386</td>\n",
       "      <td id=\"T_86ab3_row5_col1\" class=\"data row5 col1\" >Synthetic_random_100</td>\n",
       "      <td id=\"T_86ab3_row5_col2\" class=\"data row5 col2\" >AE</td>\n",
       "      <td id=\"T_86ab3_row5_col3\" class=\"data row5 col3\" >0.001</td>\n",
       "      <td id=\"T_86ab3_row5_col4\" class=\"data row5 col4\" >RBF</td>\n",
       "      <td id=\"T_86ab3_row5_col5\" class=\"data row5 col5\" >predicted</td>\n",
       "      <td id=\"T_86ab3_row5_col6\" class=\"data row5 col6\" >1.000</td>\n",
       "      <td id=\"T_86ab3_row5_col7\" class=\"data row5 col7\" >26.244</td>\n",
       "      <td id=\"T_86ab3_row5_col8\" class=\"data row5 col8\" >0.750</td>\n",
       "      <td id=\"T_86ab3_row5_col9\" class=\"data row5 col9\" >27.834</td>\n",
       "      <td id=\"T_86ab3_row5_col10\" class=\"data row5 col10\" >0.380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_86ab3_level0_row6\" class=\"row_heading level0 row6\" >183</th>\n",
       "      <td id=\"T_86ab3_row6_col0\" class=\"data row6 col0\" >1727000386</td>\n",
       "      <td id=\"T_86ab3_row6_col1\" class=\"data row6 col1\" >Synthetic_random_100</td>\n",
       "      <td id=\"T_86ab3_row6_col2\" class=\"data row6 col2\" >AE</td>\n",
       "      <td id=\"T_86ab3_row6_col3\" class=\"data row6 col3\" >0.001</td>\n",
       "      <td id=\"T_86ab3_row6_col4\" class=\"data row6 col4\" >RBF</td>\n",
       "      <td id=\"T_86ab3_row6_col5\" class=\"data row6 col5\" >predicted</td>\n",
       "      <td id=\"T_86ab3_row6_col6\" class=\"data row6 col6\" >1.000</td>\n",
       "      <td id=\"T_86ab3_row6_col7\" class=\"data row6 col7\" >26.244</td>\n",
       "      <td id=\"T_86ab3_row6_col8\" class=\"data row6 col8\" >1.000</td>\n",
       "      <td id=\"T_86ab3_row6_col9\" class=\"data row6 col9\" >28.137</td>\n",
       "      <td id=\"T_86ab3_row6_col10\" class=\"data row6 col10\" >0.390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_86ab3_level0_row7\" class=\"row_heading level0 row7\" >344</th>\n",
       "      <td id=\"T_86ab3_row7_col0\" class=\"data row7 col0\" >1727065255</td>\n",
       "      <td id=\"T_86ab3_row7_col1\" class=\"data row7 col1\" >Synthetic_random_100</td>\n",
       "      <td id=\"T_86ab3_row7_col2\" class=\"data row7 col2\" >AE</td>\n",
       "      <td id=\"T_86ab3_row7_col3\" class=\"data row7 col3\" >0.001</td>\n",
       "      <td id=\"T_86ab3_row7_col4\" class=\"data row7 col4\" >SM</td>\n",
       "      <td id=\"T_86ab3_row7_col5\" class=\"data row7 col5\" >fixed</td>\n",
       "      <td id=\"T_86ab3_row7_col6\" class=\"data row7 col6\" >4.000</td>\n",
       "      <td id=\"T_86ab3_row7_col7\" class=\"data row7 col7\" >26.146</td>\n",
       "      <td id=\"T_86ab3_row7_col8\" class=\"data row7 col8\" >0.000</td>\n",
       "      <td id=\"T_86ab3_row7_col9\" class=\"data row7 col9\" >27.531</td>\n",
       "      <td id=\"T_86ab3_row7_col10\" class=\"data row7 col10\" >0.322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_86ab3_level0_row8\" class=\"row_heading level0 row8\" >345</th>\n",
       "      <td id=\"T_86ab3_row8_col0\" class=\"data row8 col0\" >1727065255</td>\n",
       "      <td id=\"T_86ab3_row8_col1\" class=\"data row8 col1\" >Synthetic_random_100</td>\n",
       "      <td id=\"T_86ab3_row8_col2\" class=\"data row8 col2\" >AE</td>\n",
       "      <td id=\"T_86ab3_row8_col3\" class=\"data row8 col3\" >0.001</td>\n",
       "      <td id=\"T_86ab3_row8_col4\" class=\"data row8 col4\" >SM</td>\n",
       "      <td id=\"T_86ab3_row8_col5\" class=\"data row8 col5\" >fixed</td>\n",
       "      <td id=\"T_86ab3_row8_col6\" class=\"data row8 col6\" >4.000</td>\n",
       "      <td id=\"T_86ab3_row8_col7\" class=\"data row8 col7\" >26.146</td>\n",
       "      <td id=\"T_86ab3_row8_col8\" class=\"data row8 col8\" >0.250</td>\n",
       "      <td id=\"T_86ab3_row8_col9\" class=\"data row8 col9\" >27.567</td>\n",
       "      <td id=\"T_86ab3_row8_col10\" class=\"data row8 col10\" >0.320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_86ab3_level0_row9\" class=\"row_heading level0 row9\" >346</th>\n",
       "      <td id=\"T_86ab3_row9_col0\" class=\"data row9 col0\" >1727065255</td>\n",
       "      <td id=\"T_86ab3_row9_col1\" class=\"data row9 col1\" >Synthetic_random_100</td>\n",
       "      <td id=\"T_86ab3_row9_col2\" class=\"data row9 col2\" >AE</td>\n",
       "      <td id=\"T_86ab3_row9_col3\" class=\"data row9 col3\" >0.001</td>\n",
       "      <td id=\"T_86ab3_row9_col4\" class=\"data row9 col4\" >SM</td>\n",
       "      <td id=\"T_86ab3_row9_col5\" class=\"data row9 col5\" >fixed</td>\n",
       "      <td id=\"T_86ab3_row9_col6\" class=\"data row9 col6\" >4.000</td>\n",
       "      <td id=\"T_86ab3_row9_col7\" class=\"data row9 col7\" >26.146</td>\n",
       "      <td id=\"T_86ab3_row9_col8\" class=\"data row9 col8\" >0.500</td>\n",
       "      <td id=\"T_86ab3_row9_col9\" class=\"data row9 col9\" >27.617</td>\n",
       "      <td id=\"T_86ab3_row9_col10\" class=\"data row9 col10\" >0.320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_86ab3_level0_row10\" class=\"row_heading level0 row10\" >347</th>\n",
       "      <td id=\"T_86ab3_row10_col0\" class=\"data row10 col0\" >1727065255</td>\n",
       "      <td id=\"T_86ab3_row10_col1\" class=\"data row10 col1\" >Synthetic_random_100</td>\n",
       "      <td id=\"T_86ab3_row10_col2\" class=\"data row10 col2\" >AE</td>\n",
       "      <td id=\"T_86ab3_row10_col3\" class=\"data row10 col3\" >0.001</td>\n",
       "      <td id=\"T_86ab3_row10_col4\" class=\"data row10 col4\" >SM</td>\n",
       "      <td id=\"T_86ab3_row10_col5\" class=\"data row10 col5\" >fixed</td>\n",
       "      <td id=\"T_86ab3_row10_col6\" class=\"data row10 col6\" >4.000</td>\n",
       "      <td id=\"T_86ab3_row10_col7\" class=\"data row10 col7\" >26.146</td>\n",
       "      <td id=\"T_86ab3_row10_col8\" class=\"data row10 col8\" >0.750</td>\n",
       "      <td id=\"T_86ab3_row10_col9\" class=\"data row10 col9\" >27.783</td>\n",
       "      <td id=\"T_86ab3_row10_col10\" class=\"data row10 col10\" >0.316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_86ab3_level0_row11\" class=\"row_heading level0 row11\" >348</th>\n",
       "      <td id=\"T_86ab3_row11_col0\" class=\"data row11 col0\" >1727065255</td>\n",
       "      <td id=\"T_86ab3_row11_col1\" class=\"data row11 col1\" >Synthetic_random_100</td>\n",
       "      <td id=\"T_86ab3_row11_col2\" class=\"data row11 col2\" >AE</td>\n",
       "      <td id=\"T_86ab3_row11_col3\" class=\"data row11 col3\" >0.001</td>\n",
       "      <td id=\"T_86ab3_row11_col4\" class=\"data row11 col4\" >SM</td>\n",
       "      <td id=\"T_86ab3_row11_col5\" class=\"data row11 col5\" >fixed</td>\n",
       "      <td id=\"T_86ab3_row11_col6\" class=\"data row11 col6\" >4.000</td>\n",
       "      <td id=\"T_86ab3_row11_col7\" class=\"data row11 col7\" >26.146</td>\n",
       "      <td id=\"T_86ab3_row11_col8\" class=\"data row11 col8\" >1.000</td>\n",
       "      <td id=\"T_86ab3_row11_col9\" class=\"data row11 col9\" >27.852</td>\n",
       "      <td id=\"T_86ab3_row11_col10\" class=\"data row11 col10\" >0.315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_86ab3_level0_row12\" class=\"row_heading level0 row12\" >623</th>\n",
       "      <td id=\"T_86ab3_row12_col0\" class=\"data row12 col0\" >1727204024</td>\n",
       "      <td id=\"T_86ab3_row12_col1\" class=\"data row12 col1\" >Synthetic_random_100</td>\n",
       "      <td id=\"T_86ab3_row12_col2\" class=\"data row12 col2\" >DDPM</td>\n",
       "      <td id=\"T_86ab3_row12_col3\" class=\"data row12 col3\" >0.010</td>\n",
       "      <td id=\"T_86ab3_row12_col4\" class=\"data row12 col4\" >RBF</td>\n",
       "      <td id=\"T_86ab3_row12_col5\" class=\"data row12 col5\" >predicted</td>\n",
       "      <td id=\"T_86ab3_row12_col6\" class=\"data row12 col6\" >2.000</td>\n",
       "      <td id=\"T_86ab3_row12_col7\" class=\"data row12 col7\" >26.464</td>\n",
       "      <td id=\"T_86ab3_row12_col8\" class=\"data row12 col8\" >0.000</td>\n",
       "      <td id=\"T_86ab3_row12_col9\" class=\"data row12 col9\" >28.116</td>\n",
       "      <td id=\"T_86ab3_row12_col10\" class=\"data row12 col10\" >0.295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_86ab3_level0_row13\" class=\"row_heading level0 row13\" >624</th>\n",
       "      <td id=\"T_86ab3_row13_col0\" class=\"data row13 col0\" >1727204024</td>\n",
       "      <td id=\"T_86ab3_row13_col1\" class=\"data row13 col1\" >Synthetic_random_100</td>\n",
       "      <td id=\"T_86ab3_row13_col2\" class=\"data row13 col2\" >DDPM</td>\n",
       "      <td id=\"T_86ab3_row13_col3\" class=\"data row13 col3\" >0.010</td>\n",
       "      <td id=\"T_86ab3_row13_col4\" class=\"data row13 col4\" >RBF</td>\n",
       "      <td id=\"T_86ab3_row13_col5\" class=\"data row13 col5\" >predicted</td>\n",
       "      <td id=\"T_86ab3_row13_col6\" class=\"data row13 col6\" >2.000</td>\n",
       "      <td id=\"T_86ab3_row13_col7\" class=\"data row13 col7\" >26.464</td>\n",
       "      <td id=\"T_86ab3_row13_col8\" class=\"data row13 col8\" >0.250</td>\n",
       "      <td id=\"T_86ab3_row13_col9\" class=\"data row13 col9\" >28.104</td>\n",
       "      <td id=\"T_86ab3_row13_col10\" class=\"data row13 col10\" >0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_86ab3_level0_row14\" class=\"row_heading level0 row14\" >625</th>\n",
       "      <td id=\"T_86ab3_row14_col0\" class=\"data row14 col0\" >1727204024</td>\n",
       "      <td id=\"T_86ab3_row14_col1\" class=\"data row14 col1\" >Synthetic_random_100</td>\n",
       "      <td id=\"T_86ab3_row14_col2\" class=\"data row14 col2\" >DDPM</td>\n",
       "      <td id=\"T_86ab3_row14_col3\" class=\"data row14 col3\" >0.010</td>\n",
       "      <td id=\"T_86ab3_row14_col4\" class=\"data row14 col4\" >RBF</td>\n",
       "      <td id=\"T_86ab3_row14_col5\" class=\"data row14 col5\" >predicted</td>\n",
       "      <td id=\"T_86ab3_row14_col6\" class=\"data row14 col6\" >2.000</td>\n",
       "      <td id=\"T_86ab3_row14_col7\" class=\"data row14 col7\" >26.464</td>\n",
       "      <td id=\"T_86ab3_row14_col8\" class=\"data row14 col8\" >0.500</td>\n",
       "      <td id=\"T_86ab3_row14_col9\" class=\"data row14 col9\" >28.341</td>\n",
       "      <td id=\"T_86ab3_row14_col10\" class=\"data row14 col10\" >0.349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_86ab3_level0_row15\" class=\"row_heading level0 row15\" >626</th>\n",
       "      <td id=\"T_86ab3_row15_col0\" class=\"data row15 col0\" >1727204024</td>\n",
       "      <td id=\"T_86ab3_row15_col1\" class=\"data row15 col1\" >Synthetic_random_100</td>\n",
       "      <td id=\"T_86ab3_row15_col2\" class=\"data row15 col2\" >DDPM</td>\n",
       "      <td id=\"T_86ab3_row15_col3\" class=\"data row15 col3\" >0.010</td>\n",
       "      <td id=\"T_86ab3_row15_col4\" class=\"data row15 col4\" >RBF</td>\n",
       "      <td id=\"T_86ab3_row15_col5\" class=\"data row15 col5\" >predicted</td>\n",
       "      <td id=\"T_86ab3_row15_col6\" class=\"data row15 col6\" >2.000</td>\n",
       "      <td id=\"T_86ab3_row15_col7\" class=\"data row15 col7\" >26.464</td>\n",
       "      <td id=\"T_86ab3_row15_col8\" class=\"data row15 col8\" >0.750</td>\n",
       "      <td id=\"T_86ab3_row15_col9\" class=\"data row15 col9\" >28.231</td>\n",
       "      <td id=\"T_86ab3_row15_col10\" class=\"data row15 col10\" >0.378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_86ab3_level0_row16\" class=\"row_heading level0 row16\" >627</th>\n",
       "      <td id=\"T_86ab3_row16_col0\" class=\"data row16 col0\" >1727204024</td>\n",
       "      <td id=\"T_86ab3_row16_col1\" class=\"data row16 col1\" >Synthetic_random_100</td>\n",
       "      <td id=\"T_86ab3_row16_col2\" class=\"data row16 col2\" >DDPM</td>\n",
       "      <td id=\"T_86ab3_row16_col3\" class=\"data row16 col3\" >0.010</td>\n",
       "      <td id=\"T_86ab3_row16_col4\" class=\"data row16 col4\" >RBF</td>\n",
       "      <td id=\"T_86ab3_row16_col5\" class=\"data row16 col5\" >predicted</td>\n",
       "      <td id=\"T_86ab3_row16_col6\" class=\"data row16 col6\" >2.000</td>\n",
       "      <td id=\"T_86ab3_row16_col7\" class=\"data row16 col7\" >26.464</td>\n",
       "      <td id=\"T_86ab3_row16_col8\" class=\"data row16 col8\" >1.000</td>\n",
       "      <td id=\"T_86ab3_row16_col9\" class=\"data row16 col9\" >28.526</td>\n",
       "      <td id=\"T_86ab3_row16_col10\" class=\"data row16 col10\" >0.389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_86ab3_level0_row17\" class=\"row_heading level0 row17\" >703</th>\n",
       "      <td id=\"T_86ab3_row17_col0\" class=\"data row17 col0\" >1727218843</td>\n",
       "      <td id=\"T_86ab3_row17_col1\" class=\"data row17 col1\" >Synthetic_random_100</td>\n",
       "      <td id=\"T_86ab3_row17_col2\" class=\"data row17 col2\" >DDPM</td>\n",
       "      <td id=\"T_86ab3_row17_col3\" class=\"data row17 col3\" >0.001</td>\n",
       "      <td id=\"T_86ab3_row17_col4\" class=\"data row17 col4\" >SM</td>\n",
       "      <td id=\"T_86ab3_row17_col5\" class=\"data row17 col5\" >predicted</td>\n",
       "      <td id=\"T_86ab3_row17_col6\" class=\"data row17 col6\" >2.000</td>\n",
       "      <td id=\"T_86ab3_row17_col7\" class=\"data row17 col7\" >26.275</td>\n",
       "      <td id=\"T_86ab3_row17_col8\" class=\"data row17 col8\" >0.000</td>\n",
       "      <td id=\"T_86ab3_row17_col9\" class=\"data row17 col9\" >27.704</td>\n",
       "      <td id=\"T_86ab3_row17_col10\" class=\"data row17 col10\" >0.280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_86ab3_level0_row18\" class=\"row_heading level0 row18\" >704</th>\n",
       "      <td id=\"T_86ab3_row18_col0\" class=\"data row18 col0\" >1727218843</td>\n",
       "      <td id=\"T_86ab3_row18_col1\" class=\"data row18 col1\" >Synthetic_random_100</td>\n",
       "      <td id=\"T_86ab3_row18_col2\" class=\"data row18 col2\" >DDPM</td>\n",
       "      <td id=\"T_86ab3_row18_col3\" class=\"data row18 col3\" >0.001</td>\n",
       "      <td id=\"T_86ab3_row18_col4\" class=\"data row18 col4\" >SM</td>\n",
       "      <td id=\"T_86ab3_row18_col5\" class=\"data row18 col5\" >predicted</td>\n",
       "      <td id=\"T_86ab3_row18_col6\" class=\"data row18 col6\" >2.000</td>\n",
       "      <td id=\"T_86ab3_row18_col7\" class=\"data row18 col7\" >26.275</td>\n",
       "      <td id=\"T_86ab3_row18_col8\" class=\"data row18 col8\" >0.250</td>\n",
       "      <td id=\"T_86ab3_row18_col9\" class=\"data row18 col9\" >27.704</td>\n",
       "      <td id=\"T_86ab3_row18_col10\" class=\"data row18 col10\" >0.280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_86ab3_level0_row19\" class=\"row_heading level0 row19\" >705</th>\n",
       "      <td id=\"T_86ab3_row19_col0\" class=\"data row19 col0\" >1727218843</td>\n",
       "      <td id=\"T_86ab3_row19_col1\" class=\"data row19 col1\" >Synthetic_random_100</td>\n",
       "      <td id=\"T_86ab3_row19_col2\" class=\"data row19 col2\" >DDPM</td>\n",
       "      <td id=\"T_86ab3_row19_col3\" class=\"data row19 col3\" >0.001</td>\n",
       "      <td id=\"T_86ab3_row19_col4\" class=\"data row19 col4\" >SM</td>\n",
       "      <td id=\"T_86ab3_row19_col5\" class=\"data row19 col5\" >predicted</td>\n",
       "      <td id=\"T_86ab3_row19_col6\" class=\"data row19 col6\" >2.000</td>\n",
       "      <td id=\"T_86ab3_row19_col7\" class=\"data row19 col7\" >26.275</td>\n",
       "      <td id=\"T_86ab3_row19_col8\" class=\"data row19 col8\" >0.500</td>\n",
       "      <td id=\"T_86ab3_row19_col9\" class=\"data row19 col9\" >27.704</td>\n",
       "      <td id=\"T_86ab3_row19_col10\" class=\"data row19 col10\" >0.280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_86ab3_level0_row20\" class=\"row_heading level0 row20\" >706</th>\n",
       "      <td id=\"T_86ab3_row20_col0\" class=\"data row20 col0\" >1727218843</td>\n",
       "      <td id=\"T_86ab3_row20_col1\" class=\"data row20 col1\" >Synthetic_random_100</td>\n",
       "      <td id=\"T_86ab3_row20_col2\" class=\"data row20 col2\" >DDPM</td>\n",
       "      <td id=\"T_86ab3_row20_col3\" class=\"data row20 col3\" >0.001</td>\n",
       "      <td id=\"T_86ab3_row20_col4\" class=\"data row20 col4\" >SM</td>\n",
       "      <td id=\"T_86ab3_row20_col5\" class=\"data row20 col5\" >predicted</td>\n",
       "      <td id=\"T_86ab3_row20_col6\" class=\"data row20 col6\" >2.000</td>\n",
       "      <td id=\"T_86ab3_row20_col7\" class=\"data row20 col7\" >26.275</td>\n",
       "      <td id=\"T_86ab3_row20_col8\" class=\"data row20 col8\" >0.750</td>\n",
       "      <td id=\"T_86ab3_row20_col9\" class=\"data row20 col9\" >27.704</td>\n",
       "      <td id=\"T_86ab3_row20_col10\" class=\"data row20 col10\" >0.280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_86ab3_level0_row21\" class=\"row_heading level0 row21\" >707</th>\n",
       "      <td id=\"T_86ab3_row21_col0\" class=\"data row21 col0\" >1727218843</td>\n",
       "      <td id=\"T_86ab3_row21_col1\" class=\"data row21 col1\" >Synthetic_random_100</td>\n",
       "      <td id=\"T_86ab3_row21_col2\" class=\"data row21 col2\" >DDPM</td>\n",
       "      <td id=\"T_86ab3_row21_col3\" class=\"data row21 col3\" >0.001</td>\n",
       "      <td id=\"T_86ab3_row21_col4\" class=\"data row21 col4\" >SM</td>\n",
       "      <td id=\"T_86ab3_row21_col5\" class=\"data row21 col5\" >predicted</td>\n",
       "      <td id=\"T_86ab3_row21_col6\" class=\"data row21 col6\" >2.000</td>\n",
       "      <td id=\"T_86ab3_row21_col7\" class=\"data row21 col7\" >26.275</td>\n",
       "      <td id=\"T_86ab3_row21_col8\" class=\"data row21 col8\" >1.000</td>\n",
       "      <td id=\"T_86ab3_row21_col9\" class=\"data row21 col9\" >27.704</td>\n",
       "      <td id=\"T_86ab3_row21_col10\" class=\"data row21 col10\" >0.280</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2b2ada1d55b0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "experiment_df['lr'] = pd.to_numeric(experiment_df['lr'], errors='coerce') \n",
    "experiment_df['Percent'] = pd.to_numeric(experiment_df['Percent'], errors='coerce') \n",
    "experiment_df['kernel_param'] = pd.to_numeric(experiment_df['kernel_param'], errors='coerce') \n",
    "experiment_df['Loss'] = pd.to_numeric(experiment_df['Loss'], errors='coerce') \n",
    "experiment_df['Loss'] = experiment_df['Loss'].apply(lambda x: '-' if x < -100000 else f\"{x:.3f}\")\n",
    "experiment_df['R2'] = pd.to_numeric(experiment_df['R2'], errors='coerce') \n",
    "experiment_df['R2'] = experiment_df['R2'].apply(lambda x: '-' if x < -100000  else f\"{x:.3f}\")\n",
    "\n",
    "baseline_rows = experiment_df[experiment_df['kernel_param'] == 0]\n",
    "NPP_rows = experiment_df[experiment_df['kernel_param'] != 0]\n",
    "\n",
    "final_baseline = baseline_rows.loc[\n",
    "    baseline_rows.groupby(['feature'])['Best Val'].idxmin() #,  , 'kernel_mode', 'kernel_param'\n",
    "]\n",
    "final_NPP = NPP_rows.loc[\n",
    "    NPP_rows.groupby(['feature', 'kernel', 'Percent'])['Best Val'].idxmin() #,  , 'kernel_mode', 'kernel_param'\n",
    "]\n",
    "\n",
    "final_df = pd.concat([final_baseline, final_NPP])\n",
    "\n",
    "# final_df\n",
    "def color_rows(row):\n",
    "    if row.name in final_baseline.index:\n",
    "        return ['background-color: lightblue'] * len(row)\n",
    "    else:\n",
    "        # Find the matching rows based on 'feature' and 'kernel'\n",
    "        matching_rows = final_baseline[\n",
    "            (final_baseline['feature'] == row['feature']) & \n",
    "            (final_baseline['kernel'] == row['kernel'])\n",
    "        ]\n",
    "        if not matching_rows.empty:\n",
    "            return ['background-color: lightgreen'] * len(row)\n",
    "        return [''] * len(row)\n",
    "\n",
    "final_df = final_df.applymap(lambda x: f\"{x:.3f}\" if isinstance(x, (float, int)) else x)\n",
    "# Apply the styling function to the final DataFrame\n",
    "styled_df = final_df.style.apply(color_rows, axis=1)\n",
    "# Display the styled DataFrame (only works in notebooks or exported as HTML)\n",
    "styled_df\n",
    "# experiment_df\n",
    "# final_baseline\n",
    "# baseline_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44af4950-6755-46e7-bf07-75419680f08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './history/PinMNIST.csv'\n",
    "pd.set_option(\"display.precision\", 3)\n",
    "if os.path.exists(file_path):\n",
    "    # If the file exists, append the new DataFrame\n",
    "    final_df.to_csv(file_path, mode='a', header=False, index=False)\n",
    "else:\n",
    "    # If the file doesn't exist, create a new file and write the DataFrame\n",
    "    final_df.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53e9671-2eb3-4b0c-b3a7-cc3a005ac20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract information from config.json\n",
    "def extract_info_from_config(config_path):\n",
    "    with open(config_path, 'r') as config_file:\n",
    "        config_data = json.load(config_file)\n",
    "        experiment_id = config_data.get('experiment_id', '')\n",
    "        dataset = config_data.get('dataset', '')\n",
    "        feature = config_data.get('feature', '')\n",
    "        mode = config_data.get('mode', '')\n",
    "        param = config_data.get('n_pins', '')\n",
    "        deeper = config_data.get('deeper', '')\n",
    "        try:\n",
    "            manual = config_data.get('manual_lr', '')\n",
    "            manual = 'Yes' if manual else 'No'\n",
    "        except: \n",
    "            manual = 'No'\n",
    "    return experiment_id, dataset, feature, mode, param, deeper, manual\n",
    "\n",
    "# Function to parse the test_PinMNIST files\n",
    "def parse_test(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read().strip()\n",
    "        parts = content.split('|')\n",
    "        mse = float(parts[0].split(',')[0].split()[1])\n",
    "        r2mse = float(parts[0].split(',')[1].split()[1])\n",
    "        npp_part = parts[1].split(';')[0]\n",
    "        gp_part = parts[1].split(';')[1]\n",
    "        npp = float(npp_part.split(',')[0].split()[1])\n",
    "        r2npp = float(npp_part.split(',')[1].split()[1])\n",
    "        gp = float(gp_part.split(',')[0].split()[1])\n",
    "        r2gp = float(gp_part.split(',')[1].split()[1])\n",
    "    return mse, r2mse, npp, r2npp, gp, r2gp\n",
    "\n",
    "def parse_res(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read()  # Read the entire file content as a single string\n",
    "    \n",
    "    # Define the regular expression pattern to capture MSE, sigma, and the two float values\n",
    "    pattern = r\"MSE: ([\\d.]+), R2: ([\\d.-]+) \\| NPP \\(sigma ([\\d.]+)\\): ([\\d.]+), R2: ([\\d.-]+); GP: ([\\d.]+), R2: ([\\d.-]+)\"\n",
    "    \n",
    "    # Find all matches in the content\n",
    "    match = re.findall(pattern, content)[0]\n",
    "    mse = float(match[0])  # Convert MSE value to float\n",
    "    r2mse = float(match[1])\n",
    "    sigma = float(match[2])  # Convert sigma value to float\n",
    "    npp = float(match[3])  # Convert NPP value to float\n",
    "    r2npp = float(match[4])\n",
    "    gp = float(match[5])  # Convert GP value to float\n",
    "    r2gp = float(match[6])\n",
    "    \n",
    "    return mse, r2mse, sigma, npp, r2npp, gp, r2gp\n",
    "\n",
    "def append(exp_id, df, pp, mse, r2mse, sigma, npp, r2npp, gp, r2gp, dataset, feature, mode, param, deeper, manual='No'):\n",
    "    return pd.concat([df, pd.DataFrame({\n",
    "        'Experiment ID': [exp_id],\n",
    "        'Dataset': [dataset],\n",
    "        'Feature': [feature],\n",
    "        'Extra layers': [deeper],\n",
    "        'Manual LR': [manual],\n",
    "        'Mode': [mode],\n",
    "        'NPins': [param],\n",
    "        'Partial Percent': [pp],\n",
    "        'MSE': [mse],\n",
    "        'R2 MSE': [r2mse], \n",
    "        'Sigma': [sigma],\n",
    "        'NPP': [npp],\n",
    "        'R2 NPP': [r2npp],\n",
    "        'GP': [gp],\n",
    "        'R2 GP': [r2gp]})], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207cf7dc-5e75-4f79-ae14-cfba5fa1177a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty DataFrame\n",
    "df = pd.DataFrame(columns=['Experiment ID', 'Dataset', 'Feature', 'Extra layers', 'Mode', 'NPins', 'Partial Percent', 'MSE', 'R2 MSE', 'Sigma', 'NPP', 'R2 NPP', 'GP', 'R2 GP'])\n",
    "\n",
    "# Directory containing the folders\n",
    "base_folders = ['./history/P100_normal']\n",
    "\n",
    "for base_folder in base_folders:\n",
    "    # Loop through each directory in the base folder\n",
    "    for dir_name in os.listdir(base_folder):\n",
    "        if dir_name.startswith('17') or dir_name.startswith('mesh') or dir_name.startswith('random'):\n",
    "            dir_path = os.path.join(base_folder, dir_name)\n",
    "\n",
    "            # Check if the item in the base folder is a directory\n",
    "            if os.path.isdir(dir_path):\n",
    "                # Get config.json info\n",
    "                config_path = os.path.join(dir_path, 'config.json')\n",
    "                exp_id, dataset, feature, mode, param, deeper, manual = extract_info_from_config(config_path)\n",
    "                mse, r2mse, sigma, npp, r2npp, gp, r2gp = parse_res(os.path.join(dir_path, 'results.txt'))\n",
    "                df = append(exp_id, df, 0.0, mse, r2mse, sigma, npp, r2npp, gp, r2gp, dataset, feature, mode, param, deeper, manual)\n",
    "\n",
    "                # Loop through files starting with 'test_PinMNIST'\n",
    "                for file_name in os.listdir(dir_path):\n",
    "                    if file_name.startswith('test_PinMNIST') or file_name.startswith('test_Synthetic') or file_name.startswith('test_Building'):\n",
    "                        pp = float(file_name.split('_')[-1][:-4])\n",
    "                        file_path = os.path.join(dir_path, file_name)\n",
    "                        mse, r2mse, npp, r2npp, gp, r2gp = parse_test(file_path)\n",
    "                        df = append(exp_id, df, pp, mse, r2mse, sigma, npp, r2npp, gp, r2gp, dataset, feature, mode, param, deeper, manual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01843de3-ea89-49e9-8487-42b4e7199be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df, pd.DataFrame({\n",
    "        'Experiment ID': [None, None, None, None, None, None],\n",
    "        'Dataset': ['Building', 'Building', 'Building', 'Building', 'Building', 'Building'],\n",
    "        'Feature': ['AE', 'AE', 'AE', 'AE', 'AE', 'AE'],\n",
    "        'Extra layers': [True, True, True, True, True, True],\n",
    "        'Manual LR': ['No', 'No', 'No', 'No', 'No', 'No'],\n",
    "        'Mode': ['mesh', 'mesh', 'mesh', 'random', 'random', 'random'],\n",
    "        'NPins': [16, 121, 225, 10, 100, 200],\n",
    "        'Partial Percent': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0],\n",
    "        'MSE': [0.461, 0.423, 1.302, 0.678, 0.548, 0.509],\n",
    "        'R2 MSE': [0.129, 0.251, -0.215, -0.192, 0.059, 0.063], \n",
    "        'Sigma': [0.1, 0.1, 2.0, 2.0, 0.2, 0.2],\n",
    "        'NPP': [0.463, 0.472, 0.355, 0.572, 0.442, 0.488],\n",
    "        'R2 NPP': [0.248, 0.220, 0.174, 0.131, 0.243, 0.117],\n",
    "        'GP': [0.463, 0.472, 0.355, 0.571, 0.416, 0.468],\n",
    "        'R2 GP': [0.248, 0.211, 0.174, 0.099, 0.261, 0.178]})], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ced106d-5cd5-49d2-a77c-560e05f360f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df, pd.DataFrame({\n",
    "        'Experiment ID': [None, None, None, None],\n",
    "        'Dataset': ['Building', 'Building', 'Building', 'Building'],\n",
    "        'Feature': ['DDPM', 'DDPM', 'DDPM', 'DDPM'],\n",
    "        'Extra layers': [True, True, True, True],\n",
    "        'Manual LR': ['Yes', 'Yes', 'Yes', 'Yes'],\n",
    "        'Mode': ['mesh', 'mesh', 'random', 'random'],\n",
    "        'NPins': [16, 121, 10, 100],\n",
    "        'Partial Percent': [1.0, 1.0, 1.0, 1.0],\n",
    "        'MSE': [1.355, 1.579, 2.032, 1.816],\n",
    "        'R2 MSE': [0.11, -0.190, -0.216, -0.205], \n",
    "        'Sigma': [0.1, 0.1, 2.0, 2.0],\n",
    "        'NPP': [0.784, 1.108, 1.564, 0.975],\n",
    "        'R2 NPP': [-0.140, -1.862, -0.233, -2.117],\n",
    "        'GP': [0.784, 1.108, 1.560, 0.944],\n",
    "        'R2 GP': [-0.140, -1.862, -0.230, -2.055]})], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1601c328-e258-459c-a13e-fec0a1786717",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['NPins'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ef8f04-5bd8-4732-8651-f70b9fe7f4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['Dataset'] == 'PinMNIST') & (df['Partial Percent'] != 0.00) & (df['Feature'] == 'AE')].drop(labels=['Extra layers', 'Manual LR'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f73e188-f5c1-4b08-9c55-a622ef1d6c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by=['NPins', 'Partial Percent', 'Extra layers', 'Manual LR'], inplace=True, ascending=False, ignore_index=True)\n",
    "df.sort_values(by=['Dataset', 'Feature', 'Mode'], inplace=True, ignore_index=True)\n",
    "df.drop_duplicates(subset=['Dataset', 'Feature', 'Extra layers', 'Mode', 'NPins', 'Partial Percent', 'Manual LR'], keep='last', inplace=True)\n",
    "#df.to_csv('data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3252208c-2db7-4337-bd15-d5b07d0bff15",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by=['NPins', 'Partial Percent', 'Extra layers', 'Manual LR'], inplace=True, ascending=False, ignore_index=True)\n",
    "df.sort_values(by=['Dataset', 'Mode'], inplace=True, ignore_index=True)\n",
    "df[(df['Dataset'] == 'PinMNIST') & (df['Partial Percent'] == 0.00)].drop(labels=['Extra layers', 'Manual LR', 'Sigma', 'R2 NPP', 'NPP'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc28edae-2951-43f0-bf14-07f1e70340fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['Partial Percent'] == 1.0) & (df['Dataset'] == 'Synthetic')].drop(labels=['Partial Percent', 'Extra layers', 'Manual LR'], axis=1).reset_index(drop=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f42cecc-9c93-45ad-ae89-86364357b7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_npins_combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6287450c-0f18-48ed-a059-aaf9cc0f6164",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "\n",
    "font = {'size': 20}\n",
    "matplotlib.rc('font', **font)\n",
    "#plt.rcParams['text.usetex'] = True\n",
    "\n",
    "# Assuming 'df' is your DataFrame, ensure it's correctly loaded or defined above this block\n",
    "# Filter data for the feature AE\n",
    "feature_data = df[df['Feature'] == 'AE']\n",
    "\n",
    "# Separate data by dataset\n",
    "datasets = ['Synthetic', 'PinMNIST', 'Building']\n",
    "colors = [\"#56CCF2\", \"#EB5757\", \"#333333\", \"#F2C94C\"]\n",
    "patterns = [None, None, \"x\", \"x\"] #[\"/\", \"\\\\\", \"x\", \".\"]\n",
    "labels = [\"grid\\nsparse\", \"grid\\ndense\", \"random\\nsparse\", \"random\\ndense\"]\n",
    "\n",
    "# Assuming 'r2_nps' is correctly defined as per your context\n",
    "r2_nps = [[0.969, 0.452, 0.405, 0.388], [-0.576, 0.607, -200.553, 0.546], [-0.271, -0.029, -0.202, -0.0958]] #-0.113 -0.032\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(nrows=1, ncols=len(datasets), figsize=(20, 6))\n",
    "\n",
    "for i, (ax, dataset) in enumerate(zip(axes, datasets)):\n",
    "    dataset_data = feature_data[feature_data['Dataset'] == dataset]\n",
    "    \n",
    "    if dataset != 'Building':\n",
    "        mode_npins_combinations = dataset_data.groupby(['Mode', 'NPins']).size().index.tolist()\n",
    "        mode_npins_combinations = sorted(mode_npins_combinations, key=lambda x: (x[0], x[1]))\n",
    "    else:\n",
    "        mode_npins_combinations = [('mesh', 16), ('mesh', 121), ('random', 10), ('random', 100)]\n",
    "    \n",
    "    #labels = [f\"random\\n{npins}\" if mode == 'random' else f\"grid\\n{npins}\" for mode, npins in mode_npins_combinations]\n",
    "    x = np.arange(len(labels))  # the label locations\n",
    "    \n",
    "    width = 0.22  # Decreased width for slight whitespace between bars\n",
    "    \n",
    "    # Assuming these lists are populated correctly from your dataset\n",
    "    r2_mse = [dataset_data[(dataset_data['Mode'] == mode) & (dataset_data['NPins'] == npins) & (dataset_data['Partial Percent'] == 1.0)]['R2 MSE'].values[0] for mode, npins in mode_npins_combinations]\n",
    "    r2_npp = [dataset_data[(dataset_data['Mode'] == mode) & (dataset_data['NPins'] == npins) & (dataset_data['Partial Percent'] == 1.0)]['R2 NPP'].values[0] for mode, npins in mode_npins_combinations]\n",
    "    r2_gp = [dataset_data[(dataset_data['Mode'] == mode) & (dataset_data['NPins'] == npins) & (dataset_data['Partial Percent'] == 1.0)]['R2 GP'].values[0] for mode, npins in mode_npins_combinations]\n",
    "    r2_np = r2_nps[i]\n",
    "    \n",
    "    # Adjusted positions for a small whitespace between bars\n",
    "    rects1 = ax.bar(x - 1.5*width, r2_mse, width, label='Plain', color=colors[0], hatch=patterns[0], zorder=3)\n",
    "    rects2 = ax.bar(x - 0.5*width, r2_npp, width, label='NPP', color=colors[1], hatch=patterns[1], zorder=3)\n",
    "    rects3 = ax.bar(x + 0.5*width, r2_gp, width, label='NPP-GP', color=colors[2], hatch=patterns[2], edgecolor='#FFFFFF', zorder=3)\n",
    "    rects4 = ax.bar(x + 1.5*width, r2_np, width, label='NP', color=colors[3], hatch=patterns[3], zorder=3)\n",
    "\n",
    "    ax.set_ylabel('$R^2$ Coefficients')\n",
    "    if dataset == 'Building':\n",
    "        dataset = 'Rotterdam'\n",
    "    ax.set_title(f'{dataset} - AE Feature')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.grid(True, which='major', linestyle='--', linewidth='0.5', color='grey')\n",
    "    if i == 1:\n",
    "        ax.set_ylim([-1, 1.05])\n",
    "\n",
    "handles, labels = axes[0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='lower center', bbox_to_anchor=(0.5, -0.05), ncol=4)\n",
    "#fig.suptitle('$R^2$ analysis for different densities')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.savefig(\"AE_barplot_pp1.pdf\", format=\"pdf\", bbox_inches=\"tight\", transparent=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856b2454-cbe6-46a0-9eb8-d2b7707c6afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "\n",
    "font = {'size': 20}\n",
    "matplotlib.rc('font', **font)\n",
    "#plt.rcParams['text.usetex'] = True\n",
    "\n",
    "# Assuming 'df' is your DataFrame, ensure it's correctly loaded or defined above this block\n",
    "# Filter data for the feature AE\n",
    "feature_data = df[df['Feature'] == 'DDPM']\n",
    "\n",
    "# Separate data by dataset\n",
    "datasets = ['Synthetic', 'PinMNIST', 'Building']\n",
    "colors = [\"#56CCF2\", \"#EB5757\", \"#333333\", \"#F2C94C\"]\n",
    "patterns = [None, None, \"x\", \"x\"] #[\"/\", \"\\\\\", \"x\", \".\"]\n",
    "labels = [\"grid\\nsparse\", \"grid\\ndense\", \"random\\nsparse\", \"random\\ndense\"]\n",
    "\n",
    "# Assuming 'r2_nps' is correctly defined as per your context\n",
    "r2_nps = [[0.969, 0.452, 0.405, 0.388], [-0.576, 0.607, -200.553, 0.546], [-0.271, -0.029, -0.202, -0.0958]] #-0.113 -0.032\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(nrows=1, ncols=len(datasets), figsize=(20, 6))\n",
    "\n",
    "for i, (ax, dataset) in enumerate(zip(axes, datasets)):\n",
    "    dataset_data = feature_data[feature_data['Dataset'] == dataset]\n",
    "    \n",
    "    if dataset != 'Building':\n",
    "        mode_npins_combinations = dataset_data.groupby(['Mode', 'NPins']).size().index.tolist()\n",
    "        mode_npins_combinations = sorted(mode_npins_combinations, key=lambda x: (x[0], x[1]))\n",
    "    else:\n",
    "        mode_npins_combinations = [('mesh', 16), ('mesh', 121), ('random', 10), ('random', 100)]\n",
    "    \n",
    "    #labels = [f\"random\\n{npins}\" if mode == 'random' else f\"grid\\n{npins}\" for mode, npins in mode_npins_combinations]\n",
    "    x = np.arange(len(labels))  # the label locations\n",
    "    \n",
    "    width = 0.22  # Decreased width for slight whitespace between bars\n",
    "    \n",
    "    # Assuming these lists are populated correctly from your dataset\n",
    "    r2_mse = [dataset_data[(dataset_data['Mode'] == mode) & (dataset_data['NPins'] == npins) & (dataset_data['Partial Percent'] == 1.0)]['R2 MSE'].values[0] for mode, npins in mode_npins_combinations]\n",
    "    r2_npp = [dataset_data[(dataset_data['Mode'] == mode) & (dataset_data['NPins'] == npins) & (dataset_data['Partial Percent'] == 1.0)]['R2 NPP'].values[0] for mode, npins in mode_npins_combinations]\n",
    "    r2_gp = [dataset_data[(dataset_data['Mode'] == mode) & (dataset_data['NPins'] == npins) & (dataset_data['Partial Percent'] == 1.0)]['R2 GP'].values[0] for mode, npins in mode_npins_combinations]\n",
    "    r2_np = r2_nps[i]\n",
    "    \n",
    "    # Adjusted positions for a small whitespace between bars\n",
    "    rects1 = ax.bar(x - 1.5*width, r2_mse, width, label='Plain', color=colors[0], hatch=patterns[0], zorder=3)\n",
    "    rects2 = ax.bar(x - 0.5*width, r2_npp, width, label='NPP', color=colors[1], hatch=patterns[1], zorder=3)\n",
    "    rects3 = ax.bar(x + 0.5*width, r2_gp, width, label='NPP-GP', color=colors[2], hatch=patterns[2], edgecolor='#FFFFFF', zorder=3)\n",
    "    rects4 = ax.bar(x + 1.5*width, r2_np, width, label='NP', color=colors[3], hatch=patterns[3], zorder=3)\n",
    "\n",
    "    ax.set_ylabel('$R^2$ Coefficients')\n",
    "    if dataset == 'Building':\n",
    "        dataset = 'Rotterdam'\n",
    "    ax.set_title(f'{dataset} - DDPM Feature')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.grid(True, which='major', linestyle='--', linewidth='0.5', color='grey')\n",
    "    if i == 1:\n",
    "        ax.set_ylim([-1, 1.05])\n",
    "\n",
    "handles, labels = axes[0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='lower center', bbox_to_anchor=(0.5, -0.05), ncol=4)\n",
    "#fig.suptitle('$R^2$ analysis for different densities')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.savefig(\"DDPM_barplot_pp1.pdf\", format=\"pdf\", bbox_inches=\"tight\", transparent=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f5cbb1-bc7a-443b-a6f5-2aa78c7d7888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NP Results\n",
    "Synthetic_NP = {\n",
    " \"mesh_sparse\": [0.969, 0.971, 0.972, 0.969],  # Grid Sparse\n",
    " \"mesh_dense\": [0.408, 0.440, 0.448, 0.452],   # Grid Dense\n",
    " \"random_sparse\": [0.418, 0.412, 0.416, 0.405], # Random Sparse\n",
    " \"random_dense\": [0.306, 0.357, 0.385, 0.388]   # Random Dense\n",
    "}\n",
    "\n",
    "PinMNIST_NP = {\n",
    " \"mesh_sparse\": [-0.576, -0.577, -0.576, -0.576],  # Grid Sparse\n",
    " \"mesh_dense\": [0.535, 0.580, 0.600, 0.607],       # Grid Dense\n",
    " \"random_sparse\": [-125.973, -131.527, -252.488, -200.553], # Random Sparse\n",
    " \"random_dense\": [0.459, 0.518, 0.534, 0.546]               # Random Dense\n",
    "}\n",
    "\n",
    "Rotterdam_NP = {\n",
    " \"mesh_sparse\": [-0.326, -0.245, -0.270, -0.271],  # Grid Sparse\n",
    " \"mesh_dense\": [-0.134, -0.059, -0.031, -0.029],  # Grid Dense\n",
    " \"mesh_ultradense\": [-0.201, -0.128, -0.121, -0.113],  # Grid Ultradense\n",
    " \"random_sparse\": [-0.200, -0.203, -0.211, -0.202], # Random Sparse\n",
    " \"random_dense\": [-0.121, -0.102, -0.101, -0.096], # Random Dense\n",
    " \"random_ultradense\": [-0.057, -0.041, -0.034, -0.032] # Random Ultradense\n",
    "}\n",
    "\n",
    "NP = [Synthetic_NP, PinMNIST_NP, Rotterdam_NP]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34372947-6103-4de8-a865-d8040a9bebc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [0.25, 0.50, 0.75, 1.00]\n",
    "\n",
    "Rotterdam_NPP_GP = {\n",
    " \"mesh_sparse\": [0.248, 0.248, 0.248, 0.248],  # Grid Sparse\n",
    " \"mesh_dense\": [0.211, 0.211, 0.211, 0.211],  # Grid Dense\n",
    " \"mesh_ultradense\": [0.174, 0.174, 0.174, 0.174],   # Grid Ultradense\n",
    " \"random_sparse\": [0.126, 0.121, 0.108, 0.099], # Random Sparse\n",
    " \"random_dense\": [0.251, 0.255, 0.256, 0.261], # Random Dense\n",
    " \"random_ultradense\": [0.132, 0.142, 0.162, 0.178]  # Random Ultradense\n",
    "}\n",
    "\n",
    "NPP_GP = [None, None, Rotterdam_NPP_GP]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e502c1ab-2a00-4b21-9075-9a651fd45821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correcting the variable naming conflict and trying again\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "font = {'size': 20}\n",
    "\n",
    "matplotlib.rc('font', **font)\n",
    "\n",
    "# Filter the DataFrame for rows where Feature is equal to \"AE\"\n",
    "df_ae = df[(df['Feature'] == 'AE')]\n",
    "\n",
    "# Settings for visualization\n",
    "metrics = ['R2']\n",
    "data_groups = ['MSE', 'NPP']\n",
    "data_names = ['Plain', 'NPP']\n",
    "datasets = ['Synthetic', 'PinMNIST', 'Building']\n",
    "modes = ['mesh'] #df_ae['Mode'].unique()\n",
    "npins = [100] #df_ae['NPins'].unique()\n",
    "mode_name = 'random' if modes[0] == 'random' else 'grid'\n",
    "density = 'dense' if npins[0] == 100 else 'sparse'\n",
    "#colors = sns.color_palette(\"tab10\", n_colors=len(modes)*len(npins))\n",
    "colors= [\"#56CCF2\", \"#EB5757\", \"#333333\", \"#F2C94C\"] #[\"#56CCF2\", None, \"#EB5757\", None, \"#333333\", \"#F2C94C\"]\n",
    "linestyles = ['-', '--', ':', '-.']\n",
    "markers = ['o', 's', '^', 'P']\n",
    "fig, axs = plt.subplots(len(metrics), len(datasets), figsize=(19, 6), sharex=True)\n",
    "# Function to plot the lines, with fixed variable naming\n",
    "def plot_lines(ax, k, metric, only_GP = False):\n",
    "    if not only_GP:       \n",
    "        if k == 2 and modes[0] == 'mesh':\n",
    "            if npins[0] == 9:\n",
    "                npins[0] = 16\n",
    "            if npins[0] == 100:\n",
    "                npins[0] = 121\n",
    "        for mode_idx, mode in enumerate(modes):\n",
    "            for npins_idx, npins_value in enumerate(sorted(npins)):  # Corrected variable name here\n",
    "                for data_group_idx, data_group in enumerate(data_groups):\n",
    "                    # Filter the dataset for the specific conditions\n",
    "                    df_filtered = df_ae[(df_ae['Dataset'] == datasets[k]) & (df_ae['Mode'] == mode) &\n",
    "                                        (df_ae['NPins'] == npins_value)]\n",
    "                    if df_filtered.empty:\n",
    "                        print('HEY')\n",
    "                    label = data_names[data_group_idx]\n",
    "                    color = colors[data_group_idx] #colors[mode_idx * len(npins) + list(sorted(npins)).index(npins_value)]\n",
    "                    linestyle = linestyles[data_group_idx] #linestyles[data_group_idx]\n",
    "                    marker =  markers[data_group_idx] #markers[data_group_idx]\n",
    "                    value = df_filtered[df_filtered['Partial Percent'] == 1.0][data_group].values[0] if metric == 'MSE' else df_filtered[df_filtered['Partial Percent'] == 1.0][f'R2 {data_group}'].values[0]\n",
    "                    ax.plot(x, [value]*len(x),\n",
    "                            label=label, color=color, linestyle=linestyle, marker=marker, markersize=12, linewidth=3)\n",
    "        if k == 2:\n",
    "            ax.plot(x, NPP_GP[j][f\"{modes[0]}_{density}\"],\n",
    "                            label=\"NPP-GP\", color=colors[2], linestyle=linestyles[2], marker=markers[2], markersize=12, linewidth=3)\n",
    "        else:\n",
    "            df_filtered = df_ae[(df_ae['Dataset'] == datasets[k]) & (df_ae['Mode'] == mode) &\n",
    "                                        (df_ae['NPins'] == npins_value)]\n",
    "            ax.plot(df_filtered[df_filtered['Partial Percent'] != 0.0]['Partial Percent'], df_filtered[df_filtered['Partial Percent'] != 0.0][f'R2 GP'],\n",
    "                            label=\"NPP-GP\", color=colors[2], linestyle=linestyles[2], marker=markers[2], markersize=12, linewidth=3)\n",
    "\n",
    "\n",
    "\n",
    "        ax.plot(x, NP[j][f\"{modes[0]}_{density}\"],\n",
    "                            label=\"NP\", color=colors[3], linestyle=linestyles[3], marker=markers[3], markersize=12, linewidth=4)\n",
    "    else:\n",
    "        if k == 2 and modes[0] == 'mesh':\n",
    "            if npins[0] == 9:\n",
    "                npins[0] = 16\n",
    "            if npins[0] == 100:\n",
    "                npins[0] = 121\n",
    "        for mode_idx, mode in enumerate(modes):\n",
    "            for npins_idx, npins_value in enumerate(sorted(npins)):  # Corrected variable name here\n",
    "                for data_group_idx, data_group in enumerate(data_groups):\n",
    "                    if k == 2:\n",
    "                        ax.plot(x, NPP_GP[j][f\"{modes[0]}_{density}\"],\n",
    "                                        label=\"NPP-GP\", color=colors[2], linestyle=linestyles[2], marker=markers[2], markersize=12, linewidth=3)\n",
    "                    else:\n",
    "                        df_filtered = df_ae[(df_ae['Dataset'] == datasets[k]) & (df_ae['Mode'] == mode) &\n",
    "                                                    (df_ae['NPins'] == npins_value)]\n",
    "                        ax.plot(df_filtered[df_filtered['Partial Percent'] != 0.0]['Partial Percent'], df_filtered[df_filtered['Partial Percent'] != 0.0][f'R2 GP'],\n",
    "                                        label=\"NPP-GP\", color=colors[2], linestyle=linestyles[2], marker=markers[2], markersize=12, linewidth=3)\n",
    "\n",
    "# Re-plotting for each subplot with the corrected function\n",
    "\n",
    "for j, dataset in enumerate(datasets):\n",
    "    plot_lines(axs[j], j, metrics[0])\n",
    "    #if j == 1:\n",
    "    #    axs[j].set_ylim([0.92, 0.96])\n",
    "    axs[j].set_title(f'{dataset} - $R^2$')\n",
    "    axs[j].set_xlabel('Partial Percent')\n",
    "    axs[j].set_ylabel('$R^2$')\n",
    "    #axs[j].set_ylim([-1.1,1.1])\n",
    "    axs[j].set_xticks([0.25, 0.50, 0.75, 1.00])\n",
    "    #axs[i, j].legend(loc='best', fontsize='small')\n",
    "    axs[j].grid(True)\n",
    "    # Add the zoom-in subplot for the second dataset (PinMNIST)\n",
    "    if j == 1:  # Assuming PinMNIST is the second dataset\n",
    "        # Set the limits for the zoomed-in area\n",
    "        #axs[j].set_ylim([0.93, 0.97])\n",
    "        \n",
    "        # Create a new inset subplot with a zoomed view\n",
    "        inset_ax = axs[j].inset_axes([0.29, 0.35, 0.65, 0.5])  # [x, y, width, height] in fractions of figure width and height\n",
    "        plot_lines(inset_ax, j, metrics[0], False)\n",
    "        inset_ax.set_ylim([0.99756, 0.99770])\n",
    "        inset_ax.set_xticks([0.25, 0.50, 0.75, 1.00])\n",
    "        inset_ax.grid(True)\n",
    "    if j == 0:  # Assuming PinMNIST is the second dataset\n",
    "        # Set the limits for the zoomed-in area\n",
    "        #axs[j].set_ylim([0.93, 0.97])\n",
    "        \n",
    "        # Create a new inset subplot with a zoomed view\n",
    "        inset_ax = axs[j].inset_axes([0.20, 0.40, 0.75, 0.40])  # [x, y, width, height] in fractions of figure width and height\n",
    "        plot_lines(inset_ax, j, metrics[0])\n",
    "        inset_ax.set_ylim([0.798, 0.81])\n",
    "        inset_ax.set_xticks([0.25, 0.50, 0.75, 1.00])\n",
    "        inset_ax.grid(True)\n",
    "    if j == 3:  # Assuming PinMNIST is the second dataset\n",
    "        # Set the limits for the zoomed-in area\n",
    "        #axs[j].set_ylim([0.93, 0.97])\n",
    "        \n",
    "        # Create a new inset subplot with a zoomed view\n",
    "        inset_ax = axs[j].inset_axes([0.25, 0.35, 0.71, 0.30])  # [x, y, width, height] in fractions of figure width and height\n",
    "        plot_lines(inset_ax, j, metrics[0])\n",
    "        inset_ax.set_ylim([0.2475, 0.2485])\n",
    "        inset_ax.set_xticks([0.25, 0.50, 0.75, 1.00])\n",
    "        inset_ax.grid(True)\n",
    "\n",
    "handles, labels = axs[0].get_legend_handles_labels()\n",
    "\n",
    "# Placing a global legend below the subplots\n",
    "fig.legend(handles, labels, loc='lower center', bbox_to_anchor=(0.5, -0.05), ncol=4, fontsize='small')\n",
    "#fig.suptitle(f'$R^2$  analysis for {mode_name} and {density}')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"AE_Results_{mode_name}_{density}.pdf\", format=\"pdf\", bbox_inches=\"tight\", transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd30a774-10c0-4408-a2fb-aa8df74fd3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correcting the variable naming conflict and trying again\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "font = {'size': 20}\n",
    "\n",
    "matplotlib.rc('font', **font)\n",
    "\n",
    "# Filter the DataFrame for rows where Feature is equal to \"AE\"\n",
    "df_ae = df[(df['Feature'] == 'DDPM')]\n",
    "\n",
    "# Settings for visualization\n",
    "metrics = ['R2']\n",
    "data_groups = ['MSE', 'NPP']\n",
    "data_names = ['Plain', 'NPP']\n",
    "datasets = ['Synthetic', 'PinMNIST']\n",
    "modes = ['mesh'] #df_ae['Mode'].unique()\n",
    "npins = [100] #df_ae['NPins'].unique()\n",
    "mode_name = 'random' if modes[0] == 'random' else 'grid'\n",
    "density = 'dense' if npins[0] == 100 else 'sparse'\n",
    "#colors = sns.color_palette(\"tab10\", n_colors=len(modes)*len(npins))\n",
    "colors= [\"#56CCF2\", \"#EB5757\", \"#333333\", \"#F2C94C\"] #[\"#56CCF2\", None, \"#EB5757\", None, \"#333333\", \"#F2C94C\"]\n",
    "linestyles = ['-', '--', ':', '-.']\n",
    "markers = ['o', 's', '^', 'P']\n",
    "fig, axs = plt.subplots(len(metrics), len(datasets), figsize=(14, 6), sharex=True)\n",
    "# Function to plot the lines, with fixed variable naming\n",
    "def plot_lines(ax, k, metric):\n",
    "    if k == 2 and modes[0] == 'mesh':\n",
    "        if npins[0] == 9:\n",
    "            npins[0] = 16\n",
    "        if npins[0] == 100:\n",
    "            npins[0] = 121\n",
    "    for mode_idx, mode in enumerate(modes):\n",
    "        for npins_idx, npins_value in enumerate(sorted(npins)):  # Corrected variable name here\n",
    "            for data_group_idx, data_group in enumerate(data_groups):\n",
    "                # Filter the dataset for the specific conditions\n",
    "                df_filtered = df_ae[(df_ae['Dataset'] == datasets[k]) & (df_ae['Mode'] == mode) &\n",
    "                                    (df_ae['NPins'] == npins_value)]\n",
    "                if df_filtered.empty:\n",
    "                    print('HEY')\n",
    "                label = data_names[data_group_idx]\n",
    "                color = colors[data_group_idx] #colors[mode_idx * len(npins) + list(sorted(npins)).index(npins_value)]\n",
    "                linestyle = linestyles[data_group_idx] #linestyles[data_group_idx]\n",
    "                marker =  markers[data_group_idx] #markers[data_group_idx]\n",
    "                value = df_filtered[df_filtered['Partial Percent'] == 1.0][data_group].values[0] if metric == 'MSE' else df_filtered[df_filtered['Partial Percent'] == 1.0][f'R2 {data_group}'].values[0]\n",
    "                ax.plot(x, [value]*len(x),\n",
    "                        label=label, color=color, linestyle=linestyle, marker=marker, markersize=12, linewidth=3)\n",
    "    if k == 2:\n",
    "        ax.plot(x, NPP_GP[j][f\"{modes[0]}_{density}\"],\n",
    "                        label=\"NPP-GP\", color=colors[2], linestyle=linestyles[2], marker=markers[2], markersize=12, linewidth=3)\n",
    "    else:\n",
    "        df_filtered = df_ae[(df_ae['Dataset'] == datasets[k]) & (df_ae['Mode'] == mode) &\n",
    "                                    (df_ae['NPins'] == npins_value)]\n",
    "        ax.plot(df_filtered[df_filtered['Partial Percent'] != 0.0]['Partial Percent'], df_filtered[df_filtered['Partial Percent'] != 0.0][f'R2 GP'],\n",
    "                        label=\"NPP-GP\", color=colors[2], linestyle=linestyles[2], marker=markers[2], markersize=12, linewidth=3)\n",
    "\n",
    "        \n",
    "        \n",
    "    ax.plot(x, NP[j][f\"{modes[0]}_{density}\"],\n",
    "                        label=\"NP\", color=colors[3], linestyle=linestyles[3], marker=markers[3], markersize=12, linewidth=4)\n",
    "# Re-plotting for each subplot with the corrected function\n",
    "\n",
    "for j, dataset in enumerate(datasets):\n",
    "    plot_lines(axs[j], j, metrics[0])\n",
    "    #if j == 1:\n",
    "    #    axs[j].set_ylim([0.92, 0.96])\n",
    "    axs[j].set_title(f'{dataset} - $R^2$')\n",
    "    axs[j].set_xlabel('Partial Percent')\n",
    "    axs[j].set_ylabel('$R^2$')\n",
    "    #axs[j].set_ylim([-1.1,1.1])\n",
    "    axs[j].set_xticks([0.25, 0.50, 0.75, 1.00])\n",
    "    #axs[i, j].legend(loc='best', fontsize='small')\n",
    "    axs[j].grid(True)\n",
    "    # Add the zoom-in subplot for the second dataset (PinMNIST)\n",
    "    if j == 0:  # Assuming PinMNIST is the second dataset\n",
    "        # Set the limits for the zoomed-in area\n",
    "        #axs[j].set_ylim([0.93, 0.97])\n",
    "        \n",
    "        # Create a new inset subplot with a zoomed view\n",
    "        inset_ax = axs[j].inset_axes([0.25, 0.65, 0.70, 0.25])  # [x, y, width, height] in fractions of figure width and height\n",
    "        plot_lines(inset_ax, j, metrics[0])\n",
    "        inset_ax.set_ylim([0.835, 0.840])\n",
    "        inset_ax.set_xticks([0.25, 0.50, 0.75, 1.00])\n",
    "        inset_ax.grid(True)\n",
    "    if j == 1:  # Assuming PinMNIST is the second dataset\n",
    "        # Set the limits for the zoomed-in area\n",
    "        #axs[j].set_ylim([0.93, 0.97])\n",
    "        \n",
    "        # Create a new inset subplot with a zoomed view\n",
    "        inset_ax = axs[j].inset_axes([0.20, 0.45, 0.75, 0.32])  # [x, y, width, height] in fractions of figure width and height\n",
    "        plot_lines(inset_ax, j, metrics[0])\n",
    "        inset_ax.set_ylim([0.815, 0.835])\n",
    "        inset_ax.set_xticks([0.25, 0.50, 0.75, 1.00])\n",
    "        inset_ax.grid(True)\n",
    "\n",
    "handles, labels = axs[0].get_legend_handles_labels()\n",
    "\n",
    "# Placing a global legend below the subplots\n",
    "fig.legend(handles, labels, loc='lower center', bbox_to_anchor=(0.5, -0.05), ncol=4, fontsize='small')\n",
    "#fig.suptitle(f'$R^2$  analysis for {mode_name} and {density} with DDPM extracted features')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"DDPM_Results_{mode_name}_{density}.pdf\", format=\"pdf\", bbox_inches=\"tight\", transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f07e88c-4fb7-4430-8e36-01ab85eeed66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[(df['Dataset'] == 'PinMNIST') & (df['Extra layers'] == False)].drop(labels='Extra layers', axis=1).reset_index(drop=True).to_csv('pinMNIST.csv', index=False)\n",
    "df[(df['Dataset'] == 'PinMNIST') & (df['Extra layers'] == False)].drop(labels='Extra layers', axis=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d25f507-152f-4dfc-88bb-3585ce8faf95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[(df['Dataset'] == 'Synthetic') & (df['Extra layers'] == False)].drop(labels='Extra layers', axis=1).reset_index(drop=True).to_csv('Synthetic.csv', index=False)\n",
    "df[(df['Dataset'] == 'Synthetic') & (df['Extra layers'] == False)].drop(labels='Extra layers', axis=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f708afc6-71cb-41de-8d1e-fde94304a6d3",
   "metadata": {},
   "source": [
    "TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4eac5b-7dcd-49ad-b6c9-817b74b87c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from tools.plot_utils import plot_and_save\n",
    "from tools.data_utils import *\n",
    "from tools.losses import NPPLoss\n",
    "from tools.models import Autoencoder\n",
    "from tools.optimization import EarlyStoppingCallback, evaluate_model\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import time\n",
    "from tools.models import *\n",
    "\n",
    "experiment_id = \"1709622898\" # 1709521764\n",
    "experiment_folder = \"./history/exp_def\"\n",
    "dataset = \"Synthetic\"\n",
    "feature = \"DDPM\"\n",
    "mode = \"random\"\n",
    "feature_extracted = True if feature == \"DDPM\" else False\n",
    "mesh = True if mode == \"mesh\" else False\n",
    "d = 3\n",
    "n_pins = 100\n",
    "partial_percent = 0.25\n",
    "r = 3\n",
    "batch_size = 32\n",
    "\n",
    "num_kernels_encoder = [32, 16]\n",
    "num_kernels_decoder = [32]\n",
    "\n",
    "\n",
    "# Set a random seed for PyTorch\n",
    "seed = 4  # You can use any integer value as the seed\n",
    "torch.manual_seed(seed)\n",
    "# Set a random seed for NumPy (if you're using NumPy operations)\n",
    "np.random.seed(seed)\n",
    "\n",
    "if dataset == \"Synthetic\":\n",
    "    input_channel = 3 \n",
    "elif dataset == \"PinMNIST\":\n",
    "    input_channel = 1\n",
    "elif dataset == \"Building\":\n",
    "    input_channel = 4\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if feature_extracted:\n",
    "    folder = f\"{dataset}_ddpm\"\n",
    "else:\n",
    "    folder = f\"{dataset}\"\n",
    "\n",
    "if dataset == \"PinMNIST\":\n",
    "    if mesh:\n",
    "        data_folder = f\"./data/{folder}/mesh_{d}step_{28}by{28}pixels_{r}radius_{seed}seed\"\n",
    "        config['n_pins'] = (28//d + 1)**2\n",
    "    else:\n",
    "        data_folder = f\"./data/{folder}/random_fixedTrue_{n_pins}pins_{28}by{28}pixels_{r}radius_{seed}seed\"\n",
    "elif dataset == \"Synthetic\":\n",
    "    folder += \"/28by28pixels_1000images_123456seed\"\n",
    "    if mesh:\n",
    "        data_folder = f\"./data/{folder}/mesh_{d}step_pins\"\n",
    "        config['n_pins'] = (28//d + 1)**2\n",
    "    else:\n",
    "        data_folder = f\"./data/{folder}/random_{n_pins}pins\"\n",
    "else: # dataset == \"Building\"\n",
    "    raise Exception(\"Building option is still not implemented.\")\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    ToTensor(),         # Convert to tensor (as you were doing)\n",
    "    Resize()  # Resize to 100x100\n",
    "])\n",
    "\n",
    "transformed_dataset = PinDataset(csv_file=f\"{data_folder}/pins.csv\",\n",
    "                                      root_dir=f\"./data/{folder}/images/\",\n",
    "                                      transform=transform)\n",
    "\n",
    "dataset_size = len(transformed_dataset)\n",
    "train_size = int(0.7 * dataset_size)\n",
    "val_size = int(0.10 * dataset_size)\n",
    "test_size = dataset_size - train_size - val_size\n",
    "\n",
    "# Split the dataset into train, validation, and test sets\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    transformed_dataset, [train_size, val_size, test_size]\n",
    ")\n",
    "\n",
    "# Create your DataLoader with the custom_collate_fn\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=custom_collate_fn)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba73a2a-62b6-424d-b62a-5cf6e9a4092b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Testing\n",
    "dataloader = train_loader\n",
    "\n",
    "if not os.path.exists(f'{experiment_folder}/{experiment_id}'):\n",
    "    raise Exception(f\"Could not find experiment with id: {experiment_id}\")\n",
    "else:\n",
    "    autoencoder_MSE = Autoencoder(num_kernels_encoder, num_kernels_decoder, input_channel=input_channel).to(device)\n",
    "    autoencoder_NPP = Autoencoder(num_kernels_encoder, num_kernels_decoder, input_channel=input_channel).to(device)\n",
    "    # Load models\n",
    "    try:\n",
    "        autoencoder_MSE.load_state_dict(torch.load(f'{experiment_folder}/{experiment_id}/best_model_MSE.pth', map_location=device))\n",
    "        autoencoder_MSE.eval()\n",
    "        autoencoder_NPP.load_state_dict(torch.load(f'{experiment_folder}/{experiment_id}/best_model_NPP.pth', map_location=device))\n",
    "        autoencoder_NPP.eval()\n",
    "    except:\n",
    "        raise Exception(\"The model you provided does not correspond with the selected architecture. Please revise and try again.\")\n",
    "\n",
    "        \n",
    "hidden_samples = 0.5\n",
    "for model in [autoencoder_MSE, autoencoder_NPP]:\n",
    "    total_loss = 0.0\n",
    "    criterion = NPPLoss(identity=True).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            x_test = batch['image'][:, :input_channel, :, :].to(device)\n",
    "            p_test = [tensor.to(device) for tensor in batch['pins']]\n",
    "            y_test = [tensor.to(device) for tensor in batch['outputs']]\n",
    "            test_outputs = model(x_test.float())\n",
    "\n",
    "            for i in range(len(x_test)):      \n",
    "                num_samples = int(len(p_test[i]) * hidden_samples)\n",
    "                p_sample = p_test[i][num_samples:]\n",
    "                y_sample = y_test[i][num_samples:]\n",
    "                mu_sample = (test_outputs[i].squeeze())[p_sample[:, 0], p_sample[:, 1]]\n",
    "                if i == 0:\n",
    "                    print('GT: ', y_sample)\n",
    "                    print('OUTPUT: ', mu_sample)\n",
    "            loss = criterion(y_test, test_outputs, p_test)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    total_loss /= len(dataloader)\n",
    "    print('TEST LOSS: ', total_loss, '\\n\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5e048e-d549-4ac2-b6ab-3ec3656da2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "experiment_id = 1709000615\n",
    "dataloader = test_loader\n",
    "\n",
    "if not os.path.exists(f'{experiment_folder}/{experiment_id}'):\n",
    "    raise Exception(f\"Could not find experiment with id: {experiment_id}\")\n",
    "else:\n",
    "    autoencoder_MSE = Autoencoder(num_kernels_encoder, num_kernels_decoder, input_channel=input_channel).to(device)\n",
    "    autoencoder_NPP = Autoencoder(num_kernels_encoder, num_kernels_decoder, input_channel=input_channel).to(device)\n",
    "    # Load models\n",
    "    try:\n",
    "        autoencoder_MSE.load_state_dict(torch.load(f'{experiment_folder}/{experiment_id}/best_model_MSE.pth', map_location=device))\n",
    "        autoencoder_MSE.eval()\n",
    "        autoencoder_NPP.load_state_dict(torch.load(f'{experiment_folder}/{experiment_id}/best_model_NPP.pth', map_location=device))\n",
    "        autoencoder_NPP.eval()\n",
    "    except:\n",
    "        raise Exception(\"The model you provided does not correspond with the selected architecture. Please revise and try again.\")\n",
    "\n",
    "        \n",
    "hidden_samples = 0.5\n",
    "for model in [autoencoder_MSE, autoencoder_NPP]:\n",
    "    total_loss = 0.0\n",
    "    criterion = NPPLoss(identity=True).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            x_test = batch['image'][:, :input_channel, :, :].to(device)\n",
    "            p_test = [tensor.to(device) for tensor in batch['pins']]\n",
    "            y_test = [tensor.to(device) for tensor in batch['outputs']]\n",
    "            test_outputs = model(x_test.float())\n",
    "\n",
    "            for i in range(len(x_test)):      \n",
    "                num_samples = int(len(p_test[i]) * hidden_samples)\n",
    "                p_sample = p_test[i][num_samples:]\n",
    "                y_sample = y_test[i][num_samples:]\n",
    "                mu_sample = (test_outputs[i].squeeze())[p_sample[:, 0], p_sample[:, 1]]\n",
    "                if i == 0:\n",
    "                    print('GT: ', y_sample)\n",
    "                    print('OUTPUT: ', mu_sample)\n",
    "            loss = criterion(y_test, test_outputs, p_test)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    total_loss /= len(dataloader)\n",
    "    print('TEST LOSS: ', total_loss, '\\n\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7be582-e391-4c9a-98d7-d9744cf8f9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "if not os.path.exists(f'{experiment_folder}/{experiment_id}'):\n",
    "    raise Exception(f\"Could not find experiment with id: {experiment_id}\")\n",
    "else:\n",
    "    autoencoder_MSE = Autoencoder(num_kernels_encoder, num_kernels_decoder, input_channel=input_channel).to(device)\n",
    "    autoencoder_NPP = Autoencoder(num_kernels_encoder, num_kernels_decoder, input_channel=input_channel).to(device)\n",
    "    # Load models\n",
    "    try:\n",
    "        autoencoder_MSE.load_state_dict(torch.load(f'{experiment_folder}/{experiment_id}/best_model_MSE.pth'))\n",
    "        autoencoder_NPP.load_state_dict(torch.load(f'{experiment_folder}/{experiment_id}/best_model_NPP.pth'))\n",
    "    except:\n",
    "        raise Exception(\"The model you provided does not correspond with the selected architecture. Please revise and try again.\")\n",
    "    # NPP\n",
    "    for percent in [0.25, 0.50, 0.75, 1.00]:\n",
    "        print(f'Percent testing {percent}')\n",
    "        best_MSE_test_loss = evaluate_model(autoencoder_MSE, test_loader, input_channel, device, partial_label_GP=False, partial_percent=percent)\n",
    "        best_NPP_test_loss = evaluate_model(autoencoder_NPP, test_loader, input_channel, device, partial_label_GP=False, partial_percent=percent)\n",
    "        try:\n",
    "            GP_best_NPP_test_loss = evaluate_model(autoencoder_NPP, test_loader, input_channel, device, partial_label_GP=True, partial_percent=percent)\n",
    "            # Write output into file\n",
    "            filename = f\"test_{folder.split('/')[0]}_{percent}.txt\"\n",
    "            with open(f\"{experiment_folder}/{experiment_id}/{filename}\", \"w\") as f:\n",
    "                f.write(f\"MSE {best_MSE_test_loss}; NPP {best_NPP_test_loss}, {GP_best_NPP_test_loss} (GP)\")\n",
    "        except Exception as Error:\n",
    "            print(Error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b277fdb9-8971-4db8-95ca-f574fb50ce5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "npp",
   "language": "python",
   "name": "npp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
