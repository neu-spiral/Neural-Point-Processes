training start for sigma:0.5
Validation Loss: 62.4391
Validation Loss: 25.9671
Validation Loss: 23.7864
Validation Loss: 23.4001
Validation Loss: 19.3397
Validation Loss: 15.3315
Validation Loss: 13.1684
Validation Loss: 13.9304
Validation Loss: 13.7320
Validation Loss: 11.5005
Validation Loss: 11.4926
Validation Loss: 11.0856
Validation Loss: 11.4903
Validation Loss: 11.3352
Validation Loss: 11.2681
Validation Loss: 10.6188
Validation Loss: 10.8969
Validation Loss: 10.9744
Validation Loss: 10.5285
Validation Loss: 10.4522
Validation Loss: 10.5326
Validation Loss: 11.2599
Validation Loss: 10.3945
Validation Loss: 10.0663
Validation Loss: 10.0953
Validation Loss: 10.3462
Validation Loss: 10.2261
Validation Loss: 10.0815
Validation Loss: 10.2382
Validation Loss: 9.9095
Validation Loss: 10.4735
Validation Loss: 10.7877
Validation Loss: 10.5758
Validation Loss: 10.6051
Validation Loss: 11.3289
Validation Loss: 10.3403
Validation Loss: 11.0141
Validation Loss: 10.2534
Validation Loss: 10.4882
Validation Loss: 10.8665
NPP sigma=0.5 Test loss:9.342, R2 loss:0.835, GP Test loss:9.342, GP R2 loss:0.835
training start for sigma:0.5
Validation Loss: 70.7407
Validation Loss: 28.2810
Validation Loss: 27.6586
Validation Loss: 25.9021
Validation Loss: 22.0524
Validation Loss: 14.8017
Validation Loss: 12.3800
Validation Loss: 12.3014
Validation Loss: 12.9598
Validation Loss: 12.3621
Validation Loss: 14.6130
Validation Loss: 11.4245
Validation Loss: 13.1338
Validation Loss: 12.0176
New LR:  0.0008
Validation Loss: 11.0586
Validation Loss: 11.8143
Validation Loss: 10.7939
Validation Loss: 11.5809
Validation Loss: 11.3315
Validation Loss: 11.0500
Validation Loss: 10.7895
Validation Loss: 11.2272
Validation Loss: 10.5672
Validation Loss: 10.6754
Validation Loss: 11.5576
Validation Loss: 11.0357
Validation Loss: 11.1777
Validation Loss: 10.4563
Validation Loss: 11.1380
Validation Loss: 10.5530
Validation Loss: 10.4026
Validation Loss: 10.5818
Validation Loss: 10.9806
Validation Loss: 10.4473
Validation Loss: 10.1237
Validation Loss: 10.8173
Validation Loss: 10.6118
Validation Loss: 10.2949
New LR:  0.00064
Validation Loss: 10.5747
Validation Loss: 10.4353
NPP sigma=0.5 Test loss:9.730, R2 loss:0.835, GP Test loss:9.730, GP R2 loss:0.835
training start for sigma:0.5
Validation Loss: 76.3275
Validation Loss: 26.6375
Validation Loss: 24.2096
Validation Loss: 22.7502
Validation Loss: 16.1330
Validation Loss: 17.1408
Validation Loss: 13.1017
Validation Loss: 13.0398
Validation Loss: 11.6092
Validation Loss: 12.5592
Validation Loss: 11.2165
Validation Loss: 11.2164
Validation Loss: 11.2633
Validation Loss: 11.7345
Validation Loss: 11.0212
Validation Loss: 11.1710
Validation Loss: 10.4986
Validation Loss: 10.9379
Validation Loss: 10.4927
Validation Loss: 10.3842
Validation Loss: 10.9097
Validation Loss: 10.2600
Validation Loss: 13.6539
Validation Loss: 10.8354
Validation Loss: 10.5761
Validation Loss: 10.4975
Validation Loss: 10.7360
Validation Loss: 10.5126
New LR:  0.0008
Validation Loss: 9.9118
Validation Loss: 10.4709
Validation Loss: 10.4393
Validation Loss: 10.2941
Validation Loss: 10.0770
Validation Loss: 10.1758
Validation Loss: 12.5901
Validation Loss: 10.4054
Validation Loss: 10.6215
Validation Loss: 10.3909
Validation Loss: 10.8759
Validation Loss: 10.3512
NPP sigma=0.5 Test loss:9.764, R2 loss:0.835, GP Test loss:9.764, GP R2 loss:0.835
metrics saved
start saving losses!
Best Learning Rate for NPP sigma=0.5: 0.01
training start for sigma:0.5
Validation Loss: 91.7430
Validation Loss: 53.2361
Validation Loss: 44.0016
Validation Loss: 40.9559
Validation Loss: 42.4837
Validation Loss: 41.9553
Validation Loss: 40.9966
Validation Loss: 41.8370
Validation Loss: 41.1758
Validation Loss: 41.9214
Validation Loss: 42.7990
Validation Loss: 42.1620
Validation Loss: 42.8708
Validation Loss: 43.2985
Validation Loss: 41.8227
Validation Loss: 42.2286
Validation Loss: 41.2329
Validation Loss: 41.2996
Validation Loss: 40.9251
Validation Loss: 41.7956
Validation Loss: 43.2944
Validation Loss: 41.5548
Validation Loss: 41.5940
Validation Loss: 42.5463
Validation Loss: 42.8403
Validation Loss: 41.9394
Validation Loss: 43.0589
Validation Loss: 42.2140
Validation Loss: 41.7749
Validation Loss: 41.3276
Validation Loss: 42.5640
Validation Loss: 41.6925
Validation Loss: 42.2473
Early stopping after 165 epochs.
NPP sigma=0.5 Test loss:46.807, R2 loss:0.237, GP Test loss:46.807, GP R2 loss:0.237
training start for sigma:0.5
Validation Loss: 90.6840
Validation Loss: 55.6276
Validation Loss: 42.0851
Validation Loss: 43.1029
Validation Loss: 41.8375
Validation Loss: 41.7877
Validation Loss: 42.3248
Validation Loss: 42.8184
Validation Loss: 42.0742
Validation Loss: 42.1811
Validation Loss: 41.3432
Validation Loss: 42.6045
Validation Loss: 42.0078
Validation Loss: 41.2966
Validation Loss: 42.5293
Validation Loss: 41.1750
Validation Loss: 42.3659
Validation Loss: 42.1740
Validation Loss: 41.5525
Validation Loss: 42.7101
Validation Loss: 41.7946
Validation Loss: 42.5490
Validation Loss: 42.4497
Validation Loss: 41.6793
Validation Loss: 41.9939
Validation Loss: 41.7856
Validation Loss: 42.8752
Validation Loss: 41.2247
Validation Loss: 41.8244
Validation Loss: 41.4875
Early stopping after 150 epochs.
NPP sigma=0.5 Test loss:46.853, R2 loss:0.237, GP Test loss:46.853, GP R2 loss:0.237
training start for sigma:0.5
Validation Loss: 91.6773
Validation Loss: 73.7581
Validation Loss: 64.7062
Validation Loss: 65.2316
Validation Loss: 65.2941
Validation Loss: 64.7335
Validation Loss: 63.5482
Validation Loss: 64.3620
Validation Loss: 63.1340
Validation Loss: 63.2258
Validation Loss: 65.1059
Validation Loss: 62.8818
Validation Loss: 64.0222
Validation Loss: 64.7343
Validation Loss: 64.5775
Validation Loss: 64.5695
Validation Loss: 64.1590
Validation Loss: 64.0061
Validation Loss: 64.0732
Validation Loss: 63.6822
Validation Loss: 64.1453
Validation Loss: 64.4231
Validation Loss: 64.6083
Validation Loss: 65.0260
Validation Loss: 63.4727
Validation Loss: 64.8543
Early stopping after 130 epochs.
NPP sigma=0.5 Test loss:72.185, R2 loss:-0.107, GP Test loss:72.185, GP R2 loss:-0.107
metrics saved
start saving losses!
