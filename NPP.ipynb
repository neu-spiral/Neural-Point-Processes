{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3836b3f8-f993-415f-b442-f772829e47e6",
   "metadata": {},
   "source": [
    "Neural Point Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca94db99-f1a0-448f-91d0-d6b98ff5bd87",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from skimage import io, transform\n",
    "from functools import lru_cache\n",
    "from tools.plot_utils import visualize_pins, plot_label_pin, plot_all, plot_and_save, plot_loss\n",
    "from tools.data_utils import *\n",
    "from tools.losses import NPPLoss\n",
    "from tools.models import Autoencoder\n",
    "from tools.optimization import EarlyStoppingCallback, train_model, evaluate_model\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_lr_finder import LRFinder\n",
    "import time\n",
    "from tools.models import *\n",
    "from torch.utils.data import Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89746e7d-dcac-4000-bd61-f4c4555562fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.17 (default, Jul  5 2023, 21:04:15) \n",
      "[GCC 11.2.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2954034f-8c6c-4f5d-a937-a367e6da871f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available kernels:\n",
      "  dsk            /home/shi.cheng/.local/share/jupyter/kernels/dsk\n",
      "  jax            /home/shi.cheng/.local/share/jupyter/kernels/jax\n",
      "  local-venv     /home/shi.cheng/.local/share/jupyter/kernels/local-venv\n",
      "  myenv          /home/shi.cheng/.local/share/jupyter/kernels/myenv\n",
      "  pycox          /home/shi.cheng/.local/share/jupyter/kernels/pycox\n",
      "  python3        /home/shi.cheng/.local/share/jupyter/kernels/python3\n",
      "  pytorch_env    /home/shi.cheng/.local/share/jupyter/kernels/pytorch_env\n",
      "  survdata       /home/shi.cheng/.local/share/jupyter/kernels/survdata\n",
      "  tf_env         /home/shi.cheng/.local/share/jupyter/kernels/tf_env\n",
      "  tools          /home/shi.cheng/.local/share/jupyter/kernels/tools\n"
     ]
    }
   ],
   "source": [
    "!jupyter kernelspec list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa53412-0c89-4617-9b7e-efffa9b63834",
   "metadata": {},
   "source": [
    "# Dataset and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "000f94a1-69f9-488f-840c-0a0079b1bb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a random seed for PyTorch\n",
    "seed = 4  # You can use any integer value as the seed\n",
    "torch.manual_seed(seed)\n",
    "# Set a random seed for NumPy (if you're using NumPy operations)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "770be29d-6a82-4394-96c9-8c37bae5e821",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"Building\"\n",
    "feature_extracted = False\n",
    "n = 10\n",
    "mesh = False\n",
    "d = 10\n",
    "n_pins = 100\n",
    "fixed_pins = True\n",
    "r = 3\n",
    "d1, d2 = 28, 28\n",
    "\n",
    "partial_label_GP = False\n",
    "partial_percent = 0.5\n",
    "\n",
    "if feature_extracted:\n",
    "    folder = f\"{dataset}_ddpm\"\n",
    "else:\n",
    "    folder = f\"{dataset}\"\n",
    "\n",
    "if dataset == \"PinMNIST\":\n",
    "    if mesh:\n",
    "        data_folder = f\"./data/{folder}/mesh_{d}step_{28}by{28}pixels_{r}radius_{seed}seed\"\n",
    "        config['n_pins'] = (28 // d + 1) ** 2\n",
    "    else:\n",
    "        data_folder = f\"./data/{folder}/random_fixedTrue_{n_pins}pins_{28}by{28}pixels_{r}radius_{seed}seed\"\n",
    "elif dataset == \"Synthetic\":\n",
    "    folder += \"/28by28pixels_1000images_123456seed\"\n",
    "    if mesh:\n",
    "        data_folder = f\"./data/{folder}/mesh_{d}step_pins\"\n",
    "        config['n_pins'] = (28 // d + 1) ** 2\n",
    "    else:\n",
    "        data_folder = f\"./data/{folder}/random_{n_pins}pins\"\n",
    "elif dataset == \"Building\":\n",
    "    if mesh:\n",
    "        data_folder = f\"./data/{folder}/processed/mesh_{d}_step\"\n",
    "    else:\n",
    "        data_folder = f\"./data/{folder}/processed/random_n_pins_{n_pins}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ac50a0b-10c2-49d1-a2fc-ed7826a04c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset == \"Building\":\n",
    "    resize = Resize100\n",
    "else:\n",
    "    resize = Resize\n",
    "        \n",
    "transform = transforms.Compose([\n",
    "    ToTensor(),  # Convert to tensor (as you were doing)\n",
    "    resize()  # Resize to 100x100\n",
    "])\n",
    "\n",
    "transformed_dataset = PinDataset(csv_file=f\"{data_folder}/pins.csv\",\n",
    "                                 root_dir=f\"./data/{folder}/images/\",\n",
    "                                 transform=transform)\n",
    "\n",
    "# data_folder = f\"./data/{folder}/\"\n",
    "# transformed_dataset = PinDataset(csv_file=f\"{data_folder}/pins_full.csv\",\n",
    "#                                  root_dir=f\"./data/{folder}/images/\",\n",
    "#                                  transform=transform)\n",
    "\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 64\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    images = [sample['image'] for sample in batch]\n",
    "    pins = [sample['pins'] for sample in batch]\n",
    "    outputs = [sample['outputs'] for sample in batch]\n",
    "\n",
    "    return {\n",
    "        'image': torch.stack(images, dim=0),\n",
    "        'pins': pins,\n",
    "        'outputs': outputs}\n",
    "\n",
    "transform = transforms.Compose([\n",
    "        ToTensor(),  # Convert to tensor (as you were doing)\n",
    "        resize()  # Resize to 100x100\n",
    "    ])\n",
    "\n",
    "transformed_dataset = PinDataset(csv_file=f\"{data_folder}/pins.csv\",\n",
    "                                 root_dir=f\"./data/{folder}/processed/images/\",\n",
    "                                 transform=transform)\n",
    "\n",
    "dataset_size = len(transformed_dataset)\n",
    "train_size = int(0.7 * dataset_size)\n",
    "val_size = int(0.10 * dataset_size)\n",
    "test_size = dataset_size - train_size - val_size\n",
    "\n",
    "\n",
    "if os.path.exists(f\"./data/{dataset}/train_indices.npy\"):\n",
    "    train_indices = np.load(f'./data/{dataset}/train_indices.npy')\n",
    "    val_indices = np.load(f'./data/{dataset}/val_indices.npy')\n",
    "    test_indices = np.load(f'./data/{dataset}/test_indices.npy')\n",
    "    # Use the indices to create new datasets\n",
    "    train_dataset = Subset(transformed_dataset, train_indices)\n",
    "    val_dataset = Subset(transformed_dataset, val_indices)\n",
    "    test_dataset = Subset(transformed_dataset, test_indices)\n",
    "else:\n",
    "    # Split the dataset into train, validation, and test sets\n",
    "    train_dataset, val_dataset, test_dataset = random_split(\n",
    "        transformed_dataset, [train_size, val_size, test_size]\n",
    "    )\n",
    "    np.save(f'./data/{dataset}/train_indices.npy', train_dataset.indices)\n",
    "    np.save(f'./data/{dataset}/val_indices.npy', val_dataset.indices)\n",
    "    np.save(f'./data/{dataset}/test_indices.npy', test_dataset.indices)\n",
    "\n",
    "# Create your DataLoader with the custom_collate_fn\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=custom_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6212ed98-0eb4-4216-bee4-4f9cbdd8a4a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "train_idx = train_dataset.indices.copy()\n",
    "train_idx.extend(range(1000, 1698))\n",
    "train_dataset = Subset(transformed_dataset, train_idx)\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371fb7d3-e158-435e-94fa-0118817950ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for batch in train_loader:\n",
    "    images = batch['image'].to(device) # get RGB instead of RGBA\n",
    "    pins = batch['pins']\n",
    "    outputs = batch['outputs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce377de-650d-4e4c-9520-437fc8752263",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_img = images[0][0].squeeze().detach().cpu()/255\n",
    "count_image = plot_label_pin(sample_img, pins[0], outputs[0])\n",
    "count_all_image = plot_all(sample_img, r=3)\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Plot the sample_img in the first subplot\n",
    "im0 = axes[0].imshow(sample_img)\n",
    "axes[0].set_title('Sample Image')\n",
    "fig.colorbar(im0, ax=axes[0], fraction=0.046, pad=0.04)  # Add colorbar\n",
    "\n",
    "# Plot the count_image in the second subplot\n",
    "im1 = axes[1].imshow(count_image)\n",
    "axes[1].set_title('Count Image')\n",
    "fig.colorbar(im1, ax=axes[1], fraction=0.046, pad=0.04)  # Add colorbar\n",
    "\n",
    "# Plot the count_all_image in the third subplot\n",
    "im2 = axes[2].imshow(count_all_image)\n",
    "axes[2].set_title('Count All Image')\n",
    "fig.colorbar(im2, ax=axes[2], fraction=0.046, pad=0.04)  # Add colorbar\n",
    "\n",
    "# Add spacing between subplots\n",
    "plt.tight_layout()\n",
    "# Display the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c306e1-061a-47bc-8584-46ad8e08ee8b",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa39fe1f-e261-4bc3-b8b4-75fc9ca989a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLRFinder:\n",
    "    def __init__(self, model, criterion, optimizer, device='cuda'):\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "        self.history = {'lr': [], 'loss': []}\n",
    "\n",
    "    def find_lr(self, train_loader, start_lr=1e-4, end_lr=1, num_iter=20,smooth_f=0.05):\n",
    "        model = self.model.to(self.device)\n",
    "        criterion = self.criterion\n",
    "        optimizer = self.optimizer\n",
    "        device = self.device\n",
    "        model.train()\n",
    "\n",
    "        lr_step = (end_lr / start_lr) ** (1 / num_iter)\n",
    "        lr = start_lr\n",
    "\n",
    "        for iteration in range(num_iter):\n",
    "            optimizer.param_groups[0]['lr'] = lr\n",
    "\n",
    "            total_loss = 0.0\n",
    "            for batch in train_loader:\n",
    "                x_train = batch['image'][:, :input_channel, :, :].to(device)\n",
    "                p_train = [tensor.to(device) for tensor in batch['pins']]\n",
    "                y_train = [tensor.to(device) for tensor in batch['outputs']]\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(x_train.float())\n",
    "                loss = criterion(y_train, outputs, p_train)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            avg_loss = total_loss / len(train_loader)\n",
    "            self.history['lr'].append(lr)\n",
    "            self.history['loss'].append(avg_loss)\n",
    "\n",
    "            lr *= lr_step\n",
    "            \n",
    "    def plot_lr_finder(self):\n",
    "        plt.plot(self.history['lr'], self.history['loss'])\n",
    "        plt.xscale('log')  # Use a logarithmic scale for better visualization\n",
    "        plt.xlabel('Learning Rate')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Learning Rate Finder Curve')\n",
    "        plt.show()\n",
    "        \n",
    "    def find_best_lr(self, skip_start=3, skip_end=3):\n",
    "        # Find the index of the minimum loss in the specified range\n",
    "        min_loss_index = skip_start + np.argmin(self.history['loss'][skip_start:-skip_end])\n",
    "\n",
    "        # Output the learning rate corresponding to the minimum loss\n",
    "        best_lr = self.history['lr'][min_loss_index]\n",
    "        return best_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04a8eba-47fc-4225-a7e9-7442ae7aca97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Case 1: identity=True\n",
    "# Set your hyperparameters\n",
    "input_channel = 1 if dataset == \"PinMNIST\" else 3\n",
    "epochs = 20\n",
    "sigmas = [0.1, 0.2, 0.5, 1, 2, 5]  # Set the sigma values you want to test\n",
    "num_kernels_encoder = [32, 16]\n",
    "num_kernels_decoder = [32]\n",
    "learning_rate = 0.01\n",
    "val_every_epoch = 5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Autoencoder(num_kernels_encoder, num_kernels_decoder, input_channel=input_channel).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "criterion_MSE = NPPLoss(identity=True).to(device)\n",
    "lr_finder_MSE = CustomLRFinder(model, criterion_MSE, optimizer, device=device)\n",
    "lr_finder_MSE.find_lr(train_loader, start_lr=1e-5, end_lr=1, num_iter=20)\n",
    "best_lr_MSE = lr_finder_MSE.find_best_lr()\n",
    "print(f\"Best Learning Rate for MSE: {best_lr_MSE}\")\n",
    "\n",
    "\n",
    "# Cases 2-6: identity=False, varying sigmas\n",
    "best_lrs = [(0,best_lr_MSE)]\n",
    "\n",
    "sigmas = [0.1, 0.2, 0.5, 1, 2]\n",
    "\n",
    "for sigma in sigmas:\n",
    "    criterion_NPP = NPPLoss(identity=False, sigma=sigma).to(device)\n",
    "    lr_finder_NPP = CustomLRFinder(model, criterion_NPP, optimizer, device=device)\n",
    "    lr_finder_NPP.find_lr(train_loader, start_lr=1e-4, end_lr=1, num_iter=10)\n",
    "    best_lr_NPP = lr_finder_NPP.find_best_lr()\n",
    "    best_lrs.append((sigma, best_lr_NPP))\n",
    "    print(f\"Best Learning Rate for NPP sigma={sigma}: {best_lr_NPP}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e9111c-3a3f-498b-9574-56626971725a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline_ci(sigmas, num_kernels_encoder, num_kernels_decoder, train_loader, val_loader, \n",
    "                    test_loader, input_channel, epochs, val_every_epoch, learning_rates, device, num_runs=3):\n",
    "    test_losses_npp_true = []\n",
    "    GP_test_losses_npp_true = []\n",
    "    test_losses_npp_false= []\n",
    "    experiment_id = int(time.time())\n",
    "    \n",
    "    for run in range(num_runs):\n",
    "        count = 0\n",
    "        test_losses_vs_sigma_npp_true = []\n",
    "        GP_test_losses_vs_sigma_npp_true = []\n",
    "        test_loss_npp_false = None\n",
    "\n",
    "        # Run NPP=False once and collect the test loss\n",
    "        early_stopping = EarlyStoppingCallback(patience=10, min_delta=0.001)\n",
    "        criterion = NPPLoss(identity=True).to(device)\n",
    "\n",
    "        autoencoder = Autoencoder(num_kernels_encoder, num_kernels_decoder, input_channel=input_channel).to(device)\n",
    "        \n",
    "        optimizer = optim.Adam(autoencoder.parameters(), lr=learning_rates[count])\n",
    "        model, train_losses, val_losses = train_model(autoencoder, train_loader, val_loader, input_channel, epochs,\\\n",
    "                                                      val_every_epoch, learning_rates[count], criterion, optimizer, device, early_stopping, experiment_id)\n",
    "\n",
    "        test_loss_npp_false = evaluate_model(autoencoder, test_loader, input_channel, device, partial_label_GP=False, partial_percent=partial_percent)\n",
    "        print(f\"MSE Test loss:{test_loss_npp_false:.3f}\")\n",
    "        test_losses_npp_false.append(test_loss_npp_false)\n",
    "        \n",
    "        count += 1\n",
    "        # Run LR Finder for different sigma values\n",
    "        for sigma in sigmas:\n",
    "            early_stopping = EarlyStoppingCallback(patience=5, min_delta=0.001)\n",
    "            criterion = NPPLoss(identity=False, sigma=sigma).to(device)\n",
    "            autoencoder = Autoencoder(num_kernels_encoder, num_kernels_decoder, input_channel=input_channel).to(device)\n",
    "            optimizer = optim.Adam(autoencoder.parameters(), lr=learning_rates[count])\n",
    "            model, train_losses, val_losses = train_model(autoencoder, train_loader, val_loader, input_channel, epochs,\\\n",
    "                                                          val_every_epoch, learning_rates[count], criterion, optimizer, device, early_stopping, experiment_id)\n",
    "            test_loss = evaluate_model(autoencoder, test_loader, input_channel, device, partial_label_GP=False, partial_percent=partial_percent, sigma=sigma)\n",
    "            GP_test_loss = evaluate_model(autoencoder, test_loader, input_channel, device, partial_label_GP=True, partial_percent=partial_percent, sigma=sigma)\n",
    "            print(f\"NPP sigma={sigma} Test loss:{test_loss:.3f}, GP Test loss:{GP_test_loss:.3f}\")\n",
    "            test_losses_vs_sigma_npp_true.append(test_loss)\n",
    "            GP_test_losses_vs_sigma_npp_true.append(GP_test_loss)\n",
    "            count += 1\n",
    "\n",
    "        test_losses_npp_true.append(test_losses_vs_sigma_npp_true)\n",
    "        GP_test_losses_npp_true.append(GP_test_losses_vs_sigma_npp_true)\n",
    "    return GP_test_losses_npp_true, test_losses_npp_true, test_losses_npp_false\n",
    "\n",
    "    \n",
    "# Function to run the pipeline and save data\n",
    "def run_and_save_pipeline(sigmas, num_kernels_encoder, num_kernels_decoder, train_loader, val_loader, test_loader, input_channel, epochs, val_every_epoch, learning_rates, device, num_runs):\n",
    "    # Run the pipeline\n",
    "    GP_test_loss_npp_true, test_loss_npp_true, test_loss_npp_false= run_pipeline_ci(sigmas, num_kernels_encoder, num_kernels_decoder, train_loader, val_loader, test_loader, input_channel, epochs, val_every_epoch, learning_rates, device,num_runs)\n",
    "    print(\"start saving!\")\n",
    "    # Save the data\n",
    "    save_loss(test_loss_npp_true, './history/test_loss_npp_true.npy')\n",
    "    save_loss(GP_test_loss_npp_true, './history/GP_test_loss_npp_true.npy')\n",
    "    save_loss(test_loss_npp_false, './history/test_loss_npp_false.npy')\n",
    "    print(\"saved\")\n",
    "    return GP_test_loss_npp_true, test_loss_npp_true, test_loss_npp_false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c681ad7-b14c-4d02-ad7f-da35693469d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate a unique experiment_id using a timestamp\n",
    "  # Using timestamp as experiment_id\n",
    "\n",
    "# Set your hyperparameters\n",
    "# dataset = \"MNIST\"\n",
    "input_channel = 1 if dataset == \"PinMNIST\" else 3\n",
    "epochs = 200\n",
    "sigmas = [0.1, 0.2, 0.5, 1, 2]  # Set the sigma values you want to test\n",
    "# best_lrs = [0.05, 0.05, 0.05, 0.05, 0.05, 0.001] #MNIST\n",
    "best_lrs = [0.1, 0.001, 0.001, 0.001, 0.001, 0.001] #Synthetic\n",
    "num_kernels_encoder = [32, 16]\n",
    "num_kernels_decoder = [32]\n",
    "# learning_rate = 0.01\n",
    "val_every_epoch = 5\n",
    "num_runs = 1\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Run and save the pipeline data\n",
    "loss_vs_sigma_data = run_and_save_pipeline(sigmas, num_kernels_encoder, num_kernels_decoder, train_loader, val_loader, test_loader,\\\n",
    "                                           input_channel, epochs, val_every_epoch, best_lrs, device, num_runs=num_runs)\n",
    "\n",
    "\n",
    "# Plot and save the plot using the saved data\n",
    "plot_and_save(loss_vs_sigma_data, sigmas, dataset, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53b7484-3290-43cb-a85e-39cca0f9e140",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_vs_sigma_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527b04d7-2308-48e9-9a75-d8071dd3d0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_loss_npp_false = [test_loss_npp_false for i in range((len(sigmas)))]\n",
    "# test_loss_npp_true.pop(1)\n",
    "test_loss_npp_true, test_loss_npp_false = load_data('./history/test_loss_npp_true.npy'), load_data('./history/test_loss_npp_false.npy')\n",
    "test_loss_npp_true, test_loss_npp_false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8bf364-d010-40de-ad6c-ce837b5dee2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_and_save(loss_vs_sigma_data, sigmas, dataset, learning_rate, dataset, model_name=\"Auto encoder\"):\n",
    "    # Unpack the data\n",
    "    test_loss_npp_true, test_loss_npp_false = loss_vs_sigma_data\n",
    "    test_loss_npp_false = [test_loss_npp_false for i in range(len(sigmas))]\n",
    "\n",
    "    # Calculate mean and confidence intervals for NPP=True runs\n",
    "    mean_test_loss_npp_true = np.mean(test_loss_npp_true, axis=0)\n",
    "    ci_test_loss_npp_true = 1.96 * np.std(test_loss_npp_true, axis=0) / np.sqrt(len(test_loss_npp_true))\n",
    "\n",
    "    # Duplicate NPP=False values for plotting\n",
    "    mean_test_loss_npp_false = np.mean(test_loss_npp_false, axis=1)\n",
    "    ci_test_loss_npp_false = 1.96 * np.std(test_loss_npp_false, axis=1) / np.sqrt(len(test_loss_npp_false))\n",
    "\n",
    "    # Plot mean and confidence intervals for NPP=True\n",
    "    plt.plot(sigmas, mean_test_loss_npp_true, marker='o', label='NPP=True', color='blue')\n",
    "\n",
    "    # Plot mean and confidence intervals for duplicated NPP=False\n",
    "    plt.plot(sigmas, mean_test_loss_npp_false, color='red', linestyle='--', label='NPP=False')\n",
    "\n",
    "    # Fill between for NPP=True with blue color\n",
    "    plt.fill_between(sigmas, mean_test_loss_npp_true - ci_test_loss_npp_true, mean_test_loss_npp_true + ci_test_loss_npp_true, color='blue', alpha=0.2)\n",
    "\n",
    "    # Fill between for NPP=False with red color\n",
    "    plt.fill_between(sigmas, mean_test_loss_npp_false - ci_test_loss_npp_false, mean_test_loss_npp_false + ci_test_loss_npp_false, color='red', alpha=0.2)\n",
    "\n",
    "    plt.xlabel('Sigma')\n",
    "    plt.ylabel('Test Loss')\n",
    "    plt.title(f'Test Loss vs. Sigma:{dataset} dataset with {model_name}')\n",
    "    plt.legend()\n",
    "\n",
    "    # Create a directory to save the results if it doesn't exist\n",
    "    results_dir = './results'\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "    # Generate a filename based on parameters in the title\n",
    "    filename = f\"test_loss_vs_sigma_{dataset}_{model_name}_lr_{learning_rate}.png\"\n",
    "    filepath = os.path.join(results_dir, filename)\n",
    "\n",
    "    # Save the plot\n",
    "    plt.savefig(filepath)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# plot_and_save(loss_vs_sigma_data, sigmas, dataset, learning_rate)\n",
    "\n",
    "# Plot and save the plot using the saved data\n",
    "plot_and_save(loss_vs_sigma_data, sigmas, dataset, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26a5d16-5dbd-4a42-917e-72b80b9fd9b2",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e275dea-f73a-4c94-a6dd-f2b5def8f4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(x, p, y, outputs, r):\n",
    "    plt.figure(figsize=(14, 14))  # Adjust the figure size\n",
    "    n_shows = 4\n",
    "\n",
    "    for i in range(n_shows):\n",
    "        # Original Images\n",
    "        img = x[i].squeeze().detach().cpu() / 255\n",
    "\n",
    "        plt.subplot(4, 4, 4*i+1 )\n",
    "        plt.imshow(img)\n",
    "\n",
    "        plt.subplot(4, 4, 4*i + 2)  # Transposed the rows and columns\n",
    "        count_all_image = plot_all(img, r=r)\n",
    "        plt.imshow(count_all_image)\n",
    "\n",
    "        # Reconstructed Images (switched with row 3)\n",
    "        plt.subplot(4, 4, 4*i + 3)  # Transposed the rows and columns\n",
    "        count_image = plot_label_pin(img, p[i], y[i])\n",
    "        plt.imshow(count_image)\n",
    "\n",
    "        plt.subplot(4, 4, 4*i + 4)\n",
    "        plt.imshow(outputs[i].squeeze().detach().cpu())\n",
    "\n",
    "    # Add an overall color bar\n",
    "    plt.subplots_adjust(bottom=0.2, hspace=0.4)  # Increase the vertical spacing\n",
    "    cbar_ax = plt.gcf().add_axes([0.15, 0.1, 0.7, 0.03])  # Define the position and size of the color bar\n",
    "    cbar = plt.colorbar(cax=cbar_ax, orientation='horizontal')\n",
    "\n",
    "    # Add titles in the middle of the entire row\n",
    "    plt.subplot(4, 4, 1).set_title(\"images\", position=(0.5, 1.05))\n",
    "    plt.subplot(4, 4, 2).set_title(\"Label all map\", position=(0.5, 1.05))\n",
    "    plt.subplot(4, 4, 3).set_title(\"Label pins\", position=(0.5, 1.05))\n",
    "    plt.subplot(4, 4, 4).set_title(\"Predicted\", position=(0.5, 1.05))\n",
    "\n",
    "    # Define the output folder based on the dataset\n",
    "    output_folder = f\"results/{dataset}\"\n",
    "    os.makedirs(output_folder, exist_ok=True)  # Create the folder if it doesn't exist\n",
    "\n",
    "    # Define the image filename based on parameters\n",
    "    image_filename = f\"NPP{NPP}_LR={learning_rate}_n={n}_mesh={mesh}_d={d}_n_pins={n_pins}_fixedpins={fixed_pins}_r={r}.png\"\n",
    "\n",
    "    # Save the figure as a high-resolution PNG in the specified folder\n",
    "    plt.savefig(os.path.join(output_folder, image_filename), dpi=100)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd6564e-cd65-4496-ba82-4784f2cdc03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_samples(dataloader, autoencoder):\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            x = batch['image'][:, :input_channel, :, :].to(device)\n",
    "            p = [tensor.to(device) for tensor in batch['pins']]\n",
    "            y = [tensor.to(device) for tensor in batch['outputs']]\n",
    "\n",
    "            outputs = autoencoder(x.float())\n",
    "            break\n",
    "    plot_results(x, p, y, outputs, r=r)\n",
    "    \n",
    "\n",
    "visualize_samples(test_loader, autoencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9cf4ef-34ad-473e-9e19-ba16e5307025",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pycox",
   "language": "python",
   "name": "pycox"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
