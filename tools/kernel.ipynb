{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e568a099-7486-47af-9e51-629456d9bf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Batch_SMKernel(nn.Module):\n",
    "    def __init__(self, Q, D):\n",
    "        \"\"\"\n",
    "        Initialize the MixtureKernel.\n",
    "\n",
    "        Parameters:\n",
    "        Q (int): Number of mixture components.\n",
    "        D (int): Dimensionality of the input space.\n",
    "        \"\"\"\n",
    "        super(Batch_SMKernel, self).__init__()\n",
    "        self.Q = Q\n",
    "        self.D = D\n",
    "        \n",
    "        # Initialize weights (w_q)\n",
    "        self.weights = nn.Parameter(torch.ones(Q) / Q)\n",
    "        \n",
    "        # Initialize means (mu_q)\n",
    "        self.means = nn.Parameter(torch.randn(Q, D))\n",
    "        \n",
    "        # Initialize diagonal elements of covariance matrices (v_qd)\n",
    "        self.log_covariances = nn.Parameter(torch.zeros(Q, D))\n",
    "\n",
    "    def forward(self, x, x_prime):\n",
    "        \"\"\"\n",
    "        Compute the kernel matrix for batched inputs.\n",
    "\n",
    "        Parameters:\n",
    "        x (torch.Tensor): Input tensor of shape (n, D)\n",
    "        x_prime (torch.Tensor): Input tensor of shape (n, D)\n",
    "\n",
    "        Returns:\n",
    "        torch.Tensor: The kernel matrix of shape (n, n)\n",
    "        \"\"\"\n",
    "        # Number of samples in the batch\n",
    "        n = x.shape[0]\n",
    "\n",
    "        # Reshape x and x_prime for broadcasting\n",
    "        x = x.unsqueeze(1)  # Shape: (n, 1, D)\n",
    "        x_prime = x_prime.unsqueeze(0)  # Shape: (1, n, D)\n",
    "\n",
    "        # Compute pairwise differences\n",
    "        diff = x - x_prime  # Shape: (n, n, D)\n",
    "        \n",
    "        kernel_matrix = torch.zeros(n, n)\n",
    "\n",
    "        for q in range(self.Q):\n",
    "            w_q = torch.exp(self.weights[q])\n",
    "            mu_q = self.means[q]\n",
    "            sigma_q_diag = torch.exp(self.log_covariances[q])\n",
    "            Sigma_q = torch.diag(sigma_q_diag)\n",
    "            \n",
    "            # Compute determinant of Σ_q (product of diagonal elements)\n",
    "            det_Sigma_q = torch.prod(sigma_q_diag)\n",
    "            \n",
    "            # Compute the normalization factor\n",
    "            norm_factor = 1 / (det_Sigma_q**0.5 * (2 * torch.pi)**(self.D / 2))\n",
    "            \n",
    "            # Compute the exponent term\n",
    "            exponent = -0.5 * torch.einsum('ijD,D,ijD->ij', diff, sigma_q_diag, diff)  # Shape: (n, n)\n",
    "            \n",
    "            # Compute the cosine term\n",
    "            cosine_term = torch.cos(2 * torch.pi * torch.einsum('ijD,D->ij', diff, mu_q))  # Shape: (n, n)\n",
    "            exp_term = torch.exp(exponent)\n",
    "            print(exp_term, cosine_term)\n",
    "            # Add the weighted component to the kernel matrix\n",
    "            kernel_matrix +=  exp_term* cosine_term\n",
    "\n",
    "        return kernel_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fbcc1e8-92a9-4cef-a1c7-5a09a4f8a159",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SMKernel(nn.Module):\n",
    "    def __init__(self, Q, D):\n",
    "        \"\"\"\n",
    "        Initialize the MixtureKernel.\n",
    "\n",
    "        Parameters:\n",
    "        Q (int): Number of mixture components.\n",
    "        D (int): Dimensionality of the input space.\n",
    "        \"\"\"\n",
    "        super(SMKernel, self).__init__()\n",
    "        self.Q = Q\n",
    "        self.D = D\n",
    "        \n",
    "        # Initialize weights (w_q)\n",
    "        self.weights = nn.Parameter(torch.ones(Q) / Q)\n",
    "        \n",
    "        # Initialize means (mu_q)\n",
    "        self.means = nn.Parameter(torch.randn(Q, D))\n",
    "        \n",
    "        # Initialize diagonal elements of covariance matrices (v_qd)\n",
    "        self.log_covariances = nn.Parameter(torch.zeros(Q, D))\n",
    "\n",
    "    def forward(self, x, x_prime):\n",
    "        \"\"\"\n",
    "        Compute the kernel function.\n",
    "\n",
    "        Parameters:\n",
    "        x (torch.Tensor): Input vector of shape (D,)\n",
    "        x_prime (torch.Tensor): Input vector of shape (D,)\n",
    "\n",
    "        Returns:\n",
    "        torch.Tensor: The value of the kernel function\n",
    "        \"\"\"\n",
    "        kernel_value = 0.0\n",
    "        \n",
    "        for q in range(self.Q):\n",
    "            w_q = torch.exp(self.weights[q])\n",
    "            mu_q = self.means[q]\n",
    "            sigma_q_diag = torch.exp(self.log_covariances[q])\n",
    "            Sigma_q = torch.diag(sigma_q_diag)\n",
    "            \n",
    "            # Compute determinant of Σ_q (product of diagonal elements)\n",
    "            det_Sigma_q = torch.prod(sigma_q_diag)\n",
    "            \n",
    "            # Compute the normalization factor\n",
    "            norm_factor = 1 / (det_Sigma_q**0.5*((2 * torch.pi)**(self.D / 2)))\n",
    "            \n",
    "            # Compute the exponent term\n",
    "            diff = x - x_prime\n",
    "\n",
    "            exponent = -0.5 * torch.mm(diff, torch.matmul(Sigma_q, diff.T))\n",
    "            # Compute the cosine term\n",
    "            cosine_term = torch.cos(2 * torch.pi * torch.dot(diff.squeeze(), mu_q))\n",
    "            \n",
    "            # Add the weighted component to the kernel value\n",
    "            kernel_value += w_q * norm_factor * torch.exp(exponent) * cosine_term\n",
    "\n",
    "        return kernel_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bfd4c440-a4d9-4c3d-95bf-887ba4e9042f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.7299e+02, 2.4206e+03, 4.3679e+02],\n",
       "        [0.0000e+00, 1.0000e+00, 8.9048e+02],\n",
       "        [3.6945e+01, 1.3357e+03, 1.4905e+04],\n",
       "        [0.0000e+00, 8.9429e+03, 1.8077e+02],\n",
       "        [2.1746e+01, 2.1839e+02, 4.3865e+03]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = 2  # Number of mixture components\n",
    "D = 3  # Dimensionality of the input space\n",
    "# Generate some test data\n",
    "n = 5  # Number of samples\n",
    "m = 10\n",
    "x = torch.randint(0, 10, (n, D)).float()\n",
    "x_prime = torch.randint(0, 10, (n, D)).float()\n",
    "# torch.exp(x)*x_prime\n",
    "torch.exp(x)* x_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a89977c2-9bba-4335-ae2b-f9a4236bef89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.5991e-10, 1.3888e-11, 6.3051e-16, 4.1399e-08, 1.2502e-09, 8.2085e-02,\n",
      "         2.5110e-08, 6.1442e-06, 1.2502e-09, 4.1399e-08],\n",
      "        [1.4069e-16, 3.4425e-14, 3.4425e-14, 6.3051e-16, 8.6441e-22, 6.8256e-08,\n",
      "         4.1938e-13, 3.6788e-01, 1.0395e-15, 2.0612e-09],\n",
      "        [2.1151e-19, 3.5786e-29, 3.1800e-22, 5.0435e-07, 2.6103e-23, 1.1254e-07,\n",
      "         1.8795e-12, 5.0435e-07, 2.3195e-16, 3.0988e-12],\n",
      "        [6.9144e-13, 7.6812e-15, 1.0395e-15, 2.0347e-04, 5.6028e-09, 3.6788e-01,\n",
      "         6.1442e-06, 2.5110e-08, 3.0590e-07, 5.0435e-07],\n",
      "        [5.2429e-22, 5.3111e-27, 2.5768e-18, 8.2085e-02, 8.5330e-17, 1.2341e-04,\n",
      "         2.2603e-06, 2.2897e-11, 5.6028e-09, 2.5110e-08]],\n",
      "       grad_fn=<ExpBackward0>) tensor([[-0.5895,  0.1869,  0.8441,  0.8596, -0.2630, -0.4058, -0.9507, -0.8643,\n",
      "         -0.9655, -0.4796],\n",
      "        [ 0.6875, -0.0604, -0.9055, -0.7876,  0.3836,  0.5188,  0.9824,  0.7933,\n",
      "          0.9245,  0.5873],\n",
      "        [-0.8173, -0.1378,  0.9714,  0.6505, -0.5583, -0.6773, -0.9999, -0.6576,\n",
      "         -0.8311, -0.7355],\n",
      "        [ 0.7029,  0.9996, -0.3941,  0.6421,  0.9102,  0.8372,  0.1538, -0.6349,\n",
      "         -0.4117,  0.7894],\n",
      "        [-0.8718, -0.9534,  0.6322, -0.4058, -0.9891, -0.9555, -0.4203,  0.3972,\n",
      "          0.1444, -0.9280]], grad_fn=<CosBackward0>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (5) must match the size of tensor b (10) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 11\u001b[0m\n\u001b[1;32m      7\u001b[0m b_kernel\u001b[38;5;241m.\u001b[39mlog_covariances\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m kernel\u001b[38;5;241m.\u001b[39mlog_covariances\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mclone()\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Compute the kernel matrix in batch\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m kernel_matrix_batch \u001b[38;5;241m=\u001b[39m \u001b[43mb_kernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_prime\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Compute the kernel matrix element-wise\u001b[39;00m\n\u001b[1;32m     14\u001b[0m kernel_matrix_elementwise \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(n, n)\n",
      "File \u001b[0;32m~/anaconda3/envs/npp/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/npp/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[14], line 70\u001b[0m, in \u001b[0;36mBatch_SMKernel.forward\u001b[0;34m(self, x, x_prime)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28mprint\u001b[39m(exp_term, cosine_term)\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# Add the weighted component to the kernel matrix\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     kernel_matrix \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m  exp_term\u001b[38;5;241m*\u001b[39m cosine_term\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m kernel_matrix\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (5) must match the size of tensor b (10) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# Create the kernel\n",
    "b_kernel = Batch_SMKernel(Q, D)\n",
    "kernel = SMKernel(Q, D)\n",
    "# Ensure they have the same parameters\n",
    "b_kernel.weights.data = kernel.weights.data.clone()\n",
    "b_kernel.means.data = kernel.means.data.clone()\n",
    "b_kernel.log_covariances.data = kernel.log_covariances.data.clone()\n",
    "\n",
    "\n",
    "# Compute the kernel matrix in batch\n",
    "kernel_matrix_batch = b_kernel(x, x_prime)\n",
    "\n",
    "# Compute the kernel matrix element-wise\n",
    "kernel_matrix_elementwise = torch.zeros(n, n)\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        kernel_matrix_elementwise[i, j] = kernel.forward(x[i].unsqueeze(0), x_prime[j].unsqueeze(0))\n",
    "\n",
    "# Check if the results are the same\n",
    "print(\"Kernel matrix (batch computation):\")\n",
    "print(kernel_matrix_batch)\n",
    "print(\"Kernel matrix (element-wise computation):\")\n",
    "print(kernel_matrix_elementwise)\n",
    "\n",
    "# Check for equality\n",
    "print(\"Are the results the same? \", torch.allclose(kernel_matrix_batch, kernel_matrix_elementwise))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3161faa-9bb4-4f21-869d-4b9282b8dfb0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MixtureKernel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Define a small MixtureKernel for testing\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Create the kernel\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m kernel \u001b[38;5;241m=\u001b[39m \u001b[43mMixtureKernel\u001b[49m(Q, D)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Generate some test data\u001b[39;00m\n\u001b[1;32m      8\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m  \u001b[38;5;66;03m# Number of samples\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MixtureKernel' is not defined"
     ]
    }
   ],
   "source": [
    "# Define a small MixtureKernel for testing\n",
    "\n",
    "\n",
    "# Create the kernel\n",
    "kernel = MixtureKernel(Q, D)\n",
    "\n",
    "# Generate some test data\n",
    "n = 5  # Number of samples\n",
    "x = torch.randn(n, D)\n",
    "x_prime = torch.randn(n+5, D)\n",
    "\n",
    "# Compute the kernel matrix in batch\n",
    "def compute_kernel_matrix_batch(kernel, x, x_prime):\n",
    "    n = x.shape[0]\n",
    "    x = x.unsqueeze(1)  # Shape: (n, 1, D)\n",
    "    x_prime = x_prime.unsqueeze(0)  # Shape: (1, n, D)\n",
    "    diff = x - x_prime  # Shape: (n, n, D)\n",
    "    \n",
    "    kernel_matrix = torch.zeros(n, n)\n",
    "    \n",
    "    for q in range(kernel.Q):\n",
    "        w_q = torch.exp(kernel.weights[q])\n",
    "        mu_q = kernel.means[q]\n",
    "        sigma_q_diag = torch.exp(kernel.log_covariances[q])\n",
    "        Sigma_q_inv = torch.diag(1 / sigma_q_diag)\n",
    "        det_Sigma_q = torch.prod(sigma_q_diag)\n",
    "        norm_factor = 1 / (det_Sigma_q**0.5 * (2 * torch.pi)**(kernel.D / 2))\n",
    "        exponent = -0.5 * torch.einsum('ijD,D,ijD->ij', diff, 1 / sigma_q_diag, diff)\n",
    "        cosine_term = torch.cos(2 * torch.pi * torch.einsum('ijD,D->ij', diff, mu_q))\n",
    "        kernel_matrix += w_q * norm_factor * torch.exp(exponent) * cosine_term\n",
    "    \n",
    "    return kernel_matrix\n",
    "\n",
    "kernel_matrix_batch = compute_kernel_matrix_batch(kernel, x, x_prime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2873f358-dbcd-4713-9bbe-d89789c20884",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "npp",
   "language": "python",
   "name": "npp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
